{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cleaned-ehr-treatment-v6/cleaned_EHR_treatment_param_lab_test_final_3diseases-v6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "os.system('pip install tensorflow-gpu==1.14.0')\n",
    "os.system('pip install keras==2.2.5')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "# from keras.models import Sequential, load_model, Model\n",
    "# from keras.layers import Dense, Dropout, Lambda, Input, Subtract, Add\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.utils import to_categorical\n",
    "# from keras import losses\n",
    "# from keras import initializers, regularizers\n",
    "# import keras.backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Lambda, Input, Subtract, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from scipy import stats\n",
    "import collections\n",
    "#from treatments import hypertension_treatments, dm_treatment, ascvd_treatment, hypertension_treatments_history, dm_treatment_history\n",
    "#from ascvd_risk import framingham_ascvd_risk\n",
    "diagnosis_reward = False\n",
    "EPISODES = 20000\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "hypertension_treatments = {'antihypertensives,_ace_inhibitors', 'calcium_channel_blocking_agents',\n",
    "                           'beta-adrenergic_blocking_agents', 'angiotensin_receptor_antag.-thiazide_diuretic_comb',\n",
    "                           'antihypertensives,_angiotensin_receptor_antagonist', 'thiazide_and_related_diuretics',\n",
    "                           'potassium_sparing_diuretics_in_combination', 'alpha/beta-adrenergic_blocking_agents',\n",
    "                           'antihyperlipid-_hmg-coa_ri-calcium_channel_blocker',\n",
    "                           'ace_inhibitor-thiazide_or_thiazide-like_diuretic',\n",
    "                           'angioten.receptr_antag-calcium_chanl_blkr-thiazide',\n",
    "                           'angiotensin_receptor_blockr-calcium_channel_blockr',\n",
    "                           'miotics_and_other_intraocular_pressure_reducers',\n",
    "                           'renin_inhibitor,direct-angiotensin_receptr_antagon',\n",
    "                           'angiotensin_recept-neprilysin_inhibitor_comb(arni)',\n",
    "                           'beta-blockers_and_thiazide,thiazide-like_diuretics', 'bulk_chemicals',\n",
    "                           'renin_inhibitor,direct_and_thiazide_diuretic_comb',\n",
    "                           'anti-inflammatory,_interleukin-1_beta_blockers'}\n",
    "\n",
    "dm_treatment = {'antihyperglycemic,_biguanide_type', 'insulins',\n",
    "                'antihyperglycemic,insulin-release_stim.-biguanide',\n",
    "                'antihyperglycemic,_dpp-4_inhibitors',\n",
    "                'antihypergly,incretin_mimetic(glp-1_recep.agonist)',\n",
    "                'antihyperglycemic,_insulin-release_stimulant_type',\n",
    "                'antihyperglycemic,dpp-4_inhibitor-biguanide_combs.',\n",
    "                'antihyperglycemc-sod/gluc_cotransport2(sglt2)inhib',\n",
    "                'antihyperglycemic,_alpha-glucosidase_inhibitors',\n",
    "                'antihyperglycemic,_thiazolidinedione_and_biguanide',\n",
    "                'antihyperglycemic,thiazolidinedione(pparg_agonist)',\n",
    "                'antihyperglycemic_-_dopamine_receptor_agonists',\n",
    "                'antihyperglycemic-sglt2_inhibitor-biguanide_combs.',\n",
    "                'antihyperglycemic,_sglt-2_and_dpp-4_inhibitor_comb',\n",
    "                'antihyperglycemic,_thiazolidinedione-sulfonylurea',\n",
    "                'antihypergly,dpp-4_enzyme_inhib.-thiazolidinedione',\n",
    "                'antihypergly,insulin,long_act-glp-1_recept.agonist',\n",
    "                'antihyperglycemic,_amylin_analog-type'}\n",
    "\n",
    "ascvd_treatment = {'antihyperlip.hmg_coa_reduct_inhib-cholest.ab.inhib',\n",
    "                   'antihyperlipidemic-hmg_coa_reductase_inhib.-niacin',\n",
    "                   'antihyperlipidemic_-_hmg_coa_reductase_inhibitors',\n",
    "                   'antihyperlipidemic_-_pcsk9_inhibitors',\n",
    "                   'bile_salt_sequestrants',\n",
    "                   'lipotropics'}\n",
    "\n",
    "hypertension_treatments_history = ['antihypertensives,_ace_inhibitors_cur',\n",
    " 'calcium_channel_blocking_agents_cur',\n",
    " 'beta-adrenergic_blocking_agents_cur',\n",
    " 'angiotensin_receptor_antag.-thiazide_diuretic_comb_cur',\n",
    " 'antihypertensives,_angiotensin_receptor_antagonist_cur',\n",
    " 'thiazide_and_related_diuretics_cur',\n",
    " 'potassium_sparing_diuretics_in_combination_cur',\n",
    " 'alpha/beta-adrenergic_blocking_agents_cur',\n",
    " 'antihyperlipid-_hmg-coa_ri-calcium_channel_blocker_cur',\n",
    " 'ace_inhibitor-thiazide_or_thiazide-like_diuretic_cur',\n",
    " 'angioten.receptr_antag-calcium_chanl_blkr-thiazide_cur',\n",
    " 'angiotensin_receptor_blockr-calcium_channel_blockr_cur',\n",
    " 'miotics_and_other_intraocular_pressure_reducers_cur',\n",
    " 'renin_inhibitor,direct-angiotensin_receptr_antagon_cur',\n",
    " 'angiotensin_recept-neprilysin_inhibitor_comb(arni)_cur',\n",
    " 'beta-blockers_and_thiazide,thiazide-like_diuretics_cur',\n",
    " 'bulk_chemicals_cur',\n",
    " 'renin_inhibitor,direct_and_thiazide_diuretic_comb_cur',\n",
    " 'anti-inflammatory,_interleukin-1_beta_blockers_cur',\n",
    " 'antihypertensives,_ace_inhibitors_hist',\n",
    " 'calcium_channel_blocking_agents_hist',\n",
    " 'beta-adrenergic_blocking_agents_hist',\n",
    " 'angiotensin_receptor_antag.-thiazide_diuretic_comb_hist',\n",
    " 'antihypertensives,_angiotensin_receptor_antagonist_hist',\n",
    " 'thiazide_and_related_diuretics_hist',\n",
    " 'potassium_sparing_diuretics_in_combination_hist',\n",
    " 'alpha/beta-adrenergic_blocking_agents_hist',\n",
    " 'antihyperlipid-_hmg-coa_ri-calcium_channel_blocker_hist',\n",
    " 'ace_inhibitor-thiazide_or_thiazide-like_diuretic_hist',\n",
    " 'angioten.receptr_antag-calcium_chanl_blkr-thiazide_hist',\n",
    " 'angiotensin_receptor_blockr-calcium_channel_blockr_hist',\n",
    " 'miotics_and_other_intraocular_pressure_reducers_hist',\n",
    " 'renin_inhibitor,direct-angiotensin_receptr_antagon_hist',\n",
    " 'angiotensin_recept-neprilysin_inhibitor_comb(arni)_hist',\n",
    " 'beta-blockers_and_thiazide,thiazide-like_diuretics_hist',\n",
    " 'bulk_chemicals_hist',\n",
    " 'renin_inhibitor,direct_and_thiazide_diuretic_comb_hist',\n",
    " 'anti-inflammatory,_interleukin-1_beta_blockers_hist']\n",
    "\n",
    "dm_treatment_history = ['antihyperglycemic,_biguanide_type_cur',\n",
    " 'insulins_cur',\n",
    " 'antihyperglycemic,insulin-release_stim.-biguanide_cur',\n",
    " 'antihyperglycemic,_dpp-4_inhibitors_cur',\n",
    " 'antihypergly,incretin_mimetic(glp-1_recep.agonist)_cur',\n",
    " 'antihyperglycemic,_insulin-release_stimulant_type_cur',\n",
    " 'antihyperglycemic,dpp-4_inhibitor-biguanide_combs._cur',\n",
    " 'antihyperglycemc-sod/gluc_cotransport2(sglt2)inhib_cur',\n",
    " 'antihyperglycemic,_alpha-glucosidase_inhibitors_cur',\n",
    " 'antihyperglycemic,_thiazolidinedione_and_biguanide_cur',\n",
    " 'antihyperglycemic,thiazolidinedione(pparg_agonist)_cur',\n",
    " 'antihyperglycemic_-_dopamine_receptor_agonists_cur',\n",
    " 'antihyperglycemic-sglt2_inhibitor-biguanide_combs._cur',\n",
    " 'antihyperglycemic,_sglt-2_and_dpp-4_inhibitor_comb_cur',\n",
    " 'antihyperglycemic,_thiazolidinedione-sulfonylurea_cur',\n",
    " 'antihypergly,dpp-4_enzyme_inhib.-thiazolidinedione_cur',\n",
    " 'antihypergly,insulin,long_act-glp-1_recept.agonist_cur',\n",
    " 'antihyperglycemic,_amylin_analog-type_cur',\n",
    " 'antihyperglycemic,_biguanide_type_hist',\n",
    " 'insulins_hist',\n",
    " 'antihyperglycemic,insulin-release_stim.-biguanide_hist',\n",
    " 'antihyperglycemic,_dpp-4_inhibitors_hist',\n",
    " 'antihypergly,incretin_mimetic(glp-1_recep.agonist)_hist',\n",
    " 'antihyperglycemic,_insulin-release_stimulant_type_hist',\n",
    " 'antihyperglycemic,dpp-4_inhibitor-biguanide_combs._hist',\n",
    " 'antihyperglycemc-sod/gluc_cotransport2(sglt2)inhib_hist',\n",
    " 'antihyperglycemic,_alpha-glucosidase_inhibitors_hist',\n",
    " 'antihyperglycemic,_thiazolidinedione_and_biguanide_hist',\n",
    " 'antihyperglycemic,thiazolidinedione(pparg_agonist)_hist',\n",
    " 'antihyperglycemic_-_dopamine_receptor_agonists_hist',\n",
    " 'antihyperglycemic-sglt2_inhibitor-biguanide_combs._hist',\n",
    " 'antihyperglycemic,_sglt-2_and_dpp-4_inhibitor_comb_hist',\n",
    " 'antihyperglycemic,_thiazolidinedione-sulfonylurea_hist',\n",
    " 'antihypergly,dpp-4_enzyme_inhib.-thiazolidinedione_hist',\n",
    " 'antihypergly,insulin,long_act-glp-1_recept.agonist_hist',\n",
    " 'antihyperglycemic,_amylin_analog-type_hist']\n",
    "\n",
    "def framingham_ascvd_risk(x):\n",
    "    if x[0] == 0:\n",
    "        AgeFactor = 2.32888\n",
    "        TotalCholFactor = 1.20904\n",
    "        HDLCholFactor = -0.70833\n",
    "        if x[6]:\n",
    "            SysBPFactor = 2.82263\n",
    "        else:\n",
    "            SysBPFactor = 2.76157\n",
    "\n",
    "        if x[5] == 1.0:\n",
    "            Cig = 0.52873\n",
    "        else:\n",
    "            Cig = 0.0\n",
    "\n",
    "        if x[7]:\n",
    "            DM = 0.69154\n",
    "        else:\n",
    "            DM = 0.0\n",
    "        AvgRisk = 26.1931\n",
    "        RiskPeriodFactor = 0.95012\n",
    "    else:\n",
    "        AgeFactor = 3.06117\n",
    "        TotalCholFactor = 1.12370\n",
    "        HDLCholFactor = -0.93263\n",
    "        if x[6]:\n",
    "            SysBPFactor = 1.99881\n",
    "        else:\n",
    "            SysBPFactor = 1.93303\n",
    "        if x[5] == 1.0:\n",
    "            Cig = 0.65451\n",
    "        else:\n",
    "            Cig = 0.0\n",
    "\n",
    "        if x[7]:\n",
    "            DM = 0.57367\n",
    "        else:\n",
    "            DM = 0.0\n",
    "        AvgRisk = 23.9802\n",
    "        RiskPeriodFactor = 0.88936\n",
    "\n",
    "    risk_factors = (np.log(x[1]) * AgeFactor) + (np.log(x[2]) * TotalCholFactor) + (np.log(x[3]) * HDLCholFactor) + (\n",
    "                np.log(x[4]) * SysBPFactor) + Cig + DM - AvgRisk\n",
    "    risk = 100 * (1 - np.power(RiskPeriodFactor, np.exp(risk_factors)))\n",
    "    return (risk)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, targets, state_cols, reward_cols, next_state_cols, class_weights=None, dueling=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.targets = targets\n",
    "        self.state_cols = state_cols\n",
    "        self.reward_cols = reward_cols\n",
    "        self.next_state_cols = next_state_cols\n",
    "        self.dueling = dueling\n",
    "        self.class_weights = class_weights\n",
    "        self.hidden_layers = {'layers': [256, 512, 256], 'activation': ['tanh', 'tanh', 'tanh']}\n",
    "        self.gamma = 0.8  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate_decay = 0.01\n",
    "        self.learning_rate = 0.0004\n",
    "        self.l2_reg = 10e-5\n",
    "        self.model = Sequential()\n",
    "        self.target_model = Sequential()\n",
    "        self._build_model()\n",
    "        self._build_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        if self.dueling:\n",
    "            input_ = Input(shape=( self.state_size,))\n",
    "\n",
    "            for i in range(len(self.hidden_layers['layers'])):\n",
    "                if i is 0:\n",
    "                    x = Dense(self.hidden_layers['layers'][i], input_dim=self.state_size,\n",
    "                                         activation=self.hidden_layers['activation'][i])(input_)\n",
    "                    x = Dropout(0.5)(x)\n",
    "                else:\n",
    "                    x = Dense(self.hidden_layers['layers'][i], activation=self.hidden_layers['activation'][i])(x)\n",
    "                    x = Dropout(0.5)(x)\n",
    "\n",
    "            value = Dense(256, activation=\"relu\")(x)\n",
    "            value = Dense(1, activation=\"relu\")(value)\n",
    "            advantage = Dense(256, activation=\"relu\")(x)\n",
    "            advantage = Dense(self.action_size, activation=\"relu\")(advantage)\n",
    "            advantage_mean = Lambda(lambda x: K.mean(x, axis=1))(advantage)\n",
    "            advantage = Subtract()([advantage, advantage_mean])\n",
    "            out = Add()([value, advantage])\n",
    "\n",
    "            model = Model(inputs=input_, outputs=out)\n",
    "            model.compile(optimizer=Adam(lr=self.learning_rate), loss=losses.mse)\n",
    "            self.model = model\n",
    "        else:\n",
    "            for i in range(len(self.hidden_layers['layers'])):\n",
    "                if i is 0:\n",
    "                    self.model.add(Dense(self.hidden_layers['layers'][i], input_dim=self.state_size,\n",
    "                                         activation=self.hidden_layers['activation'][i]))\n",
    "                    self.model.add(Dropout(0.5))\n",
    "                else:\n",
    "                    self.model.add(Dense(self.hidden_layers['layers'][i], activation=self.hidden_layers['activation'][i]))\n",
    "                    self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(self.action_size, activation='linear'))\n",
    "            self.model.compile(optimizer=Adam(lr=self.learning_rate), loss=losses.mse)\n",
    "\n",
    "\n",
    "    def _build_target_model(self):\n",
    "        if self.dueling:\n",
    "            input_ = Input(shape=(self.state_size,))\n",
    "\n",
    "            for i in range(len(self.hidden_layers['layers'])):\n",
    "                if i is 0:\n",
    "                    x = Dense(self.hidden_layers['layers'][i], input_dim=self.state_size,\n",
    "                              activation=self.hidden_layers['activation'][i])(input_)\n",
    "                    x = Dropout(0.5)(x)\n",
    "                else:\n",
    "                    x = Dense(self.hidden_layers['layers'][i], activation=self.hidden_layers['activation'][i])(x)\n",
    "                    x = Dropout(0.5)(x)\n",
    "\n",
    "            value = Dense(256, activation=\"relu\")(x)\n",
    "            value = Dense(1, activation=\"relu\")(value)\n",
    "            advantage = Dense(256, activation=\"relu\")(x)\n",
    "            advantage = Dense(self.action_size, activation=\"relu\")(advantage)\n",
    "            advantage_mean = Lambda(lambda x: K.mean(x, axis=1))(advantage)\n",
    "            advantage = Subtract()([advantage, advantage_mean])\n",
    "            out = Add()([value, advantage])\n",
    "\n",
    "            model = Model(inputs=input_, outputs=out)\n",
    "            model.compile(optimizer=Adam(lr=self.learning_rate), loss=losses.mse)\n",
    "            self.target_model = model\n",
    "        else:\n",
    "            for i in range(len(self.hidden_layers['layers'])):\n",
    "                if i is 0:\n",
    "                    self.target_model.add(Dense(self.hidden_layers['layers'][i], input_dim=self.state_size,\n",
    "                                                activation=self.hidden_layers['activation'][i]))\n",
    "                    self.model.add(Dropout(0.5))\n",
    "                else:\n",
    "                    self.target_model.add(\n",
    "                        Dense(self.hidden_layers['layers'][i], activation=self.hidden_layers['activation'][i]))\n",
    "                    self.model.add(Dropout(0.5))\n",
    "            self.target_model.add(Dense(self.action_size, activation='linear'))\n",
    "            self.target_model.compile(optimizer=Adam(lr=self.learning_rate), loss=losses.mse)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def _hash_action(self, actions):\n",
    "        return sum([pow(2, i) for i in range(8) if actions[i] == 1])\n",
    "\n",
    "    def _to_categorical(self, x):\n",
    "        to_categorical(x, num_classes=self.action_size)\n",
    "\n",
    "    def replay(self, minibatch, DDQN):\n",
    "        states = np.array(minibatch[self.state_cols])\n",
    "        targets_f = self.model.predict(states)\n",
    "        for idx in range(minibatch.shape[0]):\n",
    "            next_state = np.reshape(minibatch[self.next_state_cols].iloc[idx].tolist(), [1, state_size])\n",
    "            reward = minibatch[self.reward_cols].iloc[idx].values[0]\n",
    "            action = np.argmax(minibatch[self.targets].iloc[\n",
    "                                   idx].tolist())  # self._hash_action(minibatch[self.targets].iloc[idx].tolist())\n",
    "            done = np.isnan(next_state[0, 0])\n",
    "            if not DDQN:\n",
    "                # Vanilla DQN\n",
    "                target = reward + self.gamma * np.max(self.model.predict(next_state)[0]) * np.invert(done)\n",
    "            else:\n",
    "                # Double DQN\n",
    "                action_index = np.argmax(self.model.predict(next_state)[0])\n",
    "                target = reward + self.gamma * self.target_model.predict(next_state)[0][action_index] * np.invert(done)\n",
    "\n",
    "            targets_f[idx][action] = target\n",
    "            # Filtering out states and targets for training\n",
    "\n",
    "        history = self.model.fit(states, targets_f, epochs=1, verbose=0, class_weight=self.class_weights)\n",
    "        # Keeping track of loss\n",
    "        loss = history.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss\n",
    "\n",
    "    def prioritize(self, state, next_state, action, reward, done, alpha=0.6):\n",
    "        q_next = reward + self.discount_factor * np.max(self.predict(next_state)[0])\n",
    "        q = self.predict(state)[0][action]\n",
    "        p = (np.abs(q_next-q)+ (np.e ** -10)) ** alpha\n",
    "        self.priority.append(p)\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def get_priority_experience_batch(self):\n",
    "        p_sum = np.sum(self.priority)\n",
    "        prob = self.priority / p_sum\n",
    "        sample_indices = random.choices(range(len(prob)), k=self.batch_size, weights=prob)\n",
    "        importance = (1/prob) * (1/len(self.priority))\n",
    "        importance = np.array(importance)[sample_indices]\n",
    "        samples = np.array(self.memory)[sample_indices]\n",
    "        return samples, importance\n",
    "\n",
    "    def _replay(self):\n",
    "        \"\"\"\n",
    "        experience replay. find the q-value and train the neural network model with state as input and q-values as targets\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        batch, importance = self.get_priority_experience_batch()\n",
    "        for b, i in zip(batch, importance):\n",
    "            state, next_state, action, reward, done = b\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.discount_factor * np.max(self.predict(next_state)[0])\n",
    "            final_target = self.predict(state)\n",
    "            final_target[0][action] = target\n",
    "            imp = i ** (1-self.epsilon)\n",
    "            imp = np.reshape(imp, 1)\n",
    "            self.fit(state, final_target, imp)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "    def model_loss(self):\n",
    "        \"\"\"\" Wrapper function which calculates auxiliary values for the complete loss function.\n",
    "         Returns a *function* which calculates the complete loss given only the input and target output \"\"\"\n",
    "        # KL loss\n",
    "        kl_loss = self.calculate_kl_loss\n",
    "        # Reconstruction loss\n",
    "        md_loss_func = self.calculate_md_loss\n",
    "\n",
    "        # KL weight (to be used by total loss and by annealing scheduler)\n",
    "        self.kl_weight = K.variable(self.hps['kl_weight_start'], name='kl_weight')\n",
    "        kl_weight = self.kl_weight\n",
    "\n",
    "        def seq2seq_loss(y_true, y_pred):\n",
    "            \"\"\" Final loss calculation function to be passed to optimizer\"\"\"\n",
    "            # Reconstruction loss\n",
    "            md_loss = md_loss_func(y_true, y_pred)\n",
    "            # Full loss\n",
    "            model_loss = kl_weight * kl_loss() + md_loss\n",
    "            return model_loss\n",
    "\n",
    "        return seq2seq_loss\n",
    "\n",
    "def hash_to_action(x):\n",
    "    return int(''.join(map(str, x)))\n",
    "    # return sum([int(x[i]) * (2**i) for i in range(len(x))])\n",
    "\n",
    "def findOccurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "def decode_to_treatment(x, labels):\n",
    "    gap = len(labels) - len(x)\n",
    "    index_temp = findOccurrences(x, '1')\n",
    "    output = [labels[idx + gap] for idx in index_temp]\n",
    "    return output\n",
    "\n",
    "def map_to_disease_category(x, disease, label_cols):\n",
    "    for t in range(len(x)):\n",
    "        if x[t] == 1 and label_cols[t] in treatment_ctg[disease]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "all_treamments = hypertension_treatments.union(dm_treatment).union(ascvd_treatment)\n",
    "cur_next_treatments = set([\"next_\" + i for i in all_treamments]).union(all_treamments)\n",
    "\n",
    "treatment_ctg = {'hypertension': hypertension_treatments, 'diabetes': dm_treatment, 'ascvd': ascvd_treatment}\n",
    "\n",
    "target_treatment = 'ascvd'\n",
    "action_size = 100\n",
    "\n",
    "excluded = ['study_id', 'encounter_dt_ran']\n",
    "\n",
    "data_path = '/kaggle/input/cleaned-ehr-treatment-v6/cleaned_EHR_treatment_param_lab_test_final_3diseases-v6.csv'\n",
    "# '~/Research/PHD/project/Hua Zheng/previous code/cleaned_EHR_treatment_param_lab_test_final_3diseases_cvd_encounter_diagnosis.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "if diagnosis_reward:\n",
    "    data['reward'] = data['reward_diagnosis']\n",
    "    data.drop('reward_diagnosis', inplace=True, axis=1)\n",
    "\n",
    "#%% weight different rewards\n",
    "reward_weight = [0, 1, 0]\n",
    "data['reward'] = data.apply(lambda x: (x['reward_bp'] * reward_weight[0] + x['reward_ascvd'] * reward_weight[1] + x['reward_diabetes'] * reward_weight[2]) / sum(reward_weight), axis = 1) # (data[['reward_bp']] * 2 + data[['reward_ascvd']] + data[['reward_ascvd']] * 2) / 5#, 'reward_ascvd', 'reward_diabetes'\n",
    "#    data = data.drop(['egfr_mdrd_african_american_min', 'egfr_mdrd_african_american_max', 'egfr_mdrd_african_american', 'egfr_mdrd_non_african_american', 'egfr_mdrd_non_african_american_max','egfr_mdrd_non_african_american_min', 'next_egfr_mdrd_african_american','next_egfr_mdrd_african_american_max','next_egfr_mdrd_african_american_min', 'next_egfr_mdrd_non_african_american','next_egfr_mdrd_non_african_american_max', 'next_egfr_mdrd_non_african_american_min', 'bulk_chemicals_hist', 'next_bulk_chemicals_hist'],axis=1)\n",
    "data = data.drop(['egfr_mdrd_african_american_min', 'egfr_mdrd_african_american_max', 'egfr_mdrd_african_american',\n",
    "                  'egfr_mdrd_non_african_american', 'egfr_mdrd_non_african_american_max',\n",
    "                  'egfr_mdrd_non_african_american_min', 'next_egfr_mdrd_african_american',\n",
    "                  'next_egfr_mdrd_african_american_max', 'next_egfr_mdrd_african_american_min',\n",
    "                  'next_egfr_mdrd_non_african_american', 'next_egfr_mdrd_non_african_american_max',\n",
    "                  'next_egfr_mdrd_non_african_american_min'], axis=1)\n",
    "data = data.dropna()\n",
    "\n",
    "if diagnosis_reward:\n",
    "    label_cols = list(data.columns[116:159])\n",
    "else:\n",
    "    label_cols = list(data.columns[117:160])\n",
    "\n",
    "full_label_cols = label_cols\n",
    "if target_treatment != None:\n",
    "    label_cols = treatment_ctg[target_treatment]\n",
    "    action_size = len(label_cols)\n",
    "\n",
    "data = data.loc[data[label_cols].sum(axis=1, skipna=True) != 0,]\n",
    "target = data[label_cols].apply(lambda x: hash_to_action(x), axis=1)\n",
    "data['target'] = target\n",
    "counter = collections.Counter(target)\n",
    "target_set = list([i[0] for i in counter.most_common(action_size)])\n",
    "target_replacement = dict(zip(iter(target_set), range(action_size)))\n",
    "target_column_renames = ['target' + str(i) for i in range(action_size)]\n",
    "data = data[data.target.apply(lambda x: x in target_set)]\n",
    "data['target'] = data['target'].replace(target_replacement)\n",
    "reward_cols = ['reward']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_state_cols = ['next_creatinine',\n",
    "                   'next_bmi',\n",
    "                   'next_hemoglobin_a1c',\n",
    "                   'next_antihyperlipidemic-hmg_coa_reductase_inhib.-niacin_hist',\n",
    "                   'next_hemoglobin_a1c_max',\n",
    "                   'next_bp_diastolic_min',\n",
    "                   'next_antihyperglycemic,_sglt-2_and_dpp-4_inhibitor_comb_cur',\n",
    "                   'next_antihyperglycemic,_dpp-4_inhibitors_hist',\n",
    "                   'next_race_white',\n",
    "                   'next_antihyperglycemc-sod/gluc_cotransport2(sglt2)inhib_hist',\n",
    "                   'next_ldl_cholesterol_min',\n",
    "                   'next_antihyperlip.hmg_coa_reduct_inhib-cholest.ab.inhib_cur',\n",
    "                   # 'next_egfr_mdrd_non_african_american_min',\n",
    "                   'next_alpha/beta-adrenergic_blocking_agents_cur',\n",
    "                   'next_antihyperglycemic,insulin-release_stim.-biguanide_cur',\n",
    "                   'next_antihypergly,insulin,long_act-glp-1_recept.agonist_cur',\n",
    "                   'next_angioten.receptr_antag-calcium_chanl_blkr-thiazide_cur',\n",
    "                   'next_anti-inflammatory,_interleukin-1_beta_blockers_cur',\n",
    "                   'next_beta-blockers_and_thiazide,thiazide-like_diuretics_hist',\n",
    "                   'next_antihyperglycemic,_thiazolidinedione_and_biguanide_hist',\n",
    "                   'next_antihypertensives,_ace_inhibitors_cur',\n",
    "                   # 'next_egfr_mdrd_african_american',\n",
    "                   # 'next_egfr_mdrd_non_african_american',\n",
    "                   'next_race_native_hawaiian_or_other_pacific_islander',\n",
    "                   'next_antihypertensives,_angiotensin_receptor_antagonist_hist',\n",
    "                   'next_antihyperglycemic,_insulin-release_stimulant_type_cur',\n",
    "                   'next_antihypergly,incretin_mimetic(glp-1_recep.agonist)_cur',\n",
    "                   'next_antihyperglycemic,insulin-release_stim.-biguanide_hist',\n",
    "                   'next_angiotensin_receptor_blockr-calcium_channel_blockr_hist',\n",
    "                   'next_antihyperglycemic,_biguanide_type_cur',\n",
    "                   'next_antihyperglycemic-sglt2_inhibitor-biguanide_combs._cur',\n",
    "                   'next_creatinine_min',\n",
    "                   'next_bmi_min',\n",
    "                   # 'next_egfr_mdrd_african_american_max',\n",
    "                   'next_antihyperglycemic_-_dopamine_receptor_agonists_cur',\n",
    "                   'next_beta-adrenergic_blocking_agents_hist',\n",
    "                   'next_antihyperglycemic_-_dopamine_receptor_agonists_hist',\n",
    "                   'next_insulins_cur',\n",
    "                   'next_race_native_american',\n",
    "                   'next_hdl_cholesterol',\n",
    "                   'next_race_multiple_race',\n",
    "                   'next_antihyperglycemic,_sglt-2_and_dpp-4_inhibitor_comb_hist',\n",
    "                   'next_antihyperlipidemic_-_hmg_coa_reductase_inhibitors_cur',\n",
    "                   'next_renin_inhibitor,direct-angiotensin_receptr_antagon_hist',\n",
    "                   'next_anti-inflammatory,_interleukin-1_beta_blockers_hist',\n",
    "                   'next_angioten.receptr_antag-calcium_chanl_blkr-thiazide_hist',\n",
    "                   'next_antihyperlipidemic-hmg_coa_reductase_inhib.-niacin_cur',\n",
    "                   'next_triglycerides_max',\n",
    "                   'next_antihyperglycemic-sglt2_inhibitor-biguanide_combs._hist',\n",
    "                   'next_hemoglobin_a1c_min',\n",
    "                   'next_triglycerides',\n",
    "                   # 'next_egfr_mdrd_non_african_american_max',\n",
    "                   'next_miotics_and_other_intraocular_pressure_reducers_hist',\n",
    "                   'next_potassium_sparing_diuretics_in_combination_cur',\n",
    "                   'next_bmi_max',\n",
    "                   'next_beta-adrenergic_blocking_agents_cur',\n",
    "                   'next_race_other',\n",
    "                   'next_time_last_vist',\n",
    "                   'next_antihyperglycemic,_thiazolidinedione_and_biguanide_cur',\n",
    "                   'next_bp_systolic',\n",
    "                   'next_bile_salt_sequestrants_cur',\n",
    "                   'next_antihyperglycemic,dpp-4_inhibitor-biguanide_combs._cur',\n",
    "                   'next_angiotensin_recept-neprilysin_inhibitor_comb(arni)_cur',\n",
    "                   'next_race_patient_refused',\n",
    "                   'next_antihyperglycemic,_amylin_analog-type_cur',\n",
    "                   'next_antihyperglycemic,_alpha-glucosidase_inhibitors_cur',\n",
    "                   'next_antihypergly,dpp-4_enzyme_inhib.-thiazolidinedione_cur',\n",
    "                   'next_renin_inhibitor,direct_and_thiazide_diuretic_comb_cur',\n",
    "                   'next_angiotensin_receptor_antag.-thiazide_diuretic_comb_hist',\n",
    "                   'next_antihyperglycemic,thiazolidinedione(pparg_agonist)_cur',\n",
    "                   'next_cholesterol,_total',\n",
    "                   'next_antihyperlipid-_hmg-coa_ri-calcium_channel_blocker_cur',\n",
    "                   'next_thiazide_and_related_diuretics_hist',\n",
    "                   'next_calcium_channel_blocking_agents_hist',\n",
    "                   'next_hdl_cholesterol_max',\n",
    "                   'next_bulk_chemicals_hist',\n",
    "                   'next_antihyperlipidemic_-_hmg_coa_reductase_inhibitors_hist',\n",
    "                   'next_potassium_sparing_diuretics_in_combination_hist',\n",
    "                   'next_hdl_cholesterol_min',\n",
    "                   # 'next_egfr_mdrd_african_american_min',\n",
    "                   'next_race_asian',\n",
    "                   'next_cholesterol,_total_max',\n",
    "                   'next_antihyperlipid-_hmg-coa_ri-calcium_channel_blocker_hist',\n",
    "                   'next_ace_inhibitor-thiazide_or_thiazide-like_diuretic_cur',\n",
    "                   'next_ace_inhibitor-thiazide_or_thiazide-like_diuretic_hist',\n",
    "                   'next_antihyperglycemic,_alpha-glucosidase_inhibitors_hist',\n",
    "                   'next_antihyperglycemic,dpp-4_inhibitor-biguanide_combs._hist',\n",
    "                   'next_lipotropics_hist',\n",
    "                   'next_antihypergly,dpp-4_enzyme_inhib.-thiazolidinedione_hist',\n",
    "                   'next_cholesterol,_total_min',\n",
    "                   'next_smoke',\n",
    "                   'next_ldl_cholesterol_max',\n",
    "                   'next_alpha/beta-adrenergic_blocking_agents_hist',\n",
    "                   'next_antihyperglycemic,_insulin-release_stimulant_type_hist',\n",
    "                   'next_antihyperlipidemic_-_pcsk9_inhibitors_hist',\n",
    "                   'next_antihypertensives,_angiotensin_receptor_antagonist_cur',\n",
    "                   'next_renin_inhibitor,direct_and_thiazide_diuretic_comb_hist',\n",
    "                   'next_bulk_chemicals_cur',\n",
    "                   'next_angiotensin_receptor_antag.-thiazide_diuretic_comb_cur',\n",
    "                   'next_antihyperglycemc-sod/gluc_cotransport2(sglt2)inhib_cur',\n",
    "                   'next_antihyperglycemic,thiazolidinedione(pparg_agonist)_hist',\n",
    "                   'next_bile_salt_sequestrants_hist',\n",
    "                   # 'next_reward',\n",
    "                   'next_creatinine_max',\n",
    "                   'next_renin_inhibitor,direct-angiotensin_receptr_antagon_cur',\n",
    "                   'next_angiotensin_receptor_blockr-calcium_channel_blockr_cur',\n",
    "                   'next_antihyperglycemic,_thiazolidinedione-sulfonylurea_cur',\n",
    "                   'next_antihyperlip.hmg_coa_reduct_inhib-cholest.ab.inhib_hist',\n",
    "                   'next_antihyperglycemic,_biguanide_type_hist',\n",
    "                   'next_sex_male',\n",
    "                   'next_antihyperglycemic,_amylin_analog-type_hist',\n",
    "                   'next_race_unknown',\n",
    "                   'next_antihypertensives,_ace_inhibitors_hist',\n",
    "                   'next_antihyperglycemic,_thiazolidinedione-sulfonylurea_hist',\n",
    "                   'next_antihyperglycemic,_dpp-4_inhibitors_cur',\n",
    "                   'next_beta-blockers_and_thiazide,thiazide-like_diuretics_cur',\n",
    "                   'next_miotics_and_other_intraocular_pressure_reducers_cur',\n",
    "                   'next_triglycerides_min',\n",
    "                   'next_bp_diastolic',\n",
    "                   'next_bp_systolic_max',\n",
    "                   'next_bp_systolic_min',\n",
    "                   'next_age',\n",
    "                   'next_calcium_channel_blocking_agents_cur',\n",
    "                   'next_antihyperlipidemic_-_pcsk9_inhibitors_cur',\n",
    "                   'next_ldl_cholesterol',\n",
    "                   'next_antihypergly,insulin,long_act-glp-1_recept.agonist_hist',\n",
    "                   'next_insulins_hist',\n",
    "                   'next_thiazide_and_related_diuretics_cur',\n",
    "                   'next_angiotensin_recept-neprilysin_inhibitor_comb(arni)_hist',\n",
    "                   'next_lipotropics_cur',\n",
    "                   'next_antihypergly,incretin_mimetic(glp-1_recep.agonist)_hist',\n",
    "                   'next_bp_diastolic_max',\n",
    "                   'next_time_to_first_visit']\n",
    "state_cols = [s[5:] for s in next_state_cols]\n",
    "# state_cols = list(set(data.columns) - set(full_label_cols) - {'target'} - set(\n",
    "#     ['reward', 'reward_bp', 'reward_ascvd', 'reward_diabetes', 'risk_ascvd', 'next_risk_ascvd']) - set(\n",
    "#     ['study_id', 'encounter_dt_ran']) - set(next_state_cols))\n",
    "if diagnosis_reward:\n",
    "    state_cols = state_cols - (['CVD', 'days_to_CVD'])\n",
    "\n",
    "# next_state_cols = ['next_' + s for s in list(state_cols)]\n",
    "patients_column = data[['study_id', 'encounter_dt_ran']]\n",
    "#data = data.drop(excluded, axis=1)\n",
    "_temp = data.drop('target', axis=1).max(skipna=True) - data.drop('target', axis=1).min(skipna=True)\n",
    "_temp[data.drop('target', axis=1).columns[(_temp == 0).values]] = 1.0\n",
    "normalized_df = (data.drop(['target'], axis=1) - data.drop('target', axis=1).min(skipna=True)) / _temp\n",
    "\n",
    "# constant_variable_df = data.drop('target', axis=1).max(skipna=True) - data.drop('target', axis=1).min(skipna=True)\n",
    "# dropped_state = normalized_df.columns[(constant_variable_df == 0).values]\n",
    "# normalized_df[dropped_state] = data[dropped_state]\n",
    "# normalized_df['reward'] = (data['reward'] - data['reward'].min(skipna=True))/(data['reward'].std()*2)\n",
    "# next_state_cols = set(next_state_cols) - set(dropped_state)\n",
    "# state_cols = set(state_cols) - set(dropped_state)\n",
    "ohe = to_categorical(data['target'], action_size)\n",
    "\n",
    "for i, col in enumerate(target_column_renames):\n",
    "    normalized_df[col] = ohe[:, i]\n",
    "normalized_df[['study_id', 'encounter_dt_ran']] = patients_column\n",
    "train, test = train_test_split(normalized_df, test_size=0.4, random_state=2019)\n",
    "train2, test2 = train_test_split(data, test_size=0.4, random_state=2019)\n",
    "\n",
    "state_size = len(state_cols)\n",
    "action_size = len(target_column_renames)\n",
    "# loss_weights = loss_weight(data)\n",
    "#class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(data['target'].values),\n",
    "#                                                  data['target'].values)\n",
    "agent = DQNAgent(state_size, action_size, target_column_renames, state_cols, reward_cols, next_state_cols)#, class_weights, True)\n",
    "#agent.load(\"./model/3d:256-512-256-episodes:20000--dqn-mse-target_treatment:{}.h5\".format(target_treatment))\n",
    "done = False\n",
    "batch_size = 64\n",
    "cur_val_error = 0\n",
    "episodes_till_target_update = 100\n",
    "interested_train = train  # train[train[targets].apply(lambda x: sum(x) > 0, axis=1)]\n",
    "interested_tests = test  # test[test[targets].apply(lambda x: sum(x) > 0, axis=1)]\n",
    "sample_patient = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/20000, loss: 2.7728093, test_acc: 0.0553\n",
      "episode: 10/20000, loss: 2.7548699, test_acc: 0.0553\n",
      "episode: 20/20000, loss: 1.7978199, test_acc: 0.0553\n",
      "episode: 30/20000, loss: 3.0415230, test_acc: 0.0553\n",
      "episode: 40/20000, loss: 3.0688909, test_acc: 0.0553\n",
      "episode: 50/20000, loss: 2.1194037, test_acc: 0.0553\n",
      "episode: 60/20000, loss: 1.2825356, test_acc: 0.0553\n",
      "episode: 70/20000, loss: 1.1207505, test_acc: 0.0553\n",
      "episode: 80/20000, loss: 1.9501070, test_acc: 0.0553\n",
      "episode: 90/20000, loss: 1.7698300, test_acc: 0.0553\n",
      "episode: 100/20000, loss: 1.4021145, test_acc: 0.0553\n",
      "episode: 110/20000, loss: 1.1607285, test_acc: 0.0553\n",
      "episode: 120/20000, loss: 0.8137211, test_acc: 0.0553\n",
      "episode: 130/20000, loss: 1.1347305, test_acc: 0.0553\n",
      "episode: 140/20000, loss: 0.7962626, test_acc: 0.0553\n",
      "episode: 150/20000, loss: 1.0082760, test_acc: 0.0553\n",
      "episode: 160/20000, loss: 0.7728631, test_acc: 0.0553\n",
      "episode: 170/20000, loss: 1.4407499, test_acc: 0.0553\n",
      "episode: 180/20000, loss: 0.8584723, test_acc: 0.0553\n",
      "episode: 190/20000, loss: 1.0887264, test_acc: 0.0553\n",
      "episode: 200/20000, loss: 1.2443020, test_acc: 0.0553\n",
      "episode: 210/20000, loss: 0.8132551, test_acc: 0.0553\n",
      "episode: 220/20000, loss: 1.0930327, test_acc: 0.0553\n",
      "episode: 230/20000, loss: 0.9972665, test_acc: 0.0553\n",
      "episode: 240/20000, loss: 0.7744371, test_acc: 0.0553\n",
      "episode: 250/20000, loss: 0.7747605, test_acc: 0.0553\n",
      "episode: 260/20000, loss: 0.6153282, test_acc: 0.0553\n",
      "episode: 270/20000, loss: 0.6956132, test_acc: 0.0553\n",
      "episode: 280/20000, loss: 0.8244190, test_acc: 0.0553\n",
      "episode: 290/20000, loss: 0.5936741, test_acc: 0.0553\n",
      "episode: 300/20000, loss: 0.6956120, test_acc: 0.0553\n",
      "episode: 310/20000, loss: 0.6292564, test_acc: 0.0553\n",
      "episode: 320/20000, loss: 0.6430800, test_acc: 0.0553\n",
      "episode: 330/20000, loss: 0.5214374, test_acc: 0.0553\n",
      "episode: 340/20000, loss: 0.7519684, test_acc: 0.0553\n",
      "episode: 350/20000, loss: 0.4392638, test_acc: 0.0553\n",
      "episode: 360/20000, loss: 0.6120543, test_acc: 0.0553\n",
      "episode: 370/20000, loss: 0.6490718, test_acc: 0.0553\n",
      "episode: 380/20000, loss: 0.4750867, test_acc: 0.0553\n",
      "episode: 390/20000, loss: 0.5744016, test_acc: 0.0553\n",
      "episode: 400/20000, loss: 0.6970663, test_acc: 0.0553\n",
      "episode: 410/20000, loss: 0.6152188, test_acc: 0.0553\n",
      "episode: 420/20000, loss: 0.3607776, test_acc: 0.0553\n",
      "episode: 430/20000, loss: 0.5163796, test_acc: 0.0553\n",
      "episode: 440/20000, loss: 0.6956704, test_acc: 0.0553\n",
      "episode: 450/20000, loss: 0.4369117, test_acc: 0.0553\n",
      "episode: 460/20000, loss: 0.5188920, test_acc: 0.0553\n",
      "episode: 470/20000, loss: 0.3310502, test_acc: 0.0553\n",
      "episode: 480/20000, loss: 0.4915087, test_acc: 0.0553\n",
      "episode: 490/20000, loss: 0.4555556, test_acc: 0.0553\n",
      "episode: 500/20000, loss: 0.5578975, test_acc: 0.7459\n",
      "episode: 510/20000, loss: 0.4707820, test_acc: 0.7459\n",
      "episode: 520/20000, loss: 0.4014713, test_acc: 0.7459\n",
      "episode: 530/20000, loss: 0.5035090, test_acc: 0.7459\n",
      "episode: 540/20000, loss: 0.2869418, test_acc: 0.7459\n",
      "episode: 550/20000, loss: 0.3094514, test_acc: 0.7459\n",
      "episode: 560/20000, loss: 0.3298834, test_acc: 0.7459\n",
      "episode: 570/20000, loss: 0.2958995, test_acc: 0.7459\n",
      "episode: 580/20000, loss: 0.3641111, test_acc: 0.7459\n",
      "episode: 590/20000, loss: 0.3156559, test_acc: 0.7459\n",
      "episode: 600/20000, loss: 0.3494650, test_acc: 0.7459\n",
      "episode: 610/20000, loss: 0.2383077, test_acc: 0.7459\n",
      "episode: 620/20000, loss: 0.4360868, test_acc: 0.7459\n",
      "episode: 630/20000, loss: 0.2285854, test_acc: 0.7459\n",
      "episode: 640/20000, loss: 0.4550188, test_acc: 0.7459\n",
      "episode: 650/20000, loss: 0.3517753, test_acc: 0.7459\n",
      "episode: 660/20000, loss: 0.3964011, test_acc: 0.7459\n",
      "episode: 670/20000, loss: 0.3054273, test_acc: 0.7459\n",
      "episode: 680/20000, loss: 0.2417924, test_acc: 0.7459\n",
      "episode: 690/20000, loss: 0.3050812, test_acc: 0.7459\n",
      "episode: 700/20000, loss: 0.1905048, test_acc: 0.7459\n",
      "episode: 710/20000, loss: 0.2897546, test_acc: 0.7459\n",
      "episode: 720/20000, loss: 0.2954783, test_acc: 0.7459\n",
      "episode: 730/20000, loss: 0.3816992, test_acc: 0.7459\n",
      "episode: 740/20000, loss: 0.2025406, test_acc: 0.7459\n",
      "episode: 750/20000, loss: 0.2201872, test_acc: 0.7459\n",
      "episode: 760/20000, loss: 0.2536975, test_acc: 0.7459\n",
      "episode: 770/20000, loss: 0.2045418, test_acc: 0.7459\n",
      "episode: 780/20000, loss: 0.2175721, test_acc: 0.7459\n",
      "episode: 790/20000, loss: 0.3330975, test_acc: 0.7459\n",
      "episode: 800/20000, loss: 0.1720153, test_acc: 0.7459\n",
      "episode: 810/20000, loss: 0.2848897, test_acc: 0.7459\n",
      "episode: 820/20000, loss: 0.1692233, test_acc: 0.7459\n",
      "episode: 830/20000, loss: 0.1915812, test_acc: 0.7459\n",
      "episode: 840/20000, loss: 0.2236920, test_acc: 0.7459\n",
      "episode: 850/20000, loss: 0.2025570, test_acc: 0.7459\n",
      "episode: 860/20000, loss: 0.2384109, test_acc: 0.7459\n",
      "episode: 870/20000, loss: 0.1614053, test_acc: 0.7459\n",
      "episode: 880/20000, loss: 0.1932105, test_acc: 0.7459\n",
      "episode: 890/20000, loss: 0.2125564, test_acc: 0.7459\n",
      "episode: 900/20000, loss: 0.1990132, test_acc: 0.7459\n",
      "episode: 910/20000, loss: 0.2020549, test_acc: 0.7459\n",
      "episode: 920/20000, loss: 0.2586557, test_acc: 0.7459\n",
      "episode: 930/20000, loss: 0.1514019, test_acc: 0.7459\n",
      "episode: 940/20000, loss: 0.1515594, test_acc: 0.7459\n",
      "episode: 950/20000, loss: 0.2063248, test_acc: 0.7459\n",
      "episode: 960/20000, loss: 0.2375504, test_acc: 0.7459\n",
      "episode: 970/20000, loss: 0.1679780, test_acc: 0.7459\n",
      "episode: 980/20000, loss: 0.2004662, test_acc: 0.7459\n",
      "episode: 990/20000, loss: 0.1734763, test_acc: 0.7459\n",
      "episode: 1000/20000, loss: 0.1830451, test_acc: 0.8735\n",
      "episode: 1010/20000, loss: 0.1371942, test_acc: 0.8735\n",
      "episode: 1020/20000, loss: 0.1401771, test_acc: 0.8735\n",
      "episode: 1030/20000, loss: 0.1485257, test_acc: 0.8735\n",
      "episode: 1040/20000, loss: 0.1317461, test_acc: 0.8735\n",
      "episode: 1050/20000, loss: 0.1410792, test_acc: 0.8735\n",
      "episode: 1060/20000, loss: 0.1407330, test_acc: 0.8735\n",
      "episode: 1070/20000, loss: 0.1461096, test_acc: 0.8735\n",
      "episode: 1080/20000, loss: 0.1607464, test_acc: 0.8735\n",
      "episode: 1090/20000, loss: 0.1074570, test_acc: 0.8735\n",
      "episode: 1100/20000, loss: 0.1271082, test_acc: 0.8735\n",
      "episode: 1110/20000, loss: 0.1717443, test_acc: 0.8735\n",
      "episode: 1120/20000, loss: 0.1315560, test_acc: 0.8735\n",
      "episode: 1130/20000, loss: 0.1235763, test_acc: 0.8735\n",
      "episode: 1140/20000, loss: 0.1435300, test_acc: 0.8735\n",
      "episode: 1150/20000, loss: 0.1371012, test_acc: 0.8735\n",
      "episode: 1160/20000, loss: 0.1846505, test_acc: 0.8735\n",
      "episode: 1170/20000, loss: 0.1589314, test_acc: 0.8735\n",
      "episode: 1180/20000, loss: 0.1402821, test_acc: 0.8735\n",
      "episode: 1190/20000, loss: 0.1213513, test_acc: 0.8735\n",
      "episode: 1200/20000, loss: 0.1845147, test_acc: 0.8735\n",
      "episode: 1210/20000, loss: 0.1413401, test_acc: 0.8735\n",
      "episode: 1220/20000, loss: 0.1801194, test_acc: 0.8735\n",
      "episode: 1230/20000, loss: 0.1150976, test_acc: 0.8735\n",
      "episode: 1240/20000, loss: 0.1634776, test_acc: 0.8735\n",
      "episode: 1250/20000, loss: 0.1760993, test_acc: 0.8735\n",
      "episode: 1260/20000, loss: 0.1215253, test_acc: 0.8735\n",
      "episode: 1270/20000, loss: 0.1455708, test_acc: 0.8735\n",
      "episode: 1280/20000, loss: 0.1256323, test_acc: 0.8735\n",
      "episode: 1290/20000, loss: 0.1135108, test_acc: 0.8735\n",
      "episode: 1300/20000, loss: 0.1094953, test_acc: 0.8735\n",
      "episode: 1310/20000, loss: 0.1194579, test_acc: 0.8735\n",
      "episode: 1320/20000, loss: 0.1221827, test_acc: 0.8735\n",
      "episode: 1330/20000, loss: 0.1548086, test_acc: 0.8735\n",
      "episode: 1340/20000, loss: 0.1243799, test_acc: 0.8735\n",
      "episode: 1350/20000, loss: 0.1399015, test_acc: 0.8735\n",
      "episode: 1360/20000, loss: 0.1292706, test_acc: 0.8735\n",
      "episode: 1370/20000, loss: 0.1127041, test_acc: 0.8735\n",
      "episode: 1380/20000, loss: 0.0972918, test_acc: 0.8735\n",
      "episode: 1390/20000, loss: 0.1392540, test_acc: 0.8735\n",
      "episode: 1400/20000, loss: 0.1317323, test_acc: 0.8735\n",
      "episode: 1410/20000, loss: 0.1069597, test_acc: 0.8735\n",
      "episode: 1420/20000, loss: 0.1024036, test_acc: 0.8735\n",
      "episode: 1430/20000, loss: 0.1293761, test_acc: 0.8735\n",
      "episode: 1440/20000, loss: 0.1037793, test_acc: 0.8735\n",
      "episode: 1450/20000, loss: 0.1041145, test_acc: 0.8735\n",
      "episode: 1460/20000, loss: 0.1140426, test_acc: 0.8735\n",
      "episode: 1470/20000, loss: 0.1174158, test_acc: 0.8735\n",
      "episode: 1480/20000, loss: 0.1414106, test_acc: 0.8735\n",
      "episode: 1490/20000, loss: 0.1025581, test_acc: 0.8735\n",
      "episode: 1500/20000, loss: 0.1065665, test_acc: 0.9204\n",
      "episode: 1510/20000, loss: 0.0940438, test_acc: 0.9204\n",
      "episode: 1520/20000, loss: 0.0764994, test_acc: 0.9204\n",
      "episode: 1530/20000, loss: 0.1060265, test_acc: 0.9204\n",
      "episode: 1540/20000, loss: 0.0991279, test_acc: 0.9204\n",
      "episode: 1550/20000, loss: 0.0866678, test_acc: 0.9204\n",
      "episode: 1560/20000, loss: 0.1102012, test_acc: 0.9204\n",
      "episode: 1570/20000, loss: 0.0925273, test_acc: 0.9204\n",
      "episode: 1580/20000, loss: 0.0801410, test_acc: 0.9204\n",
      "episode: 1590/20000, loss: 0.1064488, test_acc: 0.9204\n",
      "episode: 1600/20000, loss: 0.1039884, test_acc: 0.9204\n",
      "episode: 1610/20000, loss: 0.1142076, test_acc: 0.9204\n",
      "episode: 1620/20000, loss: 0.0954444, test_acc: 0.9204\n",
      "episode: 1630/20000, loss: 0.0855039, test_acc: 0.9204\n",
      "episode: 1640/20000, loss: 0.0863553, test_acc: 0.9204\n",
      "episode: 1650/20000, loss: 0.0877123, test_acc: 0.9204\n",
      "episode: 1660/20000, loss: 0.0834624, test_acc: 0.9204\n",
      "episode: 1670/20000, loss: 0.0829070, test_acc: 0.9204\n",
      "episode: 1680/20000, loss: 0.0883038, test_acc: 0.9204\n",
      "episode: 1690/20000, loss: 0.0718714, test_acc: 0.9204\n",
      "episode: 1700/20000, loss: 0.0774955, test_acc: 0.9204\n",
      "episode: 1710/20000, loss: 0.0867164, test_acc: 0.9204\n",
      "episode: 1720/20000, loss: 0.0893170, test_acc: 0.9204\n",
      "episode: 1730/20000, loss: 0.0781059, test_acc: 0.9204\n",
      "episode: 1740/20000, loss: 0.0757845, test_acc: 0.9204\n",
      "episode: 1750/20000, loss: 0.0733919, test_acc: 0.9204\n",
      "episode: 1760/20000, loss: 0.0813622, test_acc: 0.9204\n",
      "episode: 1770/20000, loss: 0.0756264, test_acc: 0.9204\n",
      "episode: 1780/20000, loss: 0.0818902, test_acc: 0.9204\n",
      "episode: 1790/20000, loss: 0.0879166, test_acc: 0.9204\n",
      "episode: 1800/20000, loss: 0.0824690, test_acc: 0.9204\n",
      "episode: 1810/20000, loss: 0.0792981, test_acc: 0.9204\n",
      "episode: 1820/20000, loss: 0.0745346, test_acc: 0.9204\n",
      "episode: 1830/20000, loss: 0.0803621, test_acc: 0.9204\n",
      "episode: 1840/20000, loss: 0.0703252, test_acc: 0.9204\n",
      "episode: 1850/20000, loss: 0.0832441, test_acc: 0.9204\n",
      "episode: 1860/20000, loss: 0.0831013, test_acc: 0.9204\n",
      "episode: 1870/20000, loss: 0.0720316, test_acc: 0.9204\n",
      "episode: 1880/20000, loss: 0.0580431, test_acc: 0.9204\n",
      "episode: 1890/20000, loss: 0.0765904, test_acc: 0.9204\n",
      "episode: 1900/20000, loss: 0.0853291, test_acc: 0.9204\n",
      "episode: 1910/20000, loss: 0.0742872, test_acc: 0.9204\n",
      "episode: 1920/20000, loss: 0.0697476, test_acc: 0.9204\n",
      "episode: 1930/20000, loss: 0.0754429, test_acc: 0.9204\n",
      "episode: 1940/20000, loss: 0.0788610, test_acc: 0.9204\n",
      "episode: 1950/20000, loss: 0.0773151, test_acc: 0.9204\n",
      "episode: 1960/20000, loss: 0.0694780, test_acc: 0.9204\n",
      "episode: 1970/20000, loss: 0.0770796, test_acc: 0.9204\n",
      "episode: 1980/20000, loss: 0.0664235, test_acc: 0.9204\n",
      "episode: 1990/20000, loss: 0.0764814, test_acc: 0.9204\n",
      "episode: 2000/20000, loss: 0.0686926, test_acc: 0.9174\n",
      "episode: 2010/20000, loss: 0.0728223, test_acc: 0.9174\n",
      "episode: 2020/20000, loss: 0.0671296, test_acc: 0.9174\n",
      "episode: 2030/20000, loss: 0.0699387, test_acc: 0.9174\n",
      "episode: 2040/20000, loss: 0.0735369, test_acc: 0.9174\n",
      "episode: 2050/20000, loss: 0.0843414, test_acc: 0.9174\n",
      "episode: 2060/20000, loss: 0.0680846, test_acc: 0.9174\n",
      "episode: 2070/20000, loss: 0.0782106, test_acc: 0.9174\n",
      "episode: 2080/20000, loss: 0.0693530, test_acc: 0.9174\n",
      "episode: 2090/20000, loss: 0.0693035, test_acc: 0.9174\n",
      "episode: 2100/20000, loss: 0.0669528, test_acc: 0.9174\n",
      "episode: 2110/20000, loss: 0.0735144, test_acc: 0.9174\n",
      "episode: 2120/20000, loss: 0.0715363, test_acc: 0.9174\n",
      "episode: 2130/20000, loss: 0.0641491, test_acc: 0.9174\n",
      "episode: 2140/20000, loss: 0.0623666, test_acc: 0.9174\n",
      "episode: 2150/20000, loss: 0.0651307, test_acc: 0.9174\n",
      "episode: 2160/20000, loss: 0.0673725, test_acc: 0.9174\n",
      "episode: 2170/20000, loss: 0.0740432, test_acc: 0.9174\n",
      "episode: 2180/20000, loss: 0.0683772, test_acc: 0.9174\n",
      "episode: 2190/20000, loss: 0.0666382, test_acc: 0.9174\n",
      "episode: 2200/20000, loss: 0.0643407, test_acc: 0.9174\n",
      "episode: 2210/20000, loss: 0.0689138, test_acc: 0.9174\n",
      "episode: 2220/20000, loss: 0.0633900, test_acc: 0.9174\n",
      "episode: 2230/20000, loss: 0.0650730, test_acc: 0.9174\n",
      "episode: 2240/20000, loss: 0.0631725, test_acc: 0.9174\n",
      "episode: 2250/20000, loss: 0.0632362, test_acc: 0.9174\n",
      "episode: 2260/20000, loss: 0.0598203, test_acc: 0.9174\n",
      "episode: 2270/20000, loss: 0.0616460, test_acc: 0.9174\n",
      "episode: 2280/20000, loss: 0.0624961, test_acc: 0.9174\n",
      "episode: 2290/20000, loss: 0.0569883, test_acc: 0.9174\n",
      "episode: 2300/20000, loss: 0.0599942, test_acc: 0.9174\n",
      "episode: 2310/20000, loss: 0.0669089, test_acc: 0.9174\n",
      "episode: 2320/20000, loss: 0.0599756, test_acc: 0.9174\n",
      "episode: 2330/20000, loss: 0.0593333, test_acc: 0.9174\n",
      "episode: 2340/20000, loss: 0.0663956, test_acc: 0.9174\n",
      "episode: 2350/20000, loss: 0.0627467, test_acc: 0.9174\n",
      "episode: 2360/20000, loss: 0.0660670, test_acc: 0.9174\n",
      "episode: 2370/20000, loss: 0.0623945, test_acc: 0.9174\n",
      "episode: 2380/20000, loss: 0.0628916, test_acc: 0.9174\n",
      "episode: 2390/20000, loss: 0.0592887, test_acc: 0.9174\n",
      "episode: 2400/20000, loss: 0.0578516, test_acc: 0.9174\n",
      "episode: 2410/20000, loss: 0.0638657, test_acc: 0.9174\n",
      "episode: 2420/20000, loss: 0.0658180, test_acc: 0.9174\n",
      "episode: 2430/20000, loss: 0.0548275, test_acc: 0.9174\n",
      "episode: 2440/20000, loss: 0.0580765, test_acc: 0.9174\n",
      "episode: 2450/20000, loss: 0.0613164, test_acc: 0.9174\n",
      "episode: 2460/20000, loss: 0.0553838, test_acc: 0.9174\n",
      "episode: 2470/20000, loss: 0.0594662, test_acc: 0.9174\n",
      "episode: 2480/20000, loss: 0.0575661, test_acc: 0.9174\n",
      "episode: 2490/20000, loss: 0.0545293, test_acc: 0.9174\n",
      "episode: 2500/20000, loss: 0.0597443, test_acc: 0.9156\n",
      "episode: 2510/20000, loss: 0.0575941, test_acc: 0.9156\n",
      "episode: 2520/20000, loss: 0.0579415, test_acc: 0.9156\n",
      "episode: 2530/20000, loss: 0.0565625, test_acc: 0.9156\n",
      "episode: 2540/20000, loss: 0.0570291, test_acc: 0.9156\n",
      "episode: 2550/20000, loss: 0.0637606, test_acc: 0.9156\n",
      "episode: 2560/20000, loss: 0.0588704, test_acc: 0.9156\n",
      "episode: 2570/20000, loss: 0.0559097, test_acc: 0.9156\n",
      "episode: 2580/20000, loss: 0.0576714, test_acc: 0.9156\n",
      "episode: 2590/20000, loss: 0.0586145, test_acc: 0.9156\n",
      "episode: 2600/20000, loss: 0.0566560, test_acc: 0.9156\n",
      "episode: 2610/20000, loss: 0.0583513, test_acc: 0.9156\n",
      "episode: 2620/20000, loss: 0.0574686, test_acc: 0.9156\n",
      "episode: 2630/20000, loss: 0.0590754, test_acc: 0.9156\n",
      "episode: 2640/20000, loss: 0.0552527, test_acc: 0.9156\n",
      "episode: 2650/20000, loss: 0.0601719, test_acc: 0.9156\n",
      "episode: 2660/20000, loss: 0.0588017, test_acc: 0.9156\n",
      "episode: 2670/20000, loss: 0.0557800, test_acc: 0.9156\n",
      "episode: 2680/20000, loss: 0.0574582, test_acc: 0.9156\n",
      "episode: 2690/20000, loss: 0.0593237, test_acc: 0.9156\n",
      "episode: 2700/20000, loss: 0.0554133, test_acc: 0.9156\n",
      "episode: 2710/20000, loss: 0.0577824, test_acc: 0.9156\n",
      "episode: 2720/20000, loss: 0.0545783, test_acc: 0.9156\n",
      "episode: 2730/20000, loss: 0.0565888, test_acc: 0.9156\n",
      "episode: 2740/20000, loss: 0.0577427, test_acc: 0.9156\n",
      "episode: 2750/20000, loss: 0.0558416, test_acc: 0.9156\n",
      "episode: 2760/20000, loss: 0.0514803, test_acc: 0.9156\n",
      "episode: 2770/20000, loss: 0.0560340, test_acc: 0.9156\n",
      "episode: 2780/20000, loss: 0.0568096, test_acc: 0.9156\n",
      "episode: 2790/20000, loss: 0.0527272, test_acc: 0.9156\n",
      "episode: 2800/20000, loss: 0.0575900, test_acc: 0.9156\n",
      "episode: 2810/20000, loss: 0.0587174, test_acc: 0.9156\n",
      "episode: 2820/20000, loss: 0.0584401, test_acc: 0.9156\n",
      "episode: 2830/20000, loss: 0.0524374, test_acc: 0.9156\n",
      "episode: 2840/20000, loss: 0.0578839, test_acc: 0.9156\n",
      "episode: 2850/20000, loss: 0.0558222, test_acc: 0.9156\n",
      "episode: 2860/20000, loss: 0.0563115, test_acc: 0.9156\n",
      "episode: 2870/20000, loss: 0.0519414, test_acc: 0.9156\n",
      "episode: 2880/20000, loss: 0.0584264, test_acc: 0.9156\n",
      "episode: 2890/20000, loss: 0.0560409, test_acc: 0.9156\n",
      "episode: 2900/20000, loss: 0.0544782, test_acc: 0.9156\n",
      "episode: 2910/20000, loss: 0.0540577, test_acc: 0.9156\n",
      "episode: 2920/20000, loss: 0.0555524, test_acc: 0.9156\n",
      "episode: 2930/20000, loss: 0.0568538, test_acc: 0.9156\n",
      "episode: 2940/20000, loss: 0.0532853, test_acc: 0.9156\n",
      "episode: 2950/20000, loss: 0.0549129, test_acc: 0.9156\n",
      "episode: 2960/20000, loss: 0.0550158, test_acc: 0.9156\n",
      "episode: 2970/20000, loss: 0.0526221, test_acc: 0.9156\n",
      "episode: 2980/20000, loss: 0.0518423, test_acc: 0.9156\n",
      "episode: 2990/20000, loss: 0.0550749, test_acc: 0.9156\n",
      "episode: 3000/20000, loss: 0.0511568, test_acc: 0.9134\n",
      "episode: 3010/20000, loss: 0.0542500, test_acc: 0.9134\n",
      "episode: 3020/20000, loss: 0.0561770, test_acc: 0.9134\n",
      "episode: 3030/20000, loss: 0.0544195, test_acc: 0.9134\n",
      "episode: 3040/20000, loss: 0.0569755, test_acc: 0.9134\n",
      "episode: 3050/20000, loss: 0.0511151, test_acc: 0.9134\n",
      "episode: 3060/20000, loss: 0.0585160, test_acc: 0.9134\n",
      "episode: 3070/20000, loss: 0.0584206, test_acc: 0.9134\n",
      "episode: 3080/20000, loss: 0.0609974, test_acc: 0.9134\n",
      "episode: 3090/20000, loss: 0.0551389, test_acc: 0.9134\n",
      "episode: 3100/20000, loss: 0.0515400, test_acc: 0.9134\n",
      "episode: 3110/20000, loss: 0.0540029, test_acc: 0.9134\n",
      "episode: 3120/20000, loss: 0.0559261, test_acc: 0.9134\n",
      "episode: 3130/20000, loss: 0.0560791, test_acc: 0.9134\n",
      "episode: 3140/20000, loss: 0.0541446, test_acc: 0.9134\n",
      "episode: 3150/20000, loss: 0.0538935, test_acc: 0.9134\n",
      "episode: 3160/20000, loss: 0.0557108, test_acc: 0.9134\n",
      "episode: 3170/20000, loss: 0.0554523, test_acc: 0.9134\n",
      "episode: 3180/20000, loss: 0.0574990, test_acc: 0.9134\n",
      "episode: 3190/20000, loss: 0.0543537, test_acc: 0.9134\n",
      "episode: 3200/20000, loss: 0.0524149, test_acc: 0.9134\n",
      "episode: 3210/20000, loss: 0.0590025, test_acc: 0.9134\n",
      "episode: 3220/20000, loss: 0.0556855, test_acc: 0.9134\n",
      "episode: 3230/20000, loss: 0.0552406, test_acc: 0.9134\n",
      "episode: 3240/20000, loss: 0.0553038, test_acc: 0.9134\n",
      "episode: 3250/20000, loss: 0.0555535, test_acc: 0.9134\n",
      "episode: 3260/20000, loss: 0.0562008, test_acc: 0.9134\n",
      "episode: 3270/20000, loss: 0.0510556, test_acc: 0.9134\n",
      "episode: 3280/20000, loss: 0.0585473, test_acc: 0.9134\n",
      "episode: 3290/20000, loss: 0.0561399, test_acc: 0.9134\n",
      "episode: 3300/20000, loss: 0.0515651, test_acc: 0.9134\n",
      "episode: 3310/20000, loss: 0.0541923, test_acc: 0.9134\n",
      "episode: 3320/20000, loss: 0.0550410, test_acc: 0.9134\n",
      "episode: 3330/20000, loss: 0.0543718, test_acc: 0.9134\n",
      "episode: 3340/20000, loss: 0.0565917, test_acc: 0.9134\n",
      "episode: 3350/20000, loss: 0.0577799, test_acc: 0.9134\n",
      "episode: 3360/20000, loss: 0.0555088, test_acc: 0.9134\n",
      "episode: 3370/20000, loss: 0.0584906, test_acc: 0.9134\n",
      "episode: 3380/20000, loss: 0.0550237, test_acc: 0.9134\n",
      "episode: 3390/20000, loss: 0.0555467, test_acc: 0.9134\n",
      "episode: 3400/20000, loss: 0.0488065, test_acc: 0.9134\n",
      "episode: 3410/20000, loss: 0.0541802, test_acc: 0.9134\n",
      "episode: 3420/20000, loss: 0.0545240, test_acc: 0.9134\n",
      "episode: 3430/20000, loss: 0.0541897, test_acc: 0.9134\n",
      "episode: 3440/20000, loss: 0.0547297, test_acc: 0.9134\n",
      "episode: 3450/20000, loss: 0.0503212, test_acc: 0.9134\n",
      "episode: 3460/20000, loss: 0.0508792, test_acc: 0.9134\n",
      "episode: 3470/20000, loss: 0.0532302, test_acc: 0.9134\n",
      "episode: 3480/20000, loss: 0.0526173, test_acc: 0.9134\n",
      "episode: 3490/20000, loss: 0.0525663, test_acc: 0.9134\n",
      "episode: 3500/20000, loss: 0.0565708, test_acc: 0.9301\n",
      "episode: 3510/20000, loss: 0.0551585, test_acc: 0.9301\n",
      "episode: 3520/20000, loss: 0.0565982, test_acc: 0.9301\n",
      "episode: 3530/20000, loss: 0.0520110, test_acc: 0.9301\n",
      "episode: 3540/20000, loss: 0.0600265, test_acc: 0.9301\n",
      "episode: 3550/20000, loss: 0.0539729, test_acc: 0.9301\n",
      "episode: 3560/20000, loss: 0.0575773, test_acc: 0.9301\n",
      "episode: 3570/20000, loss: 0.0575155, test_acc: 0.9301\n",
      "episode: 3580/20000, loss: 0.0568573, test_acc: 0.9301\n",
      "episode: 3590/20000, loss: 0.0524623, test_acc: 0.9301\n",
      "episode: 3600/20000, loss: 0.0584580, test_acc: 0.9301\n",
      "episode: 3610/20000, loss: 0.0545220, test_acc: 0.9301\n",
      "episode: 3620/20000, loss: 0.0563755, test_acc: 0.9301\n",
      "episode: 3630/20000, loss: 0.0519719, test_acc: 0.9301\n",
      "episode: 3640/20000, loss: 0.0520144, test_acc: 0.9301\n",
      "episode: 3650/20000, loss: 0.0578628, test_acc: 0.9301\n",
      "episode: 3660/20000, loss: 0.0580382, test_acc: 0.9301\n",
      "episode: 3670/20000, loss: 0.0550930, test_acc: 0.9301\n",
      "episode: 3680/20000, loss: 0.0566284, test_acc: 0.9301\n",
      "episode: 3690/20000, loss: 0.0563275, test_acc: 0.9301\n",
      "episode: 3700/20000, loss: 0.0532654, test_acc: 0.9301\n",
      "episode: 3710/20000, loss: 0.0517216, test_acc: 0.9301\n",
      "episode: 3720/20000, loss: 0.0547776, test_acc: 0.9301\n",
      "episode: 3730/20000, loss: 0.0526659, test_acc: 0.9301\n",
      "episode: 3740/20000, loss: 0.0560149, test_acc: 0.9301\n",
      "episode: 3750/20000, loss: 0.0489204, test_acc: 0.9301\n",
      "episode: 3760/20000, loss: 0.0565191, test_acc: 0.9301\n",
      "episode: 3770/20000, loss: 0.0510232, test_acc: 0.9301\n",
      "episode: 3780/20000, loss: 0.0547013, test_acc: 0.9301\n",
      "episode: 3790/20000, loss: 0.0578696, test_acc: 0.9301\n",
      "episode: 3800/20000, loss: 0.0494364, test_acc: 0.9301\n",
      "episode: 3810/20000, loss: 0.0538946, test_acc: 0.9301\n",
      "episode: 3820/20000, loss: 0.0577681, test_acc: 0.9301\n",
      "episode: 3830/20000, loss: 0.0570078, test_acc: 0.9301\n",
      "episode: 3840/20000, loss: 0.0555323, test_acc: 0.9301\n",
      "episode: 3850/20000, loss: 0.0546509, test_acc: 0.9301\n",
      "episode: 3860/20000, loss: 0.0502926, test_acc: 0.9301\n",
      "episode: 3870/20000, loss: 0.0500311, test_acc: 0.9301\n",
      "episode: 3880/20000, loss: 0.0531426, test_acc: 0.9301\n",
      "episode: 3890/20000, loss: 0.0550760, test_acc: 0.9301\n",
      "episode: 3900/20000, loss: 0.0528552, test_acc: 0.9301\n",
      "episode: 3910/20000, loss: 0.0554082, test_acc: 0.9301\n",
      "episode: 3920/20000, loss: 0.0542053, test_acc: 0.9301\n",
      "episode: 3930/20000, loss: 0.0545687, test_acc: 0.9301\n",
      "episode: 3940/20000, loss: 0.0506141, test_acc: 0.9301\n",
      "episode: 3950/20000, loss: 0.0559685, test_acc: 0.9301\n",
      "episode: 3960/20000, loss: 0.0572354, test_acc: 0.9301\n",
      "episode: 3970/20000, loss: 0.0563356, test_acc: 0.9301\n",
      "episode: 3980/20000, loss: 0.0537648, test_acc: 0.9301\n",
      "episode: 3990/20000, loss: 0.0560065, test_acc: 0.9301\n",
      "episode: 4000/20000, loss: 0.0575355, test_acc: 0.9316\n",
      "episode: 4010/20000, loss: 0.0521985, test_acc: 0.9316\n",
      "episode: 4020/20000, loss: 0.0541006, test_acc: 0.9316\n",
      "episode: 4030/20000, loss: 0.0538147, test_acc: 0.9316\n",
      "episode: 4040/20000, loss: 0.0591082, test_acc: 0.9316\n",
      "episode: 4050/20000, loss: 0.0519331, test_acc: 0.9316\n",
      "episode: 4060/20000, loss: 0.0529408, test_acc: 0.9316\n",
      "episode: 4070/20000, loss: 0.0532449, test_acc: 0.9316\n",
      "episode: 4080/20000, loss: 0.0568474, test_acc: 0.9316\n",
      "episode: 4090/20000, loss: 0.0508655, test_acc: 0.9316\n",
      "episode: 4100/20000, loss: 0.0516509, test_acc: 0.9316\n",
      "episode: 4110/20000, loss: 0.0561203, test_acc: 0.9316\n",
      "episode: 4120/20000, loss: 0.0577351, test_acc: 0.9316\n",
      "episode: 4130/20000, loss: 0.0536068, test_acc: 0.9316\n",
      "episode: 4140/20000, loss: 0.0533032, test_acc: 0.9316\n",
      "episode: 4150/20000, loss: 0.0549063, test_acc: 0.9316\n",
      "episode: 4160/20000, loss: 0.0544765, test_acc: 0.9316\n",
      "episode: 4170/20000, loss: 0.0566547, test_acc: 0.9316\n",
      "episode: 4180/20000, loss: 0.0506926, test_acc: 0.9316\n",
      "episode: 4190/20000, loss: 0.0579008, test_acc: 0.9316\n",
      "episode: 4200/20000, loss: 0.0546215, test_acc: 0.9316\n",
      "episode: 4210/20000, loss: 0.0602650, test_acc: 0.9316\n",
      "episode: 4220/20000, loss: 0.0526378, test_acc: 0.9316\n",
      "episode: 4230/20000, loss: 0.0592337, test_acc: 0.9316\n",
      "episode: 4240/20000, loss: 0.0587426, test_acc: 0.9316\n",
      "episode: 4250/20000, loss: 0.0557711, test_acc: 0.9316\n",
      "episode: 4260/20000, loss: 0.0507012, test_acc: 0.9316\n",
      "episode: 4270/20000, loss: 0.0544173, test_acc: 0.9316\n",
      "episode: 4280/20000, loss: 0.0565891, test_acc: 0.9316\n",
      "episode: 4290/20000, loss: 0.0553359, test_acc: 0.9316\n",
      "episode: 4300/20000, loss: 0.0582769, test_acc: 0.9316\n",
      "episode: 4310/20000, loss: 0.0570453, test_acc: 0.9316\n",
      "episode: 4320/20000, loss: 0.0539733, test_acc: 0.9316\n",
      "episode: 4330/20000, loss: 0.0513875, test_acc: 0.9316\n",
      "episode: 4340/20000, loss: 0.0518380, test_acc: 0.9316\n",
      "episode: 4350/20000, loss: 0.0541204, test_acc: 0.9316\n",
      "episode: 4360/20000, loss: 0.0524212, test_acc: 0.9316\n",
      "episode: 4370/20000, loss: 0.0556052, test_acc: 0.9316\n",
      "episode: 4380/20000, loss: 0.0543279, test_acc: 0.9316\n",
      "episode: 4390/20000, loss: 0.0562183, test_acc: 0.9316\n",
      "episode: 4400/20000, loss: 0.0526929, test_acc: 0.9316\n",
      "episode: 4410/20000, loss: 0.0550261, test_acc: 0.9316\n",
      "episode: 4420/20000, loss: 0.0560726, test_acc: 0.9316\n",
      "episode: 4430/20000, loss: 0.0531602, test_acc: 0.9316\n",
      "episode: 4440/20000, loss: 0.0543443, test_acc: 0.9316\n",
      "episode: 4450/20000, loss: 0.0559753, test_acc: 0.9316\n",
      "episode: 4460/20000, loss: 0.0489450, test_acc: 0.9316\n",
      "episode: 4470/20000, loss: 0.0550960, test_acc: 0.9316\n",
      "episode: 4480/20000, loss: 0.0521051, test_acc: 0.9316\n",
      "episode: 4490/20000, loss: 0.0564915, test_acc: 0.9316\n",
      "episode: 4500/20000, loss: 0.0535046, test_acc: 0.9496\n",
      "episode: 4510/20000, loss: 0.0588060, test_acc: 0.9496\n",
      "episode: 4520/20000, loss: 0.0547620, test_acc: 0.9496\n",
      "episode: 4530/20000, loss: 0.0551536, test_acc: 0.9496\n",
      "episode: 4540/20000, loss: 0.0549045, test_acc: 0.9496\n",
      "episode: 4550/20000, loss: 0.0554790, test_acc: 0.9496\n",
      "episode: 4560/20000, loss: 0.0548987, test_acc: 0.9496\n",
      "episode: 4570/20000, loss: 0.0605468, test_acc: 0.9496\n",
      "episode: 4580/20000, loss: 0.0565355, test_acc: 0.9496\n",
      "episode: 4590/20000, loss: 0.0583843, test_acc: 0.9496\n",
      "episode: 4600/20000, loss: 0.0582828, test_acc: 0.9496\n",
      "episode: 4610/20000, loss: 0.0539395, test_acc: 0.9496\n",
      "episode: 4620/20000, loss: 0.0544078, test_acc: 0.9496\n",
      "episode: 4630/20000, loss: 0.0564217, test_acc: 0.9496\n",
      "episode: 4640/20000, loss: 0.0541606, test_acc: 0.9496\n",
      "episode: 4650/20000, loss: 0.0545835, test_acc: 0.9496\n",
      "episode: 4660/20000, loss: 0.0537369, test_acc: 0.9496\n",
      "episode: 4670/20000, loss: 0.0599931, test_acc: 0.9496\n",
      "episode: 4680/20000, loss: 0.0570945, test_acc: 0.9496\n",
      "episode: 4690/20000, loss: 0.0490283, test_acc: 0.9496\n",
      "episode: 4700/20000, loss: 0.0550902, test_acc: 0.9496\n",
      "episode: 4710/20000, loss: 0.0533403, test_acc: 0.9496\n",
      "episode: 4720/20000, loss: 0.0564966, test_acc: 0.9496\n",
      "episode: 4730/20000, loss: 0.0522071, test_acc: 0.9496\n",
      "episode: 4740/20000, loss: 0.0600383, test_acc: 0.9496\n",
      "episode: 4750/20000, loss: 0.0557773, test_acc: 0.9496\n",
      "episode: 4760/20000, loss: 0.0523324, test_acc: 0.9496\n",
      "episode: 4770/20000, loss: 0.0560928, test_acc: 0.9496\n",
      "episode: 4780/20000, loss: 0.0530605, test_acc: 0.9496\n",
      "episode: 4790/20000, loss: 0.0558037, test_acc: 0.9496\n",
      "episode: 4800/20000, loss: 0.0542811, test_acc: 0.9496\n",
      "episode: 4810/20000, loss: 0.0500798, test_acc: 0.9496\n",
      "episode: 4820/20000, loss: 0.0572034, test_acc: 0.9496\n",
      "episode: 4830/20000, loss: 0.0493703, test_acc: 0.9496\n",
      "episode: 4840/20000, loss: 0.0595165, test_acc: 0.9496\n",
      "episode: 4850/20000, loss: 0.0526115, test_acc: 0.9496\n",
      "episode: 4860/20000, loss: 0.0534867, test_acc: 0.9496\n",
      "episode: 4870/20000, loss: 0.0570137, test_acc: 0.9496\n",
      "episode: 4880/20000, loss: 0.0509294, test_acc: 0.9496\n",
      "episode: 4890/20000, loss: 0.0521128, test_acc: 0.9496\n",
      "episode: 4900/20000, loss: 0.0533211, test_acc: 0.9496\n",
      "episode: 4910/20000, loss: 0.0515634, test_acc: 0.9496\n",
      "episode: 4920/20000, loss: 0.0584445, test_acc: 0.9496\n",
      "episode: 4930/20000, loss: 0.0552589, test_acc: 0.9496\n",
      "episode: 4940/20000, loss: 0.0544835, test_acc: 0.9496\n",
      "episode: 4950/20000, loss: 0.0543740, test_acc: 0.9496\n",
      "episode: 4960/20000, loss: 0.0527890, test_acc: 0.9496\n",
      "episode: 4970/20000, loss: 0.0550776, test_acc: 0.9496\n",
      "episode: 4980/20000, loss: 0.0577448, test_acc: 0.9496\n",
      "episode: 4990/20000, loss: 0.0526725, test_acc: 0.9496\n",
      "episode: 5000/20000, loss: 0.0544425, test_acc: 0.9556\n",
      "episode: 5010/20000, loss: 0.0545536, test_acc: 0.9556\n",
      "episode: 5020/20000, loss: 0.0553344, test_acc: 0.9556\n",
      "episode: 5030/20000, loss: 0.0522289, test_acc: 0.9556\n",
      "episode: 5040/20000, loss: 0.0542686, test_acc: 0.9556\n",
      "episode: 5050/20000, loss: 0.0548128, test_acc: 0.9556\n",
      "episode: 5060/20000, loss: 0.0554775, test_acc: 0.9556\n",
      "episode: 5070/20000, loss: 0.0568183, test_acc: 0.9556\n",
      "episode: 5080/20000, loss: 0.0554879, test_acc: 0.9556\n",
      "episode: 5090/20000, loss: 0.0570721, test_acc: 0.9556\n",
      "episode: 5100/20000, loss: 0.0479877, test_acc: 0.9556\n",
      "episode: 5110/20000, loss: 0.0550264, test_acc: 0.9556\n",
      "episode: 5120/20000, loss: 0.0580314, test_acc: 0.9556\n",
      "episode: 5130/20000, loss: 0.0548869, test_acc: 0.9556\n",
      "episode: 5140/20000, loss: 0.0572327, test_acc: 0.9556\n",
      "episode: 5150/20000, loss: 0.0554049, test_acc: 0.9556\n",
      "episode: 5160/20000, loss: 0.0567580, test_acc: 0.9556\n",
      "episode: 5170/20000, loss: 0.0592619, test_acc: 0.9556\n",
      "episode: 5180/20000, loss: 0.0591816, test_acc: 0.9556\n",
      "episode: 5190/20000, loss: 0.0517274, test_acc: 0.9556\n",
      "episode: 5200/20000, loss: 0.0548659, test_acc: 0.9556\n",
      "episode: 5210/20000, loss: 0.0567369, test_acc: 0.9556\n",
      "episode: 5220/20000, loss: 0.0541744, test_acc: 0.9556\n",
      "episode: 5230/20000, loss: 0.0559223, test_acc: 0.9556\n",
      "episode: 5240/20000, loss: 0.0577907, test_acc: 0.9556\n",
      "episode: 5250/20000, loss: 0.0575597, test_acc: 0.9556\n",
      "episode: 5260/20000, loss: 0.0551327, test_acc: 0.9556\n",
      "episode: 5270/20000, loss: 0.0574371, test_acc: 0.9556\n",
      "episode: 5280/20000, loss: 0.0540259, test_acc: 0.9556\n",
      "episode: 5290/20000, loss: 0.0509057, test_acc: 0.9556\n",
      "episode: 5300/20000, loss: 0.0547567, test_acc: 0.9556\n",
      "episode: 5310/20000, loss: 0.0549263, test_acc: 0.9556\n",
      "episode: 5320/20000, loss: 0.0562427, test_acc: 0.9556\n",
      "episode: 5330/20000, loss: 0.0570678, test_acc: 0.9556\n",
      "episode: 5340/20000, loss: 0.0529166, test_acc: 0.9556\n",
      "episode: 5350/20000, loss: 0.0569738, test_acc: 0.9556\n",
      "episode: 5360/20000, loss: 0.0600387, test_acc: 0.9556\n",
      "episode: 5370/20000, loss: 0.0577345, test_acc: 0.9556\n",
      "episode: 5380/20000, loss: 0.0530713, test_acc: 0.9556\n",
      "episode: 5390/20000, loss: 0.0592532, test_acc: 0.9556\n",
      "episode: 5400/20000, loss: 0.0493363, test_acc: 0.9556\n",
      "episode: 5410/20000, loss: 0.0560393, test_acc: 0.9556\n",
      "episode: 5420/20000, loss: 0.0517428, test_acc: 0.9556\n",
      "episode: 5430/20000, loss: 0.0564501, test_acc: 0.9556\n",
      "episode: 5440/20000, loss: 0.0548722, test_acc: 0.9556\n",
      "episode: 5450/20000, loss: 0.0563349, test_acc: 0.9556\n",
      "episode: 5460/20000, loss: 0.0545108, test_acc: 0.9556\n",
      "episode: 5470/20000, loss: 0.0546945, test_acc: 0.9556\n",
      "episode: 5480/20000, loss: 0.0546090, test_acc: 0.9556\n",
      "episode: 5490/20000, loss: 0.0552563, test_acc: 0.9556\n",
      "episode: 5500/20000, loss: 0.0568190, test_acc: 0.9682\n",
      "episode: 5510/20000, loss: 0.0513803, test_acc: 0.9682\n",
      "episode: 5520/20000, loss: 0.0577078, test_acc: 0.9682\n",
      "episode: 5530/20000, loss: 0.0582618, test_acc: 0.9682\n",
      "episode: 5540/20000, loss: 0.0554275, test_acc: 0.9682\n",
      "episode: 5550/20000, loss: 0.0547001, test_acc: 0.9682\n",
      "episode: 5560/20000, loss: 0.0569000, test_acc: 0.9682\n",
      "episode: 5570/20000, loss: 0.0564884, test_acc: 0.9682\n",
      "episode: 5580/20000, loss: 0.0548094, test_acc: 0.9682\n",
      "episode: 5590/20000, loss: 0.0524255, test_acc: 0.9682\n",
      "episode: 5600/20000, loss: 0.0559078, test_acc: 0.9682\n",
      "episode: 5610/20000, loss: 0.0590931, test_acc: 0.9682\n",
      "episode: 5620/20000, loss: 0.0570690, test_acc: 0.9682\n",
      "episode: 5630/20000, loss: 0.0524947, test_acc: 0.9682\n",
      "episode: 5640/20000, loss: 0.0544081, test_acc: 0.9682\n",
      "episode: 5650/20000, loss: 0.0594263, test_acc: 0.9682\n",
      "episode: 5660/20000, loss: 0.0567705, test_acc: 0.9682\n",
      "episode: 5670/20000, loss: 0.0570688, test_acc: 0.9682\n",
      "episode: 5680/20000, loss: 0.0547004, test_acc: 0.9682\n",
      "episode: 5690/20000, loss: 0.0602956, test_acc: 0.9682\n",
      "episode: 5700/20000, loss: 0.0565118, test_acc: 0.9682\n",
      "episode: 5710/20000, loss: 0.0578240, test_acc: 0.9682\n",
      "episode: 5720/20000, loss: 0.0547579, test_acc: 0.9682\n",
      "episode: 5730/20000, loss: 0.0562164, test_acc: 0.9682\n",
      "episode: 5740/20000, loss: 0.0567200, test_acc: 0.9682\n",
      "episode: 5750/20000, loss: 0.0576813, test_acc: 0.9682\n",
      "episode: 5760/20000, loss: 0.0518350, test_acc: 0.9682\n",
      "episode: 5770/20000, loss: 0.0556387, test_acc: 0.9682\n",
      "episode: 5780/20000, loss: 0.0568230, test_acc: 0.9682\n",
      "episode: 5790/20000, loss: 0.0575720, test_acc: 0.9682\n",
      "episode: 5800/20000, loss: 0.0563404, test_acc: 0.9682\n",
      "episode: 5810/20000, loss: 0.0542690, test_acc: 0.9682\n",
      "episode: 5820/20000, loss: 0.0510935, test_acc: 0.9682\n",
      "episode: 5830/20000, loss: 0.0564276, test_acc: 0.9682\n",
      "episode: 5840/20000, loss: 0.0548300, test_acc: 0.9682\n",
      "episode: 5850/20000, loss: 0.0504741, test_acc: 0.9682\n",
      "episode: 5860/20000, loss: 0.0528082, test_acc: 0.9682\n",
      "episode: 5870/20000, loss: 0.0602975, test_acc: 0.9682\n",
      "episode: 5880/20000, loss: 0.0533881, test_acc: 0.9682\n",
      "episode: 5890/20000, loss: 0.0538321, test_acc: 0.9682\n",
      "episode: 5900/20000, loss: 0.0546335, test_acc: 0.9682\n",
      "episode: 5910/20000, loss: 0.0516591, test_acc: 0.9682\n",
      "episode: 5920/20000, loss: 0.0541482, test_acc: 0.9682\n",
      "episode: 5930/20000, loss: 0.0597434, test_acc: 0.9682\n",
      "episode: 5940/20000, loss: 0.0542694, test_acc: 0.9682\n",
      "episode: 5950/20000, loss: 0.0540853, test_acc: 0.9682\n",
      "episode: 5960/20000, loss: 0.0543885, test_acc: 0.9682\n",
      "episode: 5970/20000, loss: 0.0534344, test_acc: 0.9682\n",
      "episode: 5980/20000, loss: 0.0565367, test_acc: 0.9682\n",
      "episode: 5990/20000, loss: 0.0556479, test_acc: 0.9682\n",
      "episode: 6000/20000, loss: 0.0562318, test_acc: 0.9774\n",
      "episode: 6010/20000, loss: 0.0546801, test_acc: 0.9774\n",
      "episode: 6020/20000, loss: 0.0551038, test_acc: 0.9774\n",
      "episode: 6030/20000, loss: 0.0560305, test_acc: 0.9774\n",
      "episode: 6040/20000, loss: 0.0543749, test_acc: 0.9774\n",
      "episode: 6050/20000, loss: 0.0565709, test_acc: 0.9774\n",
      "episode: 6060/20000, loss: 0.0594544, test_acc: 0.9774\n",
      "episode: 6070/20000, loss: 0.0547257, test_acc: 0.9774\n",
      "episode: 6080/20000, loss: 0.0543337, test_acc: 0.9774\n",
      "episode: 6090/20000, loss: 0.0559905, test_acc: 0.9774\n",
      "episode: 6100/20000, loss: 0.0550400, test_acc: 0.9774\n",
      "episode: 6110/20000, loss: 0.0521374, test_acc: 0.9774\n",
      "episode: 6120/20000, loss: 0.0536704, test_acc: 0.9774\n",
      "episode: 6130/20000, loss: 0.0563870, test_acc: 0.9774\n",
      "episode: 6140/20000, loss: 0.0537633, test_acc: 0.9774\n",
      "episode: 6150/20000, loss: 0.0559963, test_acc: 0.9774\n",
      "episode: 6160/20000, loss: 0.0532681, test_acc: 0.9774\n",
      "episode: 6170/20000, loss: 0.0545920, test_acc: 0.9774\n",
      "episode: 6180/20000, loss: 0.0552786, test_acc: 0.9774\n",
      "episode: 6190/20000, loss: 0.0557860, test_acc: 0.9774\n",
      "episode: 6200/20000, loss: 0.0550306, test_acc: 0.9774\n",
      "episode: 6210/20000, loss: 0.0538693, test_acc: 0.9774\n",
      "episode: 6220/20000, loss: 0.0563376, test_acc: 0.9774\n",
      "episode: 6230/20000, loss: 0.0542355, test_acc: 0.9774\n",
      "episode: 6240/20000, loss: 0.0560850, test_acc: 0.9774\n",
      "episode: 6250/20000, loss: 0.0555709, test_acc: 0.9774\n",
      "episode: 6260/20000, loss: 0.0567829, test_acc: 0.9774\n",
      "episode: 6270/20000, loss: 0.0578770, test_acc: 0.9774\n",
      "episode: 6280/20000, loss: 0.0554047, test_acc: 0.9774\n",
      "episode: 6290/20000, loss: 0.0543339, test_acc: 0.9774\n",
      "episode: 6300/20000, loss: 0.0535269, test_acc: 0.9774\n",
      "episode: 6310/20000, loss: 0.0552929, test_acc: 0.9774\n",
      "episode: 6320/20000, loss: 0.0530679, test_acc: 0.9774\n",
      "episode: 6330/20000, loss: 0.0556551, test_acc: 0.9774\n",
      "episode: 6340/20000, loss: 0.0550514, test_acc: 0.9774\n",
      "episode: 6350/20000, loss: 0.0485350, test_acc: 0.9774\n",
      "episode: 6360/20000, loss: 0.0507361, test_acc: 0.9774\n",
      "episode: 6370/20000, loss: 0.0543594, test_acc: 0.9774\n",
      "episode: 6380/20000, loss: 0.0534715, test_acc: 0.9774\n",
      "episode: 6390/20000, loss: 0.0530143, test_acc: 0.9774\n",
      "episode: 6400/20000, loss: 0.0514484, test_acc: 0.9774\n",
      "episode: 6410/20000, loss: 0.0554820, test_acc: 0.9774\n",
      "episode: 6420/20000, loss: 0.0556095, test_acc: 0.9774\n",
      "episode: 6430/20000, loss: 0.0530547, test_acc: 0.9774\n",
      "episode: 6440/20000, loss: 0.0536672, test_acc: 0.9774\n",
      "episode: 6450/20000, loss: 0.0534394, test_acc: 0.9774\n",
      "episode: 6460/20000, loss: 0.0554552, test_acc: 0.9774\n",
      "episode: 6470/20000, loss: 0.0559702, test_acc: 0.9774\n",
      "episode: 6480/20000, loss: 0.0577178, test_acc: 0.9774\n",
      "episode: 6490/20000, loss: 0.0527933, test_acc: 0.9774\n",
      "episode: 6500/20000, loss: 0.0608140, test_acc: 0.9243\n",
      "episode: 6510/20000, loss: 0.0518170, test_acc: 0.9243\n",
      "episode: 6520/20000, loss: 0.0586237, test_acc: 0.9243\n",
      "episode: 6530/20000, loss: 0.0543755, test_acc: 0.9243\n",
      "episode: 6540/20000, loss: 0.0551911, test_acc: 0.9243\n",
      "episode: 6550/20000, loss: 0.0543321, test_acc: 0.9243\n",
      "episode: 6560/20000, loss: 0.0534446, test_acc: 0.9243\n",
      "episode: 6570/20000, loss: 0.0518274, test_acc: 0.9243\n",
      "episode: 6580/20000, loss: 0.0523862, test_acc: 0.9243\n",
      "episode: 6590/20000, loss: 0.0592205, test_acc: 0.9243\n",
      "episode: 6600/20000, loss: 0.0566375, test_acc: 0.9243\n",
      "episode: 6610/20000, loss: 0.0561812, test_acc: 0.9243\n",
      "episode: 6620/20000, loss: 0.0575416, test_acc: 0.9243\n",
      "episode: 6630/20000, loss: 0.0594584, test_acc: 0.9243\n",
      "episode: 6640/20000, loss: 0.0553775, test_acc: 0.9243\n",
      "episode: 6650/20000, loss: 0.0522485, test_acc: 0.9243\n",
      "episode: 6660/20000, loss: 0.0541498, test_acc: 0.9243\n",
      "episode: 6670/20000, loss: 0.0548186, test_acc: 0.9243\n",
      "episode: 6680/20000, loss: 0.0532769, test_acc: 0.9243\n",
      "episode: 6690/20000, loss: 0.0558559, test_acc: 0.9243\n",
      "episode: 6700/20000, loss: 0.0557863, test_acc: 0.9243\n",
      "episode: 6710/20000, loss: 0.0558816, test_acc: 0.9243\n",
      "episode: 6720/20000, loss: 0.0530924, test_acc: 0.9243\n",
      "episode: 6730/20000, loss: 0.0548368, test_acc: 0.9243\n",
      "episode: 6740/20000, loss: 0.0581195, test_acc: 0.9243\n",
      "episode: 6750/20000, loss: 0.0471865, test_acc: 0.9243\n",
      "episode: 6760/20000, loss: 0.0543776, test_acc: 0.9243\n",
      "episode: 6770/20000, loss: 0.0546573, test_acc: 0.9243\n",
      "episode: 6780/20000, loss: 0.0575672, test_acc: 0.9243\n",
      "episode: 6790/20000, loss: 0.0562313, test_acc: 0.9243\n",
      "episode: 6800/20000, loss: 0.0564075, test_acc: 0.9243\n",
      "episode: 6810/20000, loss: 0.0550921, test_acc: 0.9243\n",
      "episode: 6820/20000, loss: 0.0574589, test_acc: 0.9243\n",
      "episode: 6830/20000, loss: 0.0583892, test_acc: 0.9243\n",
      "episode: 6840/20000, loss: 0.0555512, test_acc: 0.9243\n",
      "episode: 6850/20000, loss: 0.0543004, test_acc: 0.9243\n",
      "episode: 6860/20000, loss: 0.0582742, test_acc: 0.9243\n",
      "episode: 6870/20000, loss: 0.0569339, test_acc: 0.9243\n",
      "episode: 6880/20000, loss: 0.0493449, test_acc: 0.9243\n",
      "episode: 6890/20000, loss: 0.0549967, test_acc: 0.9243\n",
      "episode: 6900/20000, loss: 0.0560783, test_acc: 0.9243\n",
      "episode: 6910/20000, loss: 0.0548871, test_acc: 0.9243\n",
      "episode: 6920/20000, loss: 0.0555091, test_acc: 0.9243\n",
      "episode: 6930/20000, loss: 0.0537024, test_acc: 0.9243\n",
      "episode: 6940/20000, loss: 0.0512426, test_acc: 0.9243\n",
      "episode: 6950/20000, loss: 0.0573216, test_acc: 0.9243\n",
      "episode: 6960/20000, loss: 0.0576661, test_acc: 0.9243\n",
      "episode: 6970/20000, loss: 0.0547417, test_acc: 0.9243\n",
      "episode: 6980/20000, loss: 0.0538668, test_acc: 0.9243\n",
      "episode: 6990/20000, loss: 0.0580930, test_acc: 0.9243\n",
      "episode: 7000/20000, loss: 0.0516219, test_acc: 0.9318\n",
      "episode: 7010/20000, loss: 0.0591990, test_acc: 0.9318\n",
      "episode: 7020/20000, loss: 0.0576120, test_acc: 0.9318\n",
      "episode: 7030/20000, loss: 0.0542655, test_acc: 0.9318\n",
      "episode: 7040/20000, loss: 0.0554357, test_acc: 0.9318\n",
      "episode: 7050/20000, loss: 0.0545592, test_acc: 0.9318\n",
      "episode: 7060/20000, loss: 0.0558675, test_acc: 0.9318\n",
      "episode: 7070/20000, loss: 0.0517787, test_acc: 0.9318\n",
      "episode: 7080/20000, loss: 0.0605559, test_acc: 0.9318\n",
      "episode: 7090/20000, loss: 0.0558852, test_acc: 0.9318\n",
      "episode: 7100/20000, loss: 0.0580687, test_acc: 0.9318\n",
      "episode: 7110/20000, loss: 0.0579258, test_acc: 0.9318\n",
      "episode: 7120/20000, loss: 0.0560260, test_acc: 0.9318\n",
      "episode: 7130/20000, loss: 0.0539291, test_acc: 0.9318\n",
      "episode: 7140/20000, loss: 0.0588364, test_acc: 0.9318\n",
      "episode: 7150/20000, loss: 0.0557320, test_acc: 0.9318\n",
      "episode: 7160/20000, loss: 0.0565330, test_acc: 0.9318\n",
      "episode: 7170/20000, loss: 0.0495516, test_acc: 0.9318\n",
      "episode: 7180/20000, loss: 0.0500633, test_acc: 0.9318\n",
      "episode: 7190/20000, loss: 0.0544010, test_acc: 0.9318\n",
      "episode: 7200/20000, loss: 0.0548697, test_acc: 0.9318\n",
      "episode: 7210/20000, loss: 0.0570706, test_acc: 0.9318\n",
      "episode: 7220/20000, loss: 0.0560385, test_acc: 0.9318\n",
      "episode: 7230/20000, loss: 0.0511209, test_acc: 0.9318\n",
      "episode: 7240/20000, loss: 0.0579897, test_acc: 0.9318\n",
      "episode: 7250/20000, loss: 0.0562867, test_acc: 0.9318\n",
      "episode: 7260/20000, loss: 0.0514372, test_acc: 0.9318\n",
      "episode: 7270/20000, loss: 0.0561057, test_acc: 0.9318\n",
      "episode: 7280/20000, loss: 0.0587980, test_acc: 0.9318\n",
      "episode: 7290/20000, loss: 0.0540622, test_acc: 0.9318\n",
      "episode: 7300/20000, loss: 0.0556790, test_acc: 0.9318\n",
      "episode: 7310/20000, loss: 0.0555019, test_acc: 0.9318\n",
      "episode: 7320/20000, loss: 0.0563020, test_acc: 0.9318\n",
      "episode: 7330/20000, loss: 0.0515113, test_acc: 0.9318\n",
      "episode: 7340/20000, loss: 0.0538743, test_acc: 0.9318\n",
      "episode: 7350/20000, loss: 0.0592504, test_acc: 0.9318\n",
      "episode: 7360/20000, loss: 0.0568650, test_acc: 0.9318\n",
      "episode: 7370/20000, loss: 0.0546709, test_acc: 0.9318\n",
      "episode: 7380/20000, loss: 0.0612634, test_acc: 0.9318\n",
      "episode: 7390/20000, loss: 0.0551763, test_acc: 0.9318\n",
      "episode: 7400/20000, loss: 0.0561706, test_acc: 0.9318\n",
      "episode: 7410/20000, loss: 0.0540825, test_acc: 0.9318\n",
      "episode: 7420/20000, loss: 0.0560114, test_acc: 0.9318\n",
      "episode: 7430/20000, loss: 0.0545481, test_acc: 0.9318\n",
      "episode: 7440/20000, loss: 0.0567893, test_acc: 0.9318\n",
      "episode: 7450/20000, loss: 0.0574402, test_acc: 0.9318\n",
      "episode: 7460/20000, loss: 0.0592083, test_acc: 0.9318\n",
      "episode: 7470/20000, loss: 0.0533742, test_acc: 0.9318\n",
      "episode: 7480/20000, loss: 0.0575052, test_acc: 0.9318\n",
      "episode: 7490/20000, loss: 0.0595873, test_acc: 0.9318\n",
      "episode: 7500/20000, loss: 0.0560339, test_acc: 0.9476\n",
      "episode: 7510/20000, loss: 0.0543242, test_acc: 0.9476\n",
      "episode: 7520/20000, loss: 0.0545191, test_acc: 0.9476\n",
      "episode: 7530/20000, loss: 0.0538160, test_acc: 0.9476\n",
      "episode: 7540/20000, loss: 0.0531983, test_acc: 0.9476\n",
      "episode: 7550/20000, loss: 0.0573564, test_acc: 0.9476\n",
      "episode: 7560/20000, loss: 0.0533848, test_acc: 0.9476\n",
      "episode: 7570/20000, loss: 0.0598970, test_acc: 0.9476\n",
      "episode: 7580/20000, loss: 0.0544767, test_acc: 0.9476\n",
      "episode: 7590/20000, loss: 0.0577910, test_acc: 0.9476\n",
      "episode: 7600/20000, loss: 0.0593051, test_acc: 0.9476\n",
      "episode: 7610/20000, loss: 0.0563817, test_acc: 0.9476\n",
      "episode: 7620/20000, loss: 0.0513853, test_acc: 0.9476\n",
      "episode: 7630/20000, loss: 0.0557310, test_acc: 0.9476\n",
      "episode: 7640/20000, loss: 0.0533109, test_acc: 0.9476\n",
      "episode: 7650/20000, loss: 0.0566563, test_acc: 0.9476\n",
      "episode: 7660/20000, loss: 0.0514910, test_acc: 0.9476\n",
      "episode: 7670/20000, loss: 0.0548127, test_acc: 0.9476\n",
      "episode: 7680/20000, loss: 0.0583829, test_acc: 0.9476\n",
      "episode: 7690/20000, loss: 0.0557339, test_acc: 0.9476\n",
      "episode: 7700/20000, loss: 0.0570263, test_acc: 0.9476\n",
      "episode: 7710/20000, loss: 0.0570090, test_acc: 0.9476\n",
      "episode: 7720/20000, loss: 0.0556644, test_acc: 0.9476\n",
      "episode: 7730/20000, loss: 0.0589353, test_acc: 0.9476\n",
      "episode: 7740/20000, loss: 0.0561047, test_acc: 0.9476\n",
      "episode: 7750/20000, loss: 0.0577715, test_acc: 0.9476\n",
      "episode: 7760/20000, loss: 0.0571764, test_acc: 0.9476\n",
      "episode: 7770/20000, loss: 0.0578051, test_acc: 0.9476\n",
      "episode: 7780/20000, loss: 0.0531986, test_acc: 0.9476\n",
      "episode: 7790/20000, loss: 0.0579308, test_acc: 0.9476\n",
      "episode: 7800/20000, loss: 0.0558173, test_acc: 0.9476\n",
      "episode: 7810/20000, loss: 0.0560899, test_acc: 0.9476\n",
      "episode: 7820/20000, loss: 0.0544917, test_acc: 0.9476\n",
      "episode: 7830/20000, loss: 0.0552613, test_acc: 0.9476\n",
      "episode: 7840/20000, loss: 0.0535457, test_acc: 0.9476\n",
      "episode: 7850/20000, loss: 0.0531848, test_acc: 0.9476\n",
      "episode: 7860/20000, loss: 0.0581651, test_acc: 0.9476\n",
      "episode: 7870/20000, loss: 0.0544841, test_acc: 0.9476\n",
      "episode: 7880/20000, loss: 0.0552703, test_acc: 0.9476\n",
      "episode: 7890/20000, loss: 0.0538079, test_acc: 0.9476\n",
      "episode: 7900/20000, loss: 0.0527873, test_acc: 0.9476\n",
      "episode: 7910/20000, loss: 0.0579538, test_acc: 0.9476\n",
      "episode: 7920/20000, loss: 0.0534774, test_acc: 0.9476\n",
      "episode: 7930/20000, loss: 0.0556151, test_acc: 0.9476\n",
      "episode: 7940/20000, loss: 0.0568041, test_acc: 0.9476\n",
      "episode: 7950/20000, loss: 0.0562769, test_acc: 0.9476\n",
      "episode: 7960/20000, loss: 0.0610028, test_acc: 0.9476\n",
      "episode: 7970/20000, loss: 0.0521240, test_acc: 0.9476\n",
      "episode: 7980/20000, loss: 0.0574455, test_acc: 0.9476\n",
      "episode: 7990/20000, loss: 0.0542659, test_acc: 0.9476\n",
      "episode: 8000/20000, loss: 0.0587727, test_acc: 0.9642\n",
      "episode: 8010/20000, loss: 0.0544264, test_acc: 0.9642\n",
      "episode: 8020/20000, loss: 0.0557419, test_acc: 0.9642\n",
      "episode: 8030/20000, loss: 0.0507970, test_acc: 0.9642\n",
      "episode: 8040/20000, loss: 0.0523212, test_acc: 0.9642\n",
      "episode: 8050/20000, loss: 0.0562767, test_acc: 0.9642\n",
      "episode: 8060/20000, loss: 0.0527112, test_acc: 0.9642\n",
      "episode: 8070/20000, loss: 0.0508583, test_acc: 0.9642\n",
      "episode: 8080/20000, loss: 0.0576608, test_acc: 0.9642\n",
      "episode: 8090/20000, loss: 0.0543847, test_acc: 0.9642\n",
      "episode: 8100/20000, loss: 0.0545711, test_acc: 0.9642\n",
      "episode: 8110/20000, loss: 0.0542089, test_acc: 0.9642\n",
      "episode: 8120/20000, loss: 0.0551069, test_acc: 0.9642\n",
      "episode: 8130/20000, loss: 0.0571732, test_acc: 0.9642\n",
      "episode: 8140/20000, loss: 0.0601585, test_acc: 0.9642\n",
      "episode: 8150/20000, loss: 0.0570560, test_acc: 0.9642\n",
      "episode: 8160/20000, loss: 0.0578104, test_acc: 0.9642\n",
      "episode: 8170/20000, loss: 0.0579592, test_acc: 0.9642\n",
      "episode: 8180/20000, loss: 0.0539382, test_acc: 0.9642\n",
      "episode: 8190/20000, loss: 0.0592395, test_acc: 0.9642\n",
      "episode: 8200/20000, loss: 0.0585917, test_acc: 0.9642\n",
      "episode: 8210/20000, loss: 0.0601905, test_acc: 0.9642\n",
      "episode: 8220/20000, loss: 0.0533499, test_acc: 0.9642\n",
      "episode: 8230/20000, loss: 0.0575480, test_acc: 0.9642\n",
      "episode: 8240/20000, loss: 0.0546107, test_acc: 0.9642\n",
      "episode: 8250/20000, loss: 0.0573880, test_acc: 0.9642\n",
      "episode: 8260/20000, loss: 0.0602916, test_acc: 0.9642\n",
      "episode: 8270/20000, loss: 0.0579189, test_acc: 0.9642\n",
      "episode: 8280/20000, loss: 0.0582456, test_acc: 0.9642\n",
      "episode: 8290/20000, loss: 0.0587526, test_acc: 0.9642\n",
      "episode: 8300/20000, loss: 0.0540099, test_acc: 0.9642\n",
      "episode: 8310/20000, loss: 0.0546422, test_acc: 0.9642\n",
      "episode: 8320/20000, loss: 0.0565044, test_acc: 0.9642\n",
      "episode: 8330/20000, loss: 0.0551740, test_acc: 0.9642\n",
      "episode: 8340/20000, loss: 0.0523908, test_acc: 0.9642\n",
      "episode: 8350/20000, loss: 0.0567497, test_acc: 0.9642\n",
      "episode: 8360/20000, loss: 0.0546205, test_acc: 0.9642\n",
      "episode: 8370/20000, loss: 0.0565641, test_acc: 0.9642\n",
      "episode: 8380/20000, loss: 0.0570967, test_acc: 0.9642\n",
      "episode: 8390/20000, loss: 0.0580948, test_acc: 0.9642\n",
      "episode: 8400/20000, loss: 0.0566079, test_acc: 0.9642\n",
      "episode: 8410/20000, loss: 0.0535773, test_acc: 0.9642\n",
      "episode: 8420/20000, loss: 0.0560699, test_acc: 0.9642\n",
      "episode: 8430/20000, loss: 0.0577761, test_acc: 0.9642\n",
      "episode: 8440/20000, loss: 0.0585644, test_acc: 0.9642\n",
      "episode: 8450/20000, loss: 0.0576028, test_acc: 0.9642\n",
      "episode: 8460/20000, loss: 0.0557600, test_acc: 0.9642\n",
      "episode: 8470/20000, loss: 0.0548502, test_acc: 0.9642\n",
      "episode: 8480/20000, loss: 0.0600527, test_acc: 0.9642\n",
      "episode: 8490/20000, loss: 0.0505169, test_acc: 0.9642\n",
      "episode: 8500/20000, loss: 0.0538639, test_acc: 0.9290\n",
      "episode: 8510/20000, loss: 0.0521465, test_acc: 0.9290\n",
      "episode: 8520/20000, loss: 0.0568895, test_acc: 0.9290\n",
      "episode: 8530/20000, loss: 0.0585023, test_acc: 0.9290\n",
      "episode: 8540/20000, loss: 0.0574271, test_acc: 0.9290\n",
      "episode: 8550/20000, loss: 0.0539208, test_acc: 0.9290\n",
      "episode: 8560/20000, loss: 0.0546157, test_acc: 0.9290\n",
      "episode: 8570/20000, loss: 0.0524728, test_acc: 0.9290\n",
      "episode: 8580/20000, loss: 0.0565576, test_acc: 0.9290\n",
      "episode: 8590/20000, loss: 0.0569030, test_acc: 0.9290\n",
      "episode: 8600/20000, loss: 0.0584034, test_acc: 0.9290\n",
      "episode: 8610/20000, loss: 0.0564138, test_acc: 0.9290\n",
      "episode: 8620/20000, loss: 0.0577414, test_acc: 0.9290\n",
      "episode: 8630/20000, loss: 0.0532813, test_acc: 0.9290\n",
      "episode: 8640/20000, loss: 0.0514747, test_acc: 0.9290\n",
      "episode: 8650/20000, loss: 0.0561023, test_acc: 0.9290\n",
      "episode: 8660/20000, loss: 0.0543060, test_acc: 0.9290\n",
      "episode: 8670/20000, loss: 0.0537294, test_acc: 0.9290\n",
      "episode: 8680/20000, loss: 0.0587524, test_acc: 0.9290\n",
      "episode: 8690/20000, loss: 0.0539620, test_acc: 0.9290\n",
      "episode: 8700/20000, loss: 0.0485026, test_acc: 0.9290\n",
      "episode: 8710/20000, loss: 0.0481427, test_acc: 0.9290\n",
      "episode: 8720/20000, loss: 0.0569137, test_acc: 0.9290\n",
      "episode: 8730/20000, loss: 0.0565578, test_acc: 0.9290\n",
      "episode: 8740/20000, loss: 0.0552977, test_acc: 0.9290\n",
      "episode: 8750/20000, loss: 0.0525548, test_acc: 0.9290\n",
      "episode: 8760/20000, loss: 0.0569833, test_acc: 0.9290\n",
      "episode: 8770/20000, loss: 0.0524362, test_acc: 0.9290\n",
      "episode: 8780/20000, loss: 0.0551519, test_acc: 0.9290\n",
      "episode: 8790/20000, loss: 0.0550133, test_acc: 0.9290\n",
      "episode: 8800/20000, loss: 0.0565029, test_acc: 0.9290\n",
      "episode: 8810/20000, loss: 0.0536540, test_acc: 0.9290\n",
      "episode: 8820/20000, loss: 0.0560587, test_acc: 0.9290\n",
      "episode: 8830/20000, loss: 0.0564322, test_acc: 0.9290\n",
      "episode: 8840/20000, loss: 0.0556089, test_acc: 0.9290\n",
      "episode: 8850/20000, loss: 0.0514058, test_acc: 0.9290\n",
      "episode: 8860/20000, loss: 0.0534916, test_acc: 0.9290\n",
      "episode: 8870/20000, loss: 0.0529566, test_acc: 0.9290\n",
      "episode: 8880/20000, loss: 0.0539360, test_acc: 0.9290\n",
      "episode: 8890/20000, loss: 0.0572255, test_acc: 0.9290\n",
      "episode: 8900/20000, loss: 0.0529363, test_acc: 0.9290\n",
      "episode: 8910/20000, loss: 0.0521148, test_acc: 0.9290\n",
      "episode: 8920/20000, loss: 0.0575931, test_acc: 0.9290\n",
      "episode: 8930/20000, loss: 0.0559518, test_acc: 0.9290\n",
      "episode: 8940/20000, loss: 0.0516332, test_acc: 0.9290\n",
      "episode: 8950/20000, loss: 0.0555372, test_acc: 0.9290\n",
      "episode: 8960/20000, loss: 0.0545266, test_acc: 0.9290\n",
      "episode: 8970/20000, loss: 0.0544901, test_acc: 0.9290\n",
      "episode: 8980/20000, loss: 0.0481960, test_acc: 0.9290\n",
      "episode: 8990/20000, loss: 0.0567701, test_acc: 0.9290\n",
      "episode: 9000/20000, loss: 0.0561313, test_acc: 0.9530\n",
      "episode: 9010/20000, loss: 0.0580879, test_acc: 0.9530\n",
      "episode: 9020/20000, loss: 0.0587464, test_acc: 0.9530\n",
      "episode: 9030/20000, loss: 0.0587240, test_acc: 0.9530\n",
      "episode: 9040/20000, loss: 0.0520852, test_acc: 0.9530\n",
      "episode: 9050/20000, loss: 0.0532336, test_acc: 0.9530\n",
      "episode: 9060/20000, loss: 0.0532477, test_acc: 0.9530\n",
      "episode: 9070/20000, loss: 0.0544420, test_acc: 0.9530\n",
      "episode: 9080/20000, loss: 0.0526313, test_acc: 0.9530\n",
      "episode: 9090/20000, loss: 0.0508778, test_acc: 0.9530\n",
      "episode: 9100/20000, loss: 0.0552194, test_acc: 0.9530\n",
      "episode: 9110/20000, loss: 0.0502469, test_acc: 0.9530\n",
      "episode: 9120/20000, loss: 0.0572449, test_acc: 0.9530\n",
      "episode: 9130/20000, loss: 0.0521315, test_acc: 0.9530\n",
      "episode: 9140/20000, loss: 0.0559470, test_acc: 0.9530\n",
      "episode: 9150/20000, loss: 0.0573759, test_acc: 0.9530\n",
      "episode: 9160/20000, loss: 0.0532451, test_acc: 0.9530\n",
      "episode: 9170/20000, loss: 0.0537745, test_acc: 0.9530\n",
      "episode: 9180/20000, loss: 0.0503919, test_acc: 0.9530\n",
      "episode: 9190/20000, loss: 0.0579795, test_acc: 0.9530\n",
      "episode: 9200/20000, loss: 0.0583812, test_acc: 0.9530\n",
      "episode: 9210/20000, loss: 0.0503315, test_acc: 0.9530\n",
      "episode: 9220/20000, loss: 0.0556239, test_acc: 0.9530\n",
      "episode: 9230/20000, loss: 0.0582688, test_acc: 0.9530\n",
      "episode: 9240/20000, loss: 0.0507531, test_acc: 0.9530\n",
      "episode: 9250/20000, loss: 0.0593076, test_acc: 0.9530\n",
      "episode: 9260/20000, loss: 0.0562794, test_acc: 0.9530\n",
      "episode: 9270/20000, loss: 0.0565840, test_acc: 0.9530\n",
      "episode: 9280/20000, loss: 0.0508149, test_acc: 0.9530\n",
      "episode: 9290/20000, loss: 0.0578243, test_acc: 0.9530\n",
      "episode: 9300/20000, loss: 0.0518138, test_acc: 0.9530\n",
      "episode: 9310/20000, loss: 0.0516092, test_acc: 0.9530\n",
      "episode: 9320/20000, loss: 0.0546684, test_acc: 0.9530\n",
      "episode: 9330/20000, loss: 0.0477348, test_acc: 0.9530\n",
      "episode: 9340/20000, loss: 0.0531881, test_acc: 0.9530\n",
      "episode: 9350/20000, loss: 0.0523375, test_acc: 0.9530\n",
      "episode: 9360/20000, loss: 0.0543994, test_acc: 0.9530\n",
      "episode: 9370/20000, loss: 0.0569132, test_acc: 0.9530\n",
      "episode: 9380/20000, loss: 0.0548481, test_acc: 0.9530\n",
      "episode: 9390/20000, loss: 0.0583925, test_acc: 0.9530\n",
      "episode: 9400/20000, loss: 0.0562561, test_acc: 0.9530\n",
      "episode: 9410/20000, loss: 0.0517827, test_acc: 0.9530\n",
      "episode: 9420/20000, loss: 0.0544019, test_acc: 0.9530\n",
      "episode: 9430/20000, loss: 0.0567831, test_acc: 0.9530\n",
      "episode: 9440/20000, loss: 0.0574448, test_acc: 0.9530\n",
      "episode: 9450/20000, loss: 0.0533367, test_acc: 0.9530\n",
      "episode: 9460/20000, loss: 0.0564874, test_acc: 0.9530\n",
      "episode: 9470/20000, loss: 0.0614473, test_acc: 0.9530\n",
      "episode: 9480/20000, loss: 0.0569306, test_acc: 0.9530\n",
      "episode: 9490/20000, loss: 0.0552517, test_acc: 0.9530\n",
      "episode: 9500/20000, loss: 0.0549203, test_acc: 0.9547\n",
      "episode: 9510/20000, loss: 0.0550037, test_acc: 0.9547\n",
      "episode: 9520/20000, loss: 0.0565216, test_acc: 0.9547\n",
      "episode: 9530/20000, loss: 0.0542556, test_acc: 0.9547\n",
      "episode: 9540/20000, loss: 0.0555861, test_acc: 0.9547\n",
      "episode: 9550/20000, loss: 0.0527644, test_acc: 0.9547\n",
      "episode: 9560/20000, loss: 0.0540950, test_acc: 0.9547\n",
      "episode: 9570/20000, loss: 0.0530216, test_acc: 0.9547\n",
      "episode: 9580/20000, loss: 0.0552640, test_acc: 0.9547\n",
      "episode: 9590/20000, loss: 0.0537231, test_acc: 0.9547\n",
      "episode: 9600/20000, loss: 0.0530168, test_acc: 0.9547\n",
      "episode: 9610/20000, loss: 0.0569164, test_acc: 0.9547\n",
      "episode: 9620/20000, loss: 0.0558541, test_acc: 0.9547\n",
      "episode: 9630/20000, loss: 0.0545718, test_acc: 0.9547\n",
      "episode: 9640/20000, loss: 0.0539600, test_acc: 0.9547\n",
      "episode: 9650/20000, loss: 0.0524356, test_acc: 0.9547\n",
      "episode: 9660/20000, loss: 0.0560533, test_acc: 0.9547\n",
      "episode: 9670/20000, loss: 0.0555634, test_acc: 0.9547\n",
      "episode: 9680/20000, loss: 0.0544555, test_acc: 0.9547\n",
      "episode: 9690/20000, loss: 0.0549097, test_acc: 0.9547\n",
      "episode: 9700/20000, loss: 0.0578790, test_acc: 0.9547\n",
      "episode: 9710/20000, loss: 0.0581763, test_acc: 0.9547\n",
      "episode: 9720/20000, loss: 0.0609912, test_acc: 0.9547\n",
      "episode: 9730/20000, loss: 0.0547220, test_acc: 0.9547\n",
      "episode: 9740/20000, loss: 0.0521018, test_acc: 0.9547\n",
      "episode: 9750/20000, loss: 0.0569240, test_acc: 0.9547\n",
      "episode: 9760/20000, loss: 0.0607352, test_acc: 0.9547\n",
      "episode: 9770/20000, loss: 0.0520812, test_acc: 0.9547\n",
      "episode: 9780/20000, loss: 0.0505052, test_acc: 0.9547\n",
      "episode: 9790/20000, loss: 0.0583949, test_acc: 0.9547\n",
      "episode: 9800/20000, loss: 0.0548920, test_acc: 0.9547\n",
      "episode: 9810/20000, loss: 0.0590380, test_acc: 0.9547\n",
      "episode: 9820/20000, loss: 0.0533707, test_acc: 0.9547\n",
      "episode: 9830/20000, loss: 0.0538882, test_acc: 0.9547\n",
      "episode: 9840/20000, loss: 0.0504591, test_acc: 0.9547\n",
      "episode: 9850/20000, loss: 0.0567339, test_acc: 0.9547\n",
      "episode: 9860/20000, loss: 0.0574116, test_acc: 0.9547\n",
      "episode: 9870/20000, loss: 0.0508968, test_acc: 0.9547\n",
      "episode: 9880/20000, loss: 0.0560404, test_acc: 0.9547\n",
      "episode: 9890/20000, loss: 0.0573033, test_acc: 0.9547\n",
      "episode: 9900/20000, loss: 0.0558079, test_acc: 0.9547\n",
      "episode: 9910/20000, loss: 0.0506033, test_acc: 0.9547\n",
      "episode: 9920/20000, loss: 0.0519984, test_acc: 0.9547\n",
      "episode: 9930/20000, loss: 0.0562354, test_acc: 0.9547\n",
      "episode: 9940/20000, loss: 0.0579679, test_acc: 0.9547\n",
      "episode: 9950/20000, loss: 0.0580157, test_acc: 0.9547\n",
      "episode: 9960/20000, loss: 0.0541037, test_acc: 0.9547\n",
      "episode: 9970/20000, loss: 0.0561322, test_acc: 0.9547\n",
      "episode: 9980/20000, loss: 0.0557971, test_acc: 0.9547\n",
      "episode: 9990/20000, loss: 0.0533159, test_acc: 0.9547\n",
      "episode: 10000/20000, loss: 0.0540778, test_acc: 0.9455\n",
      "episode: 10010/20000, loss: 0.0470373, test_acc: 0.9455\n",
      "episode: 10020/20000, loss: 0.0575798, test_acc: 0.9455\n",
      "episode: 10030/20000, loss: 0.0557777, test_acc: 0.9455\n",
      "episode: 10040/20000, loss: 0.0574525, test_acc: 0.9455\n",
      "episode: 10050/20000, loss: 0.0602047, test_acc: 0.9455\n",
      "episode: 10060/20000, loss: 0.0526959, test_acc: 0.9455\n",
      "episode: 10070/20000, loss: 0.0533068, test_acc: 0.9455\n",
      "episode: 10080/20000, loss: 0.0553291, test_acc: 0.9455\n",
      "episode: 10090/20000, loss: 0.0541529, test_acc: 0.9455\n",
      "episode: 10100/20000, loss: 0.0584662, test_acc: 0.9455\n",
      "episode: 10110/20000, loss: 0.0514265, test_acc: 0.9455\n",
      "episode: 10120/20000, loss: 0.0553112, test_acc: 0.9455\n",
      "episode: 10130/20000, loss: 0.0576938, test_acc: 0.9455\n",
      "episode: 10140/20000, loss: 0.0592330, test_acc: 0.9455\n",
      "episode: 10150/20000, loss: 0.0570413, test_acc: 0.9455\n",
      "episode: 10160/20000, loss: 0.0548073, test_acc: 0.9455\n",
      "episode: 10170/20000, loss: 0.0534811, test_acc: 0.9455\n",
      "episode: 10180/20000, loss: 0.0574703, test_acc: 0.9455\n",
      "episode: 10190/20000, loss: 0.0532132, test_acc: 0.9455\n",
      "episode: 10200/20000, loss: 0.0569982, test_acc: 0.9455\n",
      "episode: 10210/20000, loss: 0.0586021, test_acc: 0.9455\n",
      "episode: 10220/20000, loss: 0.0512614, test_acc: 0.9455\n",
      "episode: 10230/20000, loss: 0.0572592, test_acc: 0.9455\n",
      "episode: 10240/20000, loss: 0.0477202, test_acc: 0.9455\n",
      "episode: 10250/20000, loss: 0.0578149, test_acc: 0.9455\n",
      "episode: 10260/20000, loss: 0.0558561, test_acc: 0.9455\n",
      "episode: 10270/20000, loss: 0.0593082, test_acc: 0.9455\n",
      "episode: 10280/20000, loss: 0.0513117, test_acc: 0.9455\n",
      "episode: 10290/20000, loss: 0.0556746, test_acc: 0.9455\n",
      "episode: 10300/20000, loss: 0.0538044, test_acc: 0.9455\n",
      "episode: 10310/20000, loss: 0.0532388, test_acc: 0.9455\n",
      "episode: 10320/20000, loss: 0.0543708, test_acc: 0.9455\n",
      "episode: 10330/20000, loss: 0.0528231, test_acc: 0.9455\n",
      "episode: 10340/20000, loss: 0.0547016, test_acc: 0.9455\n",
      "episode: 10350/20000, loss: 0.0567848, test_acc: 0.9455\n",
      "episode: 10360/20000, loss: 0.0595711, test_acc: 0.9455\n",
      "episode: 10370/20000, loss: 0.0571565, test_acc: 0.9455\n",
      "episode: 10380/20000, loss: 0.0547382, test_acc: 0.9455\n",
      "episode: 10390/20000, loss: 0.0560830, test_acc: 0.9455\n",
      "episode: 10400/20000, loss: 0.0557601, test_acc: 0.9455\n",
      "episode: 10410/20000, loss: 0.0566592, test_acc: 0.9455\n",
      "episode: 10420/20000, loss: 0.0635352, test_acc: 0.9455\n",
      "episode: 10430/20000, loss: 0.0570514, test_acc: 0.9455\n",
      "episode: 10440/20000, loss: 0.0508658, test_acc: 0.9455\n",
      "episode: 10450/20000, loss: 0.0524194, test_acc: 0.9455\n",
      "episode: 10460/20000, loss: 0.0556589, test_acc: 0.9455\n",
      "episode: 10470/20000, loss: 0.0563661, test_acc: 0.9455\n",
      "episode: 10480/20000, loss: 0.0597466, test_acc: 0.9455\n",
      "episode: 10490/20000, loss: 0.0515326, test_acc: 0.9455\n",
      "episode: 10500/20000, loss: 0.0527925, test_acc: 0.9525\n",
      "episode: 10510/20000, loss: 0.0558065, test_acc: 0.9525\n",
      "episode: 10520/20000, loss: 0.0580289, test_acc: 0.9525\n",
      "episode: 10530/20000, loss: 0.0565700, test_acc: 0.9525\n",
      "episode: 10540/20000, loss: 0.0544669, test_acc: 0.9525\n",
      "episode: 10550/20000, loss: 0.0536449, test_acc: 0.9525\n",
      "episode: 10560/20000, loss: 0.0503623, test_acc: 0.9525\n",
      "episode: 10570/20000, loss: 0.0552254, test_acc: 0.9525\n",
      "episode: 10580/20000, loss: 0.0593901, test_acc: 0.9525\n",
      "episode: 10590/20000, loss: 0.0515950, test_acc: 0.9525\n",
      "episode: 10600/20000, loss: 0.0566007, test_acc: 0.9525\n",
      "episode: 10610/20000, loss: 0.0554390, test_acc: 0.9525\n",
      "episode: 10620/20000, loss: 0.0499526, test_acc: 0.9525\n",
      "episode: 10630/20000, loss: 0.0543961, test_acc: 0.9525\n",
      "episode: 10640/20000, loss: 0.0524201, test_acc: 0.9525\n",
      "episode: 10650/20000, loss: 0.0571339, test_acc: 0.9525\n",
      "episode: 10660/20000, loss: 0.0548546, test_acc: 0.9525\n",
      "episode: 10670/20000, loss: 0.0546594, test_acc: 0.9525\n",
      "episode: 10680/20000, loss: 0.0550564, test_acc: 0.9525\n",
      "episode: 10690/20000, loss: 0.0560839, test_acc: 0.9525\n",
      "episode: 10700/20000, loss: 0.0566853, test_acc: 0.9525\n",
      "episode: 10710/20000, loss: 0.0516739, test_acc: 0.9525\n",
      "episode: 10720/20000, loss: 0.0472818, test_acc: 0.9525\n",
      "episode: 10730/20000, loss: 0.0529822, test_acc: 0.9525\n",
      "episode: 10740/20000, loss: 0.0571822, test_acc: 0.9525\n",
      "episode: 10750/20000, loss: 0.0501299, test_acc: 0.9525\n",
      "episode: 10760/20000, loss: 0.0568441, test_acc: 0.9525\n",
      "episode: 10770/20000, loss: 0.0558061, test_acc: 0.9525\n",
      "episode: 10780/20000, loss: 0.0548837, test_acc: 0.9525\n",
      "episode: 10790/20000, loss: 0.0549986, test_acc: 0.9525\n",
      "episode: 10800/20000, loss: 0.0540863, test_acc: 0.9525\n",
      "episode: 10810/20000, loss: 0.0544372, test_acc: 0.9525\n",
      "episode: 10820/20000, loss: 0.0543532, test_acc: 0.9525\n",
      "episode: 10830/20000, loss: 0.0515160, test_acc: 0.9525\n",
      "episode: 10840/20000, loss: 0.0562684, test_acc: 0.9525\n",
      "episode: 10850/20000, loss: 0.0514677, test_acc: 0.9525\n",
      "episode: 10860/20000, loss: 0.0531864, test_acc: 0.9525\n",
      "episode: 10870/20000, loss: 0.0543516, test_acc: 0.9525\n",
      "episode: 10880/20000, loss: 0.0549739, test_acc: 0.9525\n",
      "episode: 10890/20000, loss: 0.0534835, test_acc: 0.9525\n",
      "episode: 10900/20000, loss: 0.0557233, test_acc: 0.9525\n",
      "episode: 10910/20000, loss: 0.0591630, test_acc: 0.9525\n",
      "episode: 10920/20000, loss: 0.0591373, test_acc: 0.9525\n",
      "episode: 10930/20000, loss: 0.0565919, test_acc: 0.9525\n",
      "episode: 10940/20000, loss: 0.0556487, test_acc: 0.9525\n",
      "episode: 10950/20000, loss: 0.0562475, test_acc: 0.9525\n",
      "episode: 10960/20000, loss: 0.0544461, test_acc: 0.9525\n",
      "episode: 10970/20000, loss: 0.0541950, test_acc: 0.9525\n",
      "episode: 10980/20000, loss: 0.0554938, test_acc: 0.9525\n",
      "episode: 10990/20000, loss: 0.0549238, test_acc: 0.9525\n",
      "episode: 11000/20000, loss: 0.0492722, test_acc: 0.9377\n",
      "episode: 11010/20000, loss: 0.0543574, test_acc: 0.9377\n",
      "episode: 11020/20000, loss: 0.0569111, test_acc: 0.9377\n",
      "episode: 11030/20000, loss: 0.0538748, test_acc: 0.9377\n",
      "episode: 11040/20000, loss: 0.0530964, test_acc: 0.9377\n",
      "episode: 11050/20000, loss: 0.0563425, test_acc: 0.9377\n",
      "episode: 11060/20000, loss: 0.0579527, test_acc: 0.9377\n",
      "episode: 11070/20000, loss: 0.0587555, test_acc: 0.9377\n",
      "episode: 11080/20000, loss: 0.0554760, test_acc: 0.9377\n",
      "episode: 11090/20000, loss: 0.0530887, test_acc: 0.9377\n",
      "episode: 11100/20000, loss: 0.0532300, test_acc: 0.9377\n",
      "episode: 11110/20000, loss: 0.0554855, test_acc: 0.9377\n",
      "episode: 11120/20000, loss: 0.0594060, test_acc: 0.9377\n",
      "episode: 11130/20000, loss: 0.0559822, test_acc: 0.9377\n",
      "episode: 11140/20000, loss: 0.0541876, test_acc: 0.9377\n",
      "episode: 11150/20000, loss: 0.0580628, test_acc: 0.9377\n",
      "episode: 11160/20000, loss: 0.0524999, test_acc: 0.9377\n",
      "episode: 11170/20000, loss: 0.0583925, test_acc: 0.9377\n",
      "episode: 11180/20000, loss: 0.0559493, test_acc: 0.9377\n",
      "episode: 11190/20000, loss: 0.0565135, test_acc: 0.9377\n",
      "episode: 11200/20000, loss: 0.0525000, test_acc: 0.9377\n",
      "episode: 11210/20000, loss: 0.0535167, test_acc: 0.9377\n",
      "episode: 11220/20000, loss: 0.0541584, test_acc: 0.9377\n",
      "episode: 11230/20000, loss: 0.0570882, test_acc: 0.9377\n",
      "episode: 11240/20000, loss: 0.0572011, test_acc: 0.9377\n",
      "episode: 11250/20000, loss: 0.0574553, test_acc: 0.9377\n",
      "episode: 11260/20000, loss: 0.0536967, test_acc: 0.9377\n",
      "episode: 11270/20000, loss: 0.0533957, test_acc: 0.9377\n",
      "episode: 11280/20000, loss: 0.0624324, test_acc: 0.9377\n",
      "episode: 11290/20000, loss: 0.0552914, test_acc: 0.9377\n",
      "episode: 11300/20000, loss: 0.0547793, test_acc: 0.9377\n",
      "episode: 11310/20000, loss: 0.0558345, test_acc: 0.9377\n",
      "episode: 11320/20000, loss: 0.0569054, test_acc: 0.9377\n",
      "episode: 11330/20000, loss: 0.0569420, test_acc: 0.9377\n",
      "episode: 11340/20000, loss: 0.0546370, test_acc: 0.9377\n",
      "episode: 11350/20000, loss: 0.0553177, test_acc: 0.9377\n",
      "episode: 11360/20000, loss: 0.0606701, test_acc: 0.9377\n",
      "episode: 11370/20000, loss: 0.0571516, test_acc: 0.9377\n",
      "episode: 11380/20000, loss: 0.0579218, test_acc: 0.9377\n",
      "episode: 11390/20000, loss: 0.0548370, test_acc: 0.9377\n",
      "episode: 11400/20000, loss: 0.0517284, test_acc: 0.9377\n",
      "episode: 11410/20000, loss: 0.0537120, test_acc: 0.9377\n",
      "episode: 11420/20000, loss: 0.0543203, test_acc: 0.9377\n",
      "episode: 11430/20000, loss: 0.0546223, test_acc: 0.9377\n",
      "episode: 11440/20000, loss: 0.0625181, test_acc: 0.9377\n",
      "episode: 11450/20000, loss: 0.0560009, test_acc: 0.9377\n",
      "episode: 11460/20000, loss: 0.0597676, test_acc: 0.9377\n",
      "episode: 11470/20000, loss: 0.0508929, test_acc: 0.9377\n",
      "episode: 11480/20000, loss: 0.0586861, test_acc: 0.9377\n",
      "episode: 11490/20000, loss: 0.0506530, test_acc: 0.9377\n",
      "episode: 11500/20000, loss: 0.0592240, test_acc: 0.9667\n",
      "episode: 11510/20000, loss: 0.0572160, test_acc: 0.9667\n",
      "episode: 11520/20000, loss: 0.0488682, test_acc: 0.9667\n",
      "episode: 11530/20000, loss: 0.0575843, test_acc: 0.9667\n",
      "episode: 11540/20000, loss: 0.0529012, test_acc: 0.9667\n",
      "episode: 11550/20000, loss: 0.0546797, test_acc: 0.9667\n",
      "episode: 11560/20000, loss: 0.0554429, test_acc: 0.9667\n",
      "episode: 11570/20000, loss: 0.0543992, test_acc: 0.9667\n",
      "episode: 11580/20000, loss: 0.0544967, test_acc: 0.9667\n",
      "episode: 11590/20000, loss: 0.0584341, test_acc: 0.9667\n",
      "episode: 11600/20000, loss: 0.0524436, test_acc: 0.9667\n",
      "episode: 11610/20000, loss: 0.0544048, test_acc: 0.9667\n",
      "episode: 11620/20000, loss: 0.0519876, test_acc: 0.9667\n",
      "episode: 11630/20000, loss: 0.0627542, test_acc: 0.9667\n",
      "episode: 11640/20000, loss: 0.0480928, test_acc: 0.9667\n",
      "episode: 11650/20000, loss: 0.0521855, test_acc: 0.9667\n",
      "episode: 11660/20000, loss: 0.0557805, test_acc: 0.9667\n",
      "episode: 11670/20000, loss: 0.0550500, test_acc: 0.9667\n",
      "episode: 11680/20000, loss: 0.0587655, test_acc: 0.9667\n",
      "episode: 11690/20000, loss: 0.0578981, test_acc: 0.9667\n",
      "episode: 11700/20000, loss: 0.0489154, test_acc: 0.9667\n",
      "episode: 11710/20000, loss: 0.0593027, test_acc: 0.9667\n",
      "episode: 11720/20000, loss: 0.0560975, test_acc: 0.9667\n",
      "episode: 11730/20000, loss: 0.0559847, test_acc: 0.9667\n",
      "episode: 11740/20000, loss: 0.0537357, test_acc: 0.9667\n",
      "episode: 11750/20000, loss: 0.0568004, test_acc: 0.9667\n",
      "episode: 11760/20000, loss: 0.0582764, test_acc: 0.9667\n",
      "episode: 11770/20000, loss: 0.0560332, test_acc: 0.9667\n",
      "episode: 11780/20000, loss: 0.0596446, test_acc: 0.9667\n",
      "episode: 11790/20000, loss: 0.0541777, test_acc: 0.9667\n",
      "episode: 11800/20000, loss: 0.0550154, test_acc: 0.9667\n",
      "episode: 11810/20000, loss: 0.0567480, test_acc: 0.9667\n",
      "episode: 11820/20000, loss: 0.0610743, test_acc: 0.9667\n",
      "episode: 11830/20000, loss: 0.0538599, test_acc: 0.9667\n",
      "episode: 11840/20000, loss: 0.0563821, test_acc: 0.9667\n",
      "episode: 11850/20000, loss: 0.0551059, test_acc: 0.9667\n",
      "episode: 11860/20000, loss: 0.0553710, test_acc: 0.9667\n",
      "episode: 11870/20000, loss: 0.0523658, test_acc: 0.9667\n",
      "episode: 11880/20000, loss: 0.0513710, test_acc: 0.9667\n",
      "episode: 11890/20000, loss: 0.0501402, test_acc: 0.9667\n",
      "episode: 11900/20000, loss: 0.0567920, test_acc: 0.9667\n",
      "episode: 11910/20000, loss: 0.0563429, test_acc: 0.9667\n",
      "episode: 11920/20000, loss: 0.0542835, test_acc: 0.9667\n",
      "episode: 11930/20000, loss: 0.0543821, test_acc: 0.9667\n",
      "episode: 11940/20000, loss: 0.0513080, test_acc: 0.9667\n",
      "episode: 11950/20000, loss: 0.0541772, test_acc: 0.9667\n",
      "episode: 11960/20000, loss: 0.0544691, test_acc: 0.9667\n",
      "episode: 11970/20000, loss: 0.0548903, test_acc: 0.9667\n",
      "episode: 11980/20000, loss: 0.0591689, test_acc: 0.9667\n",
      "episode: 11990/20000, loss: 0.0547765, test_acc: 0.9667\n",
      "episode: 12000/20000, loss: 0.0518160, test_acc: 0.9567\n",
      "episode: 12010/20000, loss: 0.0580487, test_acc: 0.9567\n",
      "episode: 12020/20000, loss: 0.0572340, test_acc: 0.9567\n",
      "episode: 12030/20000, loss: 0.0557630, test_acc: 0.9567\n",
      "episode: 12040/20000, loss: 0.0516871, test_acc: 0.9567\n",
      "episode: 12050/20000, loss: 0.0585929, test_acc: 0.9567\n",
      "episode: 12060/20000, loss: 0.0592754, test_acc: 0.9567\n",
      "episode: 12070/20000, loss: 0.0545604, test_acc: 0.9567\n",
      "episode: 12080/20000, loss: 0.0553067, test_acc: 0.9567\n",
      "episode: 12090/20000, loss: 0.0575413, test_acc: 0.9567\n",
      "episode: 12100/20000, loss: 0.0610724, test_acc: 0.9567\n",
      "episode: 12110/20000, loss: 0.0555737, test_acc: 0.9567\n",
      "episode: 12120/20000, loss: 0.0515961, test_acc: 0.9567\n",
      "episode: 12130/20000, loss: 0.0533205, test_acc: 0.9567\n",
      "episode: 12140/20000, loss: 0.0534561, test_acc: 0.9567\n",
      "episode: 12150/20000, loss: 0.0558515, test_acc: 0.9567\n",
      "episode: 12160/20000, loss: 0.0501158, test_acc: 0.9567\n",
      "episode: 12170/20000, loss: 0.0577014, test_acc: 0.9567\n",
      "episode: 12180/20000, loss: 0.0579747, test_acc: 0.9567\n",
      "episode: 12190/20000, loss: 0.0516916, test_acc: 0.9567\n",
      "episode: 12200/20000, loss: 0.0556551, test_acc: 0.9567\n",
      "episode: 12210/20000, loss: 0.0539535, test_acc: 0.9567\n",
      "episode: 12220/20000, loss: 0.0534369, test_acc: 0.9567\n",
      "episode: 12230/20000, loss: 0.0529499, test_acc: 0.9567\n",
      "episode: 12240/20000, loss: 0.0547796, test_acc: 0.9567\n",
      "episode: 12250/20000, loss: 0.0563436, test_acc: 0.9567\n",
      "episode: 12260/20000, loss: 0.0531812, test_acc: 0.9567\n",
      "episode: 12270/20000, loss: 0.0549618, test_acc: 0.9567\n",
      "episode: 12280/20000, loss: 0.0556314, test_acc: 0.9567\n",
      "episode: 12290/20000, loss: 0.0526474, test_acc: 0.9567\n",
      "episode: 12300/20000, loss: 0.0539550, test_acc: 0.9567\n",
      "episode: 12310/20000, loss: 0.0521186, test_acc: 0.9567\n",
      "episode: 12320/20000, loss: 0.0511762, test_acc: 0.9567\n",
      "episode: 12330/20000, loss: 0.0547211, test_acc: 0.9567\n",
      "episode: 12340/20000, loss: 0.0562227, test_acc: 0.9567\n",
      "episode: 12350/20000, loss: 0.0525942, test_acc: 0.9567\n",
      "episode: 12360/20000, loss: 0.0530971, test_acc: 0.9567\n",
      "episode: 12370/20000, loss: 0.0534706, test_acc: 0.9567\n",
      "episode: 12380/20000, loss: 0.0563676, test_acc: 0.9567\n",
      "episode: 12390/20000, loss: 0.0529297, test_acc: 0.9567\n",
      "episode: 12400/20000, loss: 0.0535807, test_acc: 0.9567\n",
      "episode: 12410/20000, loss: 0.0528236, test_acc: 0.9567\n",
      "episode: 12420/20000, loss: 0.0571368, test_acc: 0.9567\n",
      "episode: 12430/20000, loss: 0.0491452, test_acc: 0.9567\n",
      "episode: 12440/20000, loss: 0.0508533, test_acc: 0.9567\n",
      "episode: 12450/20000, loss: 0.0524566, test_acc: 0.9567\n",
      "episode: 12460/20000, loss: 0.0508019, test_acc: 0.9567\n",
      "episode: 12470/20000, loss: 0.0552085, test_acc: 0.9567\n",
      "episode: 12480/20000, loss: 0.0567021, test_acc: 0.9567\n",
      "episode: 12490/20000, loss: 0.0543934, test_acc: 0.9567\n",
      "episode: 12500/20000, loss: 0.0561790, test_acc: 0.9529\n",
      "episode: 12510/20000, loss: 0.0546349, test_acc: 0.9529\n",
      "episode: 12520/20000, loss: 0.0565757, test_acc: 0.9529\n",
      "episode: 12530/20000, loss: 0.0551412, test_acc: 0.9529\n",
      "episode: 12540/20000, loss: 0.0534281, test_acc: 0.9529\n",
      "episode: 12550/20000, loss: 0.0539514, test_acc: 0.9529\n",
      "episode: 12560/20000, loss: 0.0549322, test_acc: 0.9529\n",
      "episode: 12570/20000, loss: 0.0537684, test_acc: 0.9529\n",
      "episode: 12580/20000, loss: 0.0581725, test_acc: 0.9529\n",
      "episode: 12590/20000, loss: 0.0532773, test_acc: 0.9529\n",
      "episode: 12600/20000, loss: 0.0575355, test_acc: 0.9529\n",
      "episode: 12610/20000, loss: 0.0567744, test_acc: 0.9529\n",
      "episode: 12620/20000, loss: 0.0473612, test_acc: 0.9529\n",
      "episode: 12630/20000, loss: 0.0524139, test_acc: 0.9529\n",
      "episode: 12640/20000, loss: 0.0576382, test_acc: 0.9529\n",
      "episode: 12650/20000, loss: 0.0526763, test_acc: 0.9529\n",
      "episode: 12660/20000, loss: 0.0567109, test_acc: 0.9529\n",
      "episode: 12670/20000, loss: 0.0533588, test_acc: 0.9529\n",
      "episode: 12680/20000, loss: 0.0545969, test_acc: 0.9529\n",
      "episode: 12690/20000, loss: 0.0531774, test_acc: 0.9529\n",
      "episode: 12700/20000, loss: 0.0567490, test_acc: 0.9529\n",
      "episode: 12710/20000, loss: 0.0549937, test_acc: 0.9529\n",
      "episode: 12720/20000, loss: 0.0604681, test_acc: 0.9529\n",
      "episode: 12730/20000, loss: 0.0555898, test_acc: 0.9529\n",
      "episode: 12740/20000, loss: 0.0554112, test_acc: 0.9529\n",
      "episode: 12750/20000, loss: 0.0544029, test_acc: 0.9529\n",
      "episode: 12760/20000, loss: 0.0521403, test_acc: 0.9529\n",
      "episode: 12770/20000, loss: 0.0594576, test_acc: 0.9529\n",
      "episode: 12780/20000, loss: 0.0548545, test_acc: 0.9529\n",
      "episode: 12790/20000, loss: 0.0533807, test_acc: 0.9529\n",
      "episode: 12800/20000, loss: 0.0565353, test_acc: 0.9529\n",
      "episode: 12810/20000, loss: 0.0574962, test_acc: 0.9529\n",
      "episode: 12820/20000, loss: 0.0614791, test_acc: 0.9529\n",
      "episode: 12830/20000, loss: 0.0546862, test_acc: 0.9529\n",
      "episode: 12840/20000, loss: 0.0542577, test_acc: 0.9529\n",
      "episode: 12850/20000, loss: 0.0543908, test_acc: 0.9529\n",
      "episode: 12860/20000, loss: 0.0584298, test_acc: 0.9529\n",
      "episode: 12870/20000, loss: 0.0548796, test_acc: 0.9529\n",
      "episode: 12880/20000, loss: 0.0549878, test_acc: 0.9529\n",
      "episode: 12890/20000, loss: 0.0539639, test_acc: 0.9529\n",
      "episode: 12900/20000, loss: 0.0529632, test_acc: 0.9529\n",
      "episode: 12910/20000, loss: 0.0583402, test_acc: 0.9529\n",
      "episode: 12920/20000, loss: 0.0553575, test_acc: 0.9529\n",
      "episode: 12930/20000, loss: 0.0527241, test_acc: 0.9529\n",
      "episode: 12940/20000, loss: 0.0551519, test_acc: 0.9529\n",
      "episode: 12950/20000, loss: 0.0541460, test_acc: 0.9529\n",
      "episode: 12960/20000, loss: 0.0523432, test_acc: 0.9529\n",
      "episode: 12970/20000, loss: 0.0504442, test_acc: 0.9529\n",
      "episode: 12980/20000, loss: 0.0542075, test_acc: 0.9529\n",
      "episode: 12990/20000, loss: 0.0535150, test_acc: 0.9529\n",
      "episode: 13000/20000, loss: 0.0559778, test_acc: 0.9349\n",
      "episode: 13010/20000, loss: 0.0512267, test_acc: 0.9349\n",
      "episode: 13020/20000, loss: 0.0529930, test_acc: 0.9349\n",
      "episode: 13030/20000, loss: 0.0572002, test_acc: 0.9349\n",
      "episode: 13040/20000, loss: 0.0552735, test_acc: 0.9349\n",
      "episode: 13050/20000, loss: 0.0554643, test_acc: 0.9349\n",
      "episode: 13060/20000, loss: 0.0544137, test_acc: 0.9349\n",
      "episode: 13070/20000, loss: 0.0531592, test_acc: 0.9349\n",
      "episode: 13080/20000, loss: 0.0535012, test_acc: 0.9349\n",
      "episode: 13090/20000, loss: 0.0545818, test_acc: 0.9349\n",
      "episode: 13100/20000, loss: 0.0581095, test_acc: 0.9349\n",
      "episode: 13110/20000, loss: 0.0511876, test_acc: 0.9349\n",
      "episode: 13120/20000, loss: 0.0558798, test_acc: 0.9349\n",
      "episode: 13130/20000, loss: 0.0546356, test_acc: 0.9349\n",
      "episode: 13140/20000, loss: 0.0523734, test_acc: 0.9349\n",
      "episode: 13150/20000, loss: 0.0556334, test_acc: 0.9349\n",
      "episode: 13160/20000, loss: 0.0554832, test_acc: 0.9349\n",
      "episode: 13170/20000, loss: 0.0558444, test_acc: 0.9349\n",
      "episode: 13180/20000, loss: 0.0549233, test_acc: 0.9349\n",
      "episode: 13190/20000, loss: 0.0549904, test_acc: 0.9349\n",
      "episode: 13200/20000, loss: 0.0507889, test_acc: 0.9349\n",
      "episode: 13210/20000, loss: 0.0599832, test_acc: 0.9349\n",
      "episode: 13220/20000, loss: 0.0519989, test_acc: 0.9349\n",
      "episode: 13230/20000, loss: 0.0575941, test_acc: 0.9349\n",
      "episode: 13240/20000, loss: 0.0555834, test_acc: 0.9349\n",
      "episode: 13250/20000, loss: 0.0527611, test_acc: 0.9349\n",
      "episode: 13260/20000, loss: 0.0580406, test_acc: 0.9349\n",
      "episode: 13270/20000, loss: 0.0514386, test_acc: 0.9349\n",
      "episode: 13280/20000, loss: 0.0517299, test_acc: 0.9349\n",
      "episode: 13290/20000, loss: 0.0540311, test_acc: 0.9349\n",
      "episode: 13300/20000, loss: 0.0509824, test_acc: 0.9349\n",
      "episode: 13310/20000, loss: 0.0542931, test_acc: 0.9349\n",
      "episode: 13320/20000, loss: 0.0611105, test_acc: 0.9349\n",
      "episode: 13330/20000, loss: 0.0528709, test_acc: 0.9349\n",
      "episode: 13340/20000, loss: 0.0542626, test_acc: 0.9349\n",
      "episode: 13350/20000, loss: 0.0562145, test_acc: 0.9349\n",
      "episode: 13360/20000, loss: 0.0536955, test_acc: 0.9349\n",
      "episode: 13370/20000, loss: 0.0547603, test_acc: 0.9349\n",
      "episode: 13380/20000, loss: 0.0575241, test_acc: 0.9349\n",
      "episode: 13390/20000, loss: 0.0575235, test_acc: 0.9349\n",
      "episode: 13400/20000, loss: 0.0569815, test_acc: 0.9349\n",
      "episode: 13410/20000, loss: 0.0578790, test_acc: 0.9349\n",
      "episode: 13420/20000, loss: 0.0525977, test_acc: 0.9349\n",
      "episode: 13430/20000, loss: 0.0544756, test_acc: 0.9349\n",
      "episode: 13440/20000, loss: 0.0612561, test_acc: 0.9349\n",
      "episode: 13450/20000, loss: 0.0534024, test_acc: 0.9349\n",
      "episode: 13460/20000, loss: 0.0544558, test_acc: 0.9349\n",
      "episode: 13470/20000, loss: 0.0575537, test_acc: 0.9349\n",
      "episode: 13480/20000, loss: 0.0551914, test_acc: 0.9349\n",
      "episode: 13490/20000, loss: 0.0517905, test_acc: 0.9349\n",
      "episode: 13500/20000, loss: 0.0500400, test_acc: 0.9667\n",
      "episode: 13510/20000, loss: 0.0540606, test_acc: 0.9667\n",
      "episode: 13520/20000, loss: 0.0526382, test_acc: 0.9667\n",
      "episode: 13530/20000, loss: 0.0553213, test_acc: 0.9667\n",
      "episode: 13540/20000, loss: 0.0506396, test_acc: 0.9667\n",
      "episode: 13550/20000, loss: 0.0556175, test_acc: 0.9667\n",
      "episode: 13560/20000, loss: 0.0570946, test_acc: 0.9667\n",
      "episode: 13570/20000, loss: 0.0600687, test_acc: 0.9667\n",
      "episode: 13580/20000, loss: 0.0533897, test_acc: 0.9667\n",
      "episode: 13590/20000, loss: 0.0573589, test_acc: 0.9667\n",
      "episode: 13600/20000, loss: 0.0551257, test_acc: 0.9667\n",
      "episode: 13610/20000, loss: 0.0546541, test_acc: 0.9667\n",
      "episode: 13620/20000, loss: 0.0556054, test_acc: 0.9667\n",
      "episode: 13630/20000, loss: 0.0509347, test_acc: 0.9667\n",
      "episode: 13640/20000, loss: 0.0573545, test_acc: 0.9667\n",
      "episode: 13650/20000, loss: 0.0534534, test_acc: 0.9667\n",
      "episode: 13660/20000, loss: 0.0578936, test_acc: 0.9667\n",
      "episode: 13670/20000, loss: 0.0528505, test_acc: 0.9667\n",
      "episode: 13680/20000, loss: 0.0576433, test_acc: 0.9667\n",
      "episode: 13690/20000, loss: 0.0577726, test_acc: 0.9667\n",
      "episode: 13700/20000, loss: 0.0544632, test_acc: 0.9667\n",
      "episode: 13710/20000, loss: 0.0545997, test_acc: 0.9667\n",
      "episode: 13720/20000, loss: 0.0541116, test_acc: 0.9667\n",
      "episode: 13730/20000, loss: 0.0514210, test_acc: 0.9667\n",
      "episode: 13740/20000, loss: 0.0534419, test_acc: 0.9667\n",
      "episode: 13750/20000, loss: 0.0562963, test_acc: 0.9667\n",
      "episode: 13760/20000, loss: 0.0586321, test_acc: 0.9667\n",
      "episode: 13770/20000, loss: 0.0571904, test_acc: 0.9667\n",
      "episode: 13780/20000, loss: 0.0546299, test_acc: 0.9667\n",
      "episode: 13790/20000, loss: 0.0578304, test_acc: 0.9667\n",
      "episode: 13800/20000, loss: 0.0592230, test_acc: 0.9667\n",
      "episode: 13810/20000, loss: 0.0513618, test_acc: 0.9667\n",
      "episode: 13820/20000, loss: 0.0546126, test_acc: 0.9667\n",
      "episode: 13830/20000, loss: 0.0539856, test_acc: 0.9667\n",
      "episode: 13840/20000, loss: 0.0528982, test_acc: 0.9667\n",
      "episode: 13850/20000, loss: 0.0558925, test_acc: 0.9667\n",
      "episode: 13860/20000, loss: 0.0579707, test_acc: 0.9667\n",
      "episode: 13870/20000, loss: 0.0526947, test_acc: 0.9667\n",
      "episode: 13880/20000, loss: 0.0591280, test_acc: 0.9667\n",
      "episode: 13890/20000, loss: 0.0502219, test_acc: 0.9667\n",
      "episode: 13900/20000, loss: 0.0470326, test_acc: 0.9667\n",
      "episode: 13910/20000, loss: 0.0577778, test_acc: 0.9667\n",
      "episode: 13920/20000, loss: 0.0543892, test_acc: 0.9667\n",
      "episode: 13930/20000, loss: 0.0565916, test_acc: 0.9667\n",
      "episode: 13940/20000, loss: 0.0532469, test_acc: 0.9667\n",
      "episode: 13950/20000, loss: 0.0520378, test_acc: 0.9667\n",
      "episode: 13960/20000, loss: 0.0544548, test_acc: 0.9667\n",
      "episode: 13970/20000, loss: 0.0568462, test_acc: 0.9667\n",
      "episode: 13980/20000, loss: 0.0530400, test_acc: 0.9667\n",
      "episode: 13990/20000, loss: 0.0538624, test_acc: 0.9667\n",
      "episode: 14000/20000, loss: 0.0565620, test_acc: 0.9476\n",
      "episode: 14010/20000, loss: 0.0575878, test_acc: 0.9476\n",
      "episode: 14020/20000, loss: 0.0567666, test_acc: 0.9476\n",
      "episode: 14030/20000, loss: 0.0527009, test_acc: 0.9476\n",
      "episode: 14040/20000, loss: 0.0525932, test_acc: 0.9476\n",
      "episode: 14050/20000, loss: 0.0550468, test_acc: 0.9476\n",
      "episode: 14060/20000, loss: 0.0548011, test_acc: 0.9476\n",
      "episode: 14070/20000, loss: 0.0579787, test_acc: 0.9476\n",
      "episode: 14080/20000, loss: 0.0561170, test_acc: 0.9476\n",
      "episode: 14090/20000, loss: 0.0550030, test_acc: 0.9476\n",
      "episode: 14100/20000, loss: 0.0564778, test_acc: 0.9476\n",
      "episode: 14110/20000, loss: 0.0529085, test_acc: 0.9476\n",
      "episode: 14120/20000, loss: 0.0577695, test_acc: 0.9476\n",
      "episode: 14130/20000, loss: 0.0559363, test_acc: 0.9476\n",
      "episode: 14140/20000, loss: 0.0572460, test_acc: 0.9476\n",
      "episode: 14150/20000, loss: 0.0544324, test_acc: 0.9476\n",
      "episode: 14160/20000, loss: 0.0528480, test_acc: 0.9476\n",
      "episode: 14170/20000, loss: 0.0584504, test_acc: 0.9476\n",
      "episode: 14180/20000, loss: 0.0586225, test_acc: 0.9476\n",
      "episode: 14190/20000, loss: 0.0554092, test_acc: 0.9476\n",
      "episode: 14200/20000, loss: 0.0563069, test_acc: 0.9476\n",
      "episode: 14210/20000, loss: 0.0561858, test_acc: 0.9476\n",
      "episode: 14220/20000, loss: 0.0561861, test_acc: 0.9476\n",
      "episode: 14230/20000, loss: 0.0529837, test_acc: 0.9476\n",
      "episode: 14240/20000, loss: 0.0536388, test_acc: 0.9476\n",
      "episode: 14250/20000, loss: 0.0585857, test_acc: 0.9476\n",
      "episode: 14260/20000, loss: 0.0579477, test_acc: 0.9476\n",
      "episode: 14270/20000, loss: 0.0548421, test_acc: 0.9476\n",
      "episode: 14280/20000, loss: 0.0574499, test_acc: 0.9476\n",
      "episode: 14290/20000, loss: 0.0564306, test_acc: 0.9476\n",
      "episode: 14300/20000, loss: 0.0557552, test_acc: 0.9476\n",
      "episode: 14310/20000, loss: 0.0507763, test_acc: 0.9476\n",
      "episode: 14320/20000, loss: 0.0547189, test_acc: 0.9476\n",
      "episode: 14330/20000, loss: 0.0585414, test_acc: 0.9476\n",
      "episode: 14340/20000, loss: 0.0556406, test_acc: 0.9476\n",
      "episode: 14350/20000, loss: 0.0554666, test_acc: 0.9476\n",
      "episode: 14360/20000, loss: 0.0564808, test_acc: 0.9476\n",
      "episode: 14370/20000, loss: 0.0551447, test_acc: 0.9476\n",
      "episode: 14380/20000, loss: 0.0545590, test_acc: 0.9476\n",
      "episode: 14390/20000, loss: 0.0549061, test_acc: 0.9476\n",
      "episode: 14400/20000, loss: 0.0544183, test_acc: 0.9476\n",
      "episode: 14410/20000, loss: 0.0558340, test_acc: 0.9476\n",
      "episode: 14420/20000, loss: 0.0566540, test_acc: 0.9476\n",
      "episode: 14430/20000, loss: 0.0579058, test_acc: 0.9476\n",
      "episode: 14440/20000, loss: 0.0562465, test_acc: 0.9476\n",
      "episode: 14450/20000, loss: 0.0542778, test_acc: 0.9476\n",
      "episode: 14460/20000, loss: 0.0572180, test_acc: 0.9476\n",
      "episode: 14470/20000, loss: 0.0588310, test_acc: 0.9476\n",
      "episode: 14480/20000, loss: 0.0545719, test_acc: 0.9476\n",
      "episode: 14490/20000, loss: 0.0554603, test_acc: 0.9476\n",
      "episode: 14500/20000, loss: 0.0555199, test_acc: 0.9810\n",
      "episode: 14510/20000, loss: 0.0546860, test_acc: 0.9810\n",
      "episode: 14520/20000, loss: 0.0545169, test_acc: 0.9810\n",
      "episode: 14530/20000, loss: 0.0574081, test_acc: 0.9810\n",
      "episode: 14540/20000, loss: 0.0587791, test_acc: 0.9810\n",
      "episode: 14550/20000, loss: 0.0568772, test_acc: 0.9810\n",
      "episode: 14560/20000, loss: 0.0512956, test_acc: 0.9810\n",
      "episode: 14570/20000, loss: 0.0561346, test_acc: 0.9810\n",
      "episode: 14580/20000, loss: 0.0522145, test_acc: 0.9810\n",
      "episode: 14590/20000, loss: 0.0571271, test_acc: 0.9810\n",
      "episode: 14600/20000, loss: 0.0528622, test_acc: 0.9810\n",
      "episode: 14610/20000, loss: 0.0546063, test_acc: 0.9810\n",
      "episode: 14620/20000, loss: 0.0514498, test_acc: 0.9810\n",
      "episode: 14630/20000, loss: 0.0536758, test_acc: 0.9810\n",
      "episode: 14640/20000, loss: 0.0574075, test_acc: 0.9810\n",
      "episode: 14650/20000, loss: 0.0554191, test_acc: 0.9810\n",
      "episode: 14660/20000, loss: 0.0555713, test_acc: 0.9810\n",
      "episode: 14670/20000, loss: 0.0531824, test_acc: 0.9810\n",
      "episode: 14680/20000, loss: 0.0518948, test_acc: 0.9810\n",
      "episode: 14690/20000, loss: 0.0543063, test_acc: 0.9810\n",
      "episode: 14700/20000, loss: 0.0515386, test_acc: 0.9810\n",
      "episode: 14710/20000, loss: 0.0579276, test_acc: 0.9810\n",
      "episode: 14720/20000, loss: 0.0555708, test_acc: 0.9810\n",
      "episode: 14730/20000, loss: 0.0546453, test_acc: 0.9810\n",
      "episode: 14740/20000, loss: 0.0553005, test_acc: 0.9810\n",
      "episode: 14750/20000, loss: 0.0508672, test_acc: 0.9810\n",
      "episode: 14760/20000, loss: 0.0593204, test_acc: 0.9810\n",
      "episode: 14770/20000, loss: 0.0518307, test_acc: 0.9810\n",
      "episode: 14780/20000, loss: 0.0517264, test_acc: 0.9810\n",
      "episode: 14790/20000, loss: 0.0552621, test_acc: 0.9810\n",
      "episode: 14800/20000, loss: 0.0503888, test_acc: 0.9810\n",
      "episode: 14810/20000, loss: 0.0536627, test_acc: 0.9810\n",
      "episode: 14820/20000, loss: 0.0556104, test_acc: 0.9810\n",
      "episode: 14830/20000, loss: 0.0530376, test_acc: 0.9810\n",
      "episode: 14840/20000, loss: 0.0546595, test_acc: 0.9810\n",
      "episode: 14850/20000, loss: 0.0559938, test_acc: 0.9810\n",
      "episode: 14860/20000, loss: 0.0555027, test_acc: 0.9810\n",
      "episode: 14870/20000, loss: 0.0592014, test_acc: 0.9810\n",
      "episode: 14880/20000, loss: 0.0546551, test_acc: 0.9810\n",
      "episode: 14890/20000, loss: 0.0581155, test_acc: 0.9810\n",
      "episode: 14900/20000, loss: 0.0582405, test_acc: 0.9810\n",
      "episode: 14910/20000, loss: 0.0567977, test_acc: 0.9810\n",
      "episode: 14920/20000, loss: 0.0530413, test_acc: 0.9810\n",
      "episode: 14930/20000, loss: 0.0573677, test_acc: 0.9810\n",
      "episode: 14940/20000, loss: 0.0545726, test_acc: 0.9810\n",
      "episode: 14950/20000, loss: 0.0534517, test_acc: 0.9810\n",
      "episode: 14960/20000, loss: 0.0567881, test_acc: 0.9810\n",
      "episode: 14970/20000, loss: 0.0529111, test_acc: 0.9810\n",
      "episode: 14980/20000, loss: 0.0559595, test_acc: 0.9810\n",
      "episode: 14990/20000, loss: 0.0517338, test_acc: 0.9810\n",
      "episode: 15000/20000, loss: 0.0557315, test_acc: 0.9785\n",
      "episode: 15010/20000, loss: 0.0554796, test_acc: 0.9785\n",
      "episode: 15020/20000, loss: 0.0590931, test_acc: 0.9785\n",
      "episode: 15030/20000, loss: 0.0551295, test_acc: 0.9785\n",
      "episode: 15040/20000, loss: 0.0578065, test_acc: 0.9785\n",
      "episode: 15050/20000, loss: 0.0581269, test_acc: 0.9785\n",
      "episode: 15060/20000, loss: 0.0577115, test_acc: 0.9785\n",
      "episode: 15070/20000, loss: 0.0551489, test_acc: 0.9785\n",
      "episode: 15080/20000, loss: 0.0583327, test_acc: 0.9785\n",
      "episode: 15090/20000, loss: 0.0566744, test_acc: 0.9785\n",
      "episode: 15100/20000, loss: 0.0523578, test_acc: 0.9785\n",
      "episode: 15110/20000, loss: 0.0519701, test_acc: 0.9785\n",
      "episode: 15120/20000, loss: 0.0537684, test_acc: 0.9785\n",
      "episode: 15130/20000, loss: 0.0561133, test_acc: 0.9785\n",
      "episode: 15140/20000, loss: 0.0559601, test_acc: 0.9785\n",
      "episode: 15150/20000, loss: 0.0546250, test_acc: 0.9785\n",
      "episode: 15160/20000, loss: 0.0590050, test_acc: 0.9785\n",
      "episode: 15170/20000, loss: 0.0548601, test_acc: 0.9785\n",
      "episode: 15180/20000, loss: 0.0567039, test_acc: 0.9785\n",
      "episode: 15190/20000, loss: 0.0541757, test_acc: 0.9785\n",
      "episode: 15200/20000, loss: 0.0576060, test_acc: 0.9785\n",
      "episode: 15210/20000, loss: 0.0567832, test_acc: 0.9785\n",
      "episode: 15220/20000, loss: 0.0516356, test_acc: 0.9785\n",
      "episode: 15230/20000, loss: 0.0578002, test_acc: 0.9785\n",
      "episode: 15240/20000, loss: 0.0496306, test_acc: 0.9785\n",
      "episode: 15250/20000, loss: 0.0577879, test_acc: 0.9785\n",
      "episode: 15260/20000, loss: 0.0522930, test_acc: 0.9785\n",
      "episode: 15270/20000, loss: 0.0509947, test_acc: 0.9785\n",
      "episode: 15280/20000, loss: 0.0532298, test_acc: 0.9785\n",
      "episode: 15290/20000, loss: 0.0604146, test_acc: 0.9785\n",
      "episode: 15300/20000, loss: 0.0558142, test_acc: 0.9785\n",
      "episode: 15310/20000, loss: 0.0534908, test_acc: 0.9785\n",
      "episode: 15320/20000, loss: 0.0521456, test_acc: 0.9785\n",
      "episode: 15330/20000, loss: 0.0550599, test_acc: 0.9785\n",
      "episode: 15340/20000, loss: 0.0558014, test_acc: 0.9785\n",
      "episode: 15350/20000, loss: 0.0532441, test_acc: 0.9785\n",
      "episode: 15360/20000, loss: 0.0545020, test_acc: 0.9785\n",
      "episode: 15370/20000, loss: 0.0563128, test_acc: 0.9785\n",
      "episode: 15380/20000, loss: 0.0565633, test_acc: 0.9785\n",
      "episode: 15390/20000, loss: 0.0525366, test_acc: 0.9785\n",
      "episode: 15400/20000, loss: 0.0541918, test_acc: 0.9785\n",
      "episode: 15410/20000, loss: 0.0542606, test_acc: 0.9785\n",
      "episode: 15420/20000, loss: 0.0602168, test_acc: 0.9785\n",
      "episode: 15430/20000, loss: 0.0563821, test_acc: 0.9785\n",
      "episode: 15440/20000, loss: 0.0567770, test_acc: 0.9785\n",
      "episode: 15450/20000, loss: 0.0565708, test_acc: 0.9785\n",
      "episode: 15460/20000, loss: 0.0529188, test_acc: 0.9785\n",
      "episode: 15470/20000, loss: 0.0550374, test_acc: 0.9785\n",
      "episode: 15480/20000, loss: 0.0563382, test_acc: 0.9785\n",
      "episode: 15490/20000, loss: 0.0544632, test_acc: 0.9785\n",
      "episode: 15500/20000, loss: 0.0569282, test_acc: 0.9549\n",
      "episode: 15510/20000, loss: 0.0508729, test_acc: 0.9549\n",
      "episode: 15520/20000, loss: 0.0479578, test_acc: 0.9549\n",
      "episode: 15530/20000, loss: 0.0541864, test_acc: 0.9549\n",
      "episode: 15540/20000, loss: 0.0524530, test_acc: 0.9549\n",
      "episode: 15550/20000, loss: 0.0540899, test_acc: 0.9549\n",
      "episode: 15560/20000, loss: 0.0525551, test_acc: 0.9549\n",
      "episode: 15570/20000, loss: 0.0532294, test_acc: 0.9549\n",
      "episode: 15580/20000, loss: 0.0501366, test_acc: 0.9549\n",
      "episode: 15590/20000, loss: 0.0543248, test_acc: 0.9549\n",
      "episode: 15600/20000, loss: 0.0532006, test_acc: 0.9549\n",
      "episode: 15610/20000, loss: 0.0559357, test_acc: 0.9549\n",
      "episode: 15620/20000, loss: 0.0545377, test_acc: 0.9549\n",
      "episode: 15630/20000, loss: 0.0545698, test_acc: 0.9549\n",
      "episode: 15640/20000, loss: 0.0618116, test_acc: 0.9549\n",
      "episode: 15650/20000, loss: 0.0557487, test_acc: 0.9549\n",
      "episode: 15660/20000, loss: 0.0496307, test_acc: 0.9549\n",
      "episode: 15670/20000, loss: 0.0592138, test_acc: 0.9549\n",
      "episode: 15680/20000, loss: 0.0590435, test_acc: 0.9549\n",
      "episode: 15690/20000, loss: 0.0546536, test_acc: 0.9549\n",
      "episode: 15700/20000, loss: 0.0550202, test_acc: 0.9549\n",
      "episode: 15710/20000, loss: 0.0556779, test_acc: 0.9549\n",
      "episode: 15720/20000, loss: 0.0514427, test_acc: 0.9549\n",
      "episode: 15730/20000, loss: 0.0586690, test_acc: 0.9549\n",
      "episode: 15740/20000, loss: 0.0565576, test_acc: 0.9549\n",
      "episode: 15750/20000, loss: 0.0606740, test_acc: 0.9549\n",
      "episode: 15760/20000, loss: 0.0593079, test_acc: 0.9549\n",
      "episode: 15770/20000, loss: 0.0507619, test_acc: 0.9549\n",
      "episode: 15780/20000, loss: 0.0548173, test_acc: 0.9549\n",
      "episode: 15790/20000, loss: 0.0546679, test_acc: 0.9549\n",
      "episode: 15800/20000, loss: 0.0575552, test_acc: 0.9549\n",
      "episode: 15810/20000, loss: 0.0571246, test_acc: 0.9549\n",
      "episode: 15820/20000, loss: 0.0581554, test_acc: 0.9549\n",
      "episode: 15830/20000, loss: 0.0543702, test_acc: 0.9549\n",
      "episode: 15840/20000, loss: 0.0507866, test_acc: 0.9549\n",
      "episode: 15850/20000, loss: 0.0541169, test_acc: 0.9549\n",
      "episode: 15860/20000, loss: 0.0579192, test_acc: 0.9549\n",
      "episode: 15870/20000, loss: 0.0535204, test_acc: 0.9549\n",
      "episode: 15880/20000, loss: 0.0516277, test_acc: 0.9549\n",
      "episode: 15890/20000, loss: 0.0518895, test_acc: 0.9549\n",
      "episode: 15900/20000, loss: 0.0577479, test_acc: 0.9549\n",
      "episode: 15910/20000, loss: 0.0513518, test_acc: 0.9549\n",
      "episode: 15920/20000, loss: 0.0529526, test_acc: 0.9549\n",
      "episode: 15930/20000, loss: 0.0543588, test_acc: 0.9549\n",
      "episode: 15940/20000, loss: 0.0550724, test_acc: 0.9549\n",
      "episode: 15950/20000, loss: 0.0565239, test_acc: 0.9549\n",
      "episode: 15960/20000, loss: 0.0536012, test_acc: 0.9549\n",
      "episode: 15970/20000, loss: 0.0529971, test_acc: 0.9549\n",
      "episode: 15980/20000, loss: 0.0515489, test_acc: 0.9549\n",
      "episode: 15990/20000, loss: 0.0567481, test_acc: 0.9549\n",
      "episode: 16000/20000, loss: 0.0543580, test_acc: 0.9595\n",
      "episode: 16010/20000, loss: 0.0511793, test_acc: 0.9595\n",
      "episode: 16020/20000, loss: 0.0512292, test_acc: 0.9595\n",
      "episode: 16030/20000, loss: 0.0585374, test_acc: 0.9595\n",
      "episode: 16040/20000, loss: 0.0546019, test_acc: 0.9595\n",
      "episode: 16050/20000, loss: 0.0565667, test_acc: 0.9595\n",
      "episode: 16060/20000, loss: 0.0501700, test_acc: 0.9595\n",
      "episode: 16070/20000, loss: 0.0566473, test_acc: 0.9595\n",
      "episode: 16080/20000, loss: 0.0530309, test_acc: 0.9595\n",
      "episode: 16090/20000, loss: 0.0516699, test_acc: 0.9595\n",
      "episode: 16100/20000, loss: 0.0542835, test_acc: 0.9595\n",
      "episode: 16110/20000, loss: 0.0553762, test_acc: 0.9595\n",
      "episode: 16120/20000, loss: 0.0507537, test_acc: 0.9595\n",
      "episode: 16130/20000, loss: 0.0560687, test_acc: 0.9595\n",
      "episode: 16140/20000, loss: 0.0531171, test_acc: 0.9595\n",
      "episode: 16150/20000, loss: 0.0565584, test_acc: 0.9595\n",
      "episode: 16160/20000, loss: 0.0544738, test_acc: 0.9595\n",
      "episode: 16170/20000, loss: 0.0505544, test_acc: 0.9595\n",
      "episode: 16180/20000, loss: 0.0572533, test_acc: 0.9595\n",
      "episode: 16190/20000, loss: 0.0565863, test_acc: 0.9595\n",
      "episode: 16200/20000, loss: 0.0544388, test_acc: 0.9595\n",
      "episode: 16210/20000, loss: 0.0566591, test_acc: 0.9595\n",
      "episode: 16220/20000, loss: 0.0537828, test_acc: 0.9595\n",
      "episode: 16230/20000, loss: 0.0545864, test_acc: 0.9595\n",
      "episode: 16240/20000, loss: 0.0570629, test_acc: 0.9595\n",
      "episode: 16250/20000, loss: 0.0587278, test_acc: 0.9595\n",
      "episode: 16260/20000, loss: 0.0521717, test_acc: 0.9595\n",
      "episode: 16270/20000, loss: 0.0557889, test_acc: 0.9595\n",
      "episode: 16280/20000, loss: 0.0487159, test_acc: 0.9595\n",
      "episode: 16290/20000, loss: 0.0561684, test_acc: 0.9595\n",
      "episode: 16300/20000, loss: 0.0552339, test_acc: 0.9595\n",
      "episode: 16310/20000, loss: 0.0556981, test_acc: 0.9595\n",
      "episode: 16320/20000, loss: 0.0568947, test_acc: 0.9595\n",
      "episode: 16330/20000, loss: 0.0568890, test_acc: 0.9595\n",
      "episode: 16340/20000, loss: 0.0559605, test_acc: 0.9595\n",
      "episode: 16350/20000, loss: 0.0550701, test_acc: 0.9595\n",
      "episode: 16360/20000, loss: 0.0555380, test_acc: 0.9595\n",
      "episode: 16370/20000, loss: 0.0519333, test_acc: 0.9595\n",
      "episode: 16380/20000, loss: 0.0574564, test_acc: 0.9595\n",
      "episode: 16390/20000, loss: 0.0561334, test_acc: 0.9595\n",
      "episode: 16400/20000, loss: 0.0560997, test_acc: 0.9595\n",
      "episode: 16410/20000, loss: 0.0552760, test_acc: 0.9595\n",
      "episode: 16420/20000, loss: 0.0538922, test_acc: 0.9595\n",
      "episode: 16430/20000, loss: 0.0567544, test_acc: 0.9595\n",
      "episode: 16440/20000, loss: 0.0537792, test_acc: 0.9595\n",
      "episode: 16450/20000, loss: 0.0536665, test_acc: 0.9595\n",
      "episode: 16460/20000, loss: 0.0561239, test_acc: 0.9595\n",
      "episode: 16470/20000, loss: 0.0546631, test_acc: 0.9595\n",
      "episode: 16480/20000, loss: 0.0578761, test_acc: 0.9595\n",
      "episode: 16490/20000, loss: 0.0497279, test_acc: 0.9595\n",
      "episode: 16500/20000, loss: 0.0539054, test_acc: 0.9296\n",
      "episode: 16510/20000, loss: 0.0576432, test_acc: 0.9296\n",
      "episode: 16520/20000, loss: 0.0557951, test_acc: 0.9296\n",
      "episode: 16530/20000, loss: 0.0542474, test_acc: 0.9296\n",
      "episode: 16540/20000, loss: 0.0554804, test_acc: 0.9296\n",
      "episode: 16550/20000, loss: 0.0581439, test_acc: 0.9296\n",
      "episode: 16560/20000, loss: 0.0531855, test_acc: 0.9296\n",
      "episode: 16570/20000, loss: 0.0542984, test_acc: 0.9296\n",
      "episode: 16580/20000, loss: 0.0554099, test_acc: 0.9296\n",
      "episode: 16590/20000, loss: 0.0559083, test_acc: 0.9296\n",
      "episode: 16600/20000, loss: 0.0559298, test_acc: 0.9296\n",
      "episode: 16610/20000, loss: 0.0558478, test_acc: 0.9296\n",
      "episode: 16620/20000, loss: 0.0543157, test_acc: 0.9296\n",
      "episode: 16630/20000, loss: 0.0570336, test_acc: 0.9296\n",
      "episode: 16640/20000, loss: 0.0482890, test_acc: 0.9296\n",
      "episode: 16650/20000, loss: 0.0518904, test_acc: 0.9296\n",
      "episode: 16660/20000, loss: 0.0468949, test_acc: 0.9296\n",
      "episode: 16670/20000, loss: 0.0547639, test_acc: 0.9296\n",
      "episode: 16680/20000, loss: 0.0608565, test_acc: 0.9296\n",
      "episode: 16690/20000, loss: 0.0571884, test_acc: 0.9296\n",
      "episode: 16700/20000, loss: 0.0559414, test_acc: 0.9296\n",
      "episode: 16710/20000, loss: 0.0530874, test_acc: 0.9296\n",
      "episode: 16720/20000, loss: 0.0524066, test_acc: 0.9296\n",
      "episode: 16730/20000, loss: 0.0538117, test_acc: 0.9296\n",
      "episode: 16740/20000, loss: 0.0547316, test_acc: 0.9296\n",
      "episode: 16750/20000, loss: 0.0559414, test_acc: 0.9296\n",
      "episode: 16760/20000, loss: 0.0491776, test_acc: 0.9296\n",
      "episode: 16770/20000, loss: 0.0525731, test_acc: 0.9296\n",
      "episode: 16780/20000, loss: 0.0544716, test_acc: 0.9296\n",
      "episode: 16790/20000, loss: 0.0575718, test_acc: 0.9296\n",
      "episode: 16800/20000, loss: 0.0554227, test_acc: 0.9296\n",
      "episode: 16810/20000, loss: 0.0547604, test_acc: 0.9296\n",
      "episode: 16820/20000, loss: 0.0542498, test_acc: 0.9296\n",
      "episode: 16830/20000, loss: 0.0532543, test_acc: 0.9296\n",
      "episode: 16840/20000, loss: 0.0555217, test_acc: 0.9296\n",
      "episode: 16850/20000, loss: 0.0525029, test_acc: 0.9296\n",
      "episode: 16860/20000, loss: 0.0558151, test_acc: 0.9296\n",
      "episode: 16870/20000, loss: 0.0563616, test_acc: 0.9296\n",
      "episode: 16880/20000, loss: 0.0564912, test_acc: 0.9296\n",
      "episode: 16890/20000, loss: 0.0559039, test_acc: 0.9296\n",
      "episode: 16900/20000, loss: 0.0519579, test_acc: 0.9296\n",
      "episode: 16910/20000, loss: 0.0509106, test_acc: 0.9296\n",
      "episode: 16920/20000, loss: 0.0554262, test_acc: 0.9296\n",
      "episode: 16930/20000, loss: 0.0540321, test_acc: 0.9296\n",
      "episode: 16940/20000, loss: 0.0542856, test_acc: 0.9296\n",
      "episode: 16950/20000, loss: 0.0552762, test_acc: 0.9296\n",
      "episode: 16960/20000, loss: 0.0558591, test_acc: 0.9296\n",
      "episode: 16970/20000, loss: 0.0543413, test_acc: 0.9296\n",
      "episode: 16980/20000, loss: 0.0546432, test_acc: 0.9296\n",
      "episode: 16990/20000, loss: 0.0492902, test_acc: 0.9296\n",
      "episode: 17000/20000, loss: 0.0531152, test_acc: 0.9608\n",
      "episode: 17010/20000, loss: 0.0579938, test_acc: 0.9608\n",
      "episode: 17020/20000, loss: 0.0575398, test_acc: 0.9608\n",
      "episode: 17030/20000, loss: 0.0536526, test_acc: 0.9608\n",
      "episode: 17040/20000, loss: 0.0501970, test_acc: 0.9608\n",
      "episode: 17050/20000, loss: 0.0578647, test_acc: 0.9608\n",
      "episode: 17060/20000, loss: 0.0532747, test_acc: 0.9608\n",
      "episode: 17070/20000, loss: 0.0552437, test_acc: 0.9608\n",
      "episode: 17080/20000, loss: 0.0536654, test_acc: 0.9608\n",
      "episode: 17090/20000, loss: 0.0557522, test_acc: 0.9608\n",
      "episode: 17100/20000, loss: 0.0581615, test_acc: 0.9608\n",
      "episode: 17110/20000, loss: 0.0545610, test_acc: 0.9608\n",
      "episode: 17120/20000, loss: 0.0550291, test_acc: 0.9608\n",
      "episode: 17130/20000, loss: 0.0595793, test_acc: 0.9608\n",
      "episode: 17140/20000, loss: 0.0524804, test_acc: 0.9608\n",
      "episode: 17150/20000, loss: 0.0555356, test_acc: 0.9608\n",
      "episode: 17160/20000, loss: 0.0585656, test_acc: 0.9608\n",
      "episode: 17170/20000, loss: 0.0546335, test_acc: 0.9608\n",
      "episode: 17180/20000, loss: 0.0588494, test_acc: 0.9608\n",
      "episode: 17190/20000, loss: 0.0554818, test_acc: 0.9608\n",
      "episode: 17200/20000, loss: 0.0548819, test_acc: 0.9608\n",
      "episode: 17210/20000, loss: 0.0538482, test_acc: 0.9608\n",
      "episode: 17220/20000, loss: 0.0557522, test_acc: 0.9608\n",
      "episode: 17230/20000, loss: 0.0550540, test_acc: 0.9608\n",
      "episode: 17240/20000, loss: 0.0539008, test_acc: 0.9608\n",
      "episode: 17250/20000, loss: 0.0567486, test_acc: 0.9608\n",
      "episode: 17260/20000, loss: 0.0564515, test_acc: 0.9608\n",
      "episode: 17270/20000, loss: 0.0595239, test_acc: 0.9608\n",
      "episode: 17280/20000, loss: 0.0550849, test_acc: 0.9608\n",
      "episode: 17290/20000, loss: 0.0556494, test_acc: 0.9608\n",
      "episode: 17300/20000, loss: 0.0546058, test_acc: 0.9608\n",
      "episode: 17310/20000, loss: 0.0535373, test_acc: 0.9608\n",
      "episode: 17320/20000, loss: 0.0546814, test_acc: 0.9608\n",
      "episode: 17330/20000, loss: 0.0569941, test_acc: 0.9608\n",
      "episode: 17340/20000, loss: 0.0573687, test_acc: 0.9608\n",
      "episode: 17350/20000, loss: 0.0594532, test_acc: 0.9608\n",
      "episode: 17360/20000, loss: 0.0559609, test_acc: 0.9608\n",
      "episode: 17370/20000, loss: 0.0529307, test_acc: 0.9608\n",
      "episode: 17380/20000, loss: 0.0538007, test_acc: 0.9608\n",
      "episode: 17390/20000, loss: 0.0549077, test_acc: 0.9608\n",
      "episode: 17400/20000, loss: 0.0538132, test_acc: 0.9608\n",
      "episode: 17410/20000, loss: 0.0527057, test_acc: 0.9608\n",
      "episode: 17420/20000, loss: 0.0503427, test_acc: 0.9608\n",
      "episode: 17430/20000, loss: 0.0533371, test_acc: 0.9608\n",
      "episode: 17440/20000, loss: 0.0580290, test_acc: 0.9608\n",
      "episode: 17450/20000, loss: 0.0583788, test_acc: 0.9608\n",
      "episode: 17460/20000, loss: 0.0581713, test_acc: 0.9608\n",
      "episode: 17470/20000, loss: 0.0551098, test_acc: 0.9608\n",
      "episode: 17480/20000, loss: 0.0582214, test_acc: 0.9608\n",
      "episode: 17490/20000, loss: 0.0548973, test_acc: 0.9608\n",
      "episode: 17500/20000, loss: 0.0582633, test_acc: 0.9518\n",
      "episode: 17510/20000, loss: 0.0557555, test_acc: 0.9518\n",
      "episode: 17520/20000, loss: 0.0538252, test_acc: 0.9518\n",
      "episode: 17530/20000, loss: 0.0561984, test_acc: 0.9518\n",
      "episode: 17540/20000, loss: 0.0543171, test_acc: 0.9518\n",
      "episode: 17550/20000, loss: 0.0583839, test_acc: 0.9518\n",
      "episode: 17560/20000, loss: 0.0561987, test_acc: 0.9518\n",
      "episode: 17570/20000, loss: 0.0515434, test_acc: 0.9518\n",
      "episode: 17580/20000, loss: 0.0565263, test_acc: 0.9518\n",
      "episode: 17590/20000, loss: 0.0540519, test_acc: 0.9518\n",
      "episode: 17600/20000, loss: 0.0562168, test_acc: 0.9518\n",
      "episode: 17610/20000, loss: 0.0527232, test_acc: 0.9518\n",
      "episode: 17620/20000, loss: 0.0555343, test_acc: 0.9518\n",
      "episode: 17630/20000, loss: 0.0489624, test_acc: 0.9518\n",
      "episode: 17640/20000, loss: 0.0558494, test_acc: 0.9518\n",
      "episode: 17650/20000, loss: 0.0542582, test_acc: 0.9518\n",
      "episode: 17660/20000, loss: 0.0577715, test_acc: 0.9518\n",
      "episode: 17670/20000, loss: 0.0545110, test_acc: 0.9518\n",
      "episode: 17680/20000, loss: 0.0541149, test_acc: 0.9518\n",
      "episode: 17690/20000, loss: 0.0564127, test_acc: 0.9518\n",
      "episode: 17700/20000, loss: 0.0550675, test_acc: 0.9518\n",
      "episode: 17710/20000, loss: 0.0595672, test_acc: 0.9518\n",
      "episode: 17720/20000, loss: 0.0527169, test_acc: 0.9518\n",
      "episode: 17730/20000, loss: 0.0600340, test_acc: 0.9518\n",
      "episode: 17740/20000, loss: 0.0588921, test_acc: 0.9518\n",
      "episode: 17750/20000, loss: 0.0590943, test_acc: 0.9518\n",
      "episode: 17760/20000, loss: 0.0538424, test_acc: 0.9518\n",
      "episode: 17770/20000, loss: 0.0580716, test_acc: 0.9518\n",
      "episode: 17780/20000, loss: 0.0529428, test_acc: 0.9518\n",
      "episode: 17790/20000, loss: 0.0514553, test_acc: 0.9518\n",
      "episode: 17800/20000, loss: 0.0567023, test_acc: 0.9518\n",
      "episode: 17810/20000, loss: 0.0574147, test_acc: 0.9518\n",
      "episode: 17820/20000, loss: 0.0567892, test_acc: 0.9518\n",
      "episode: 17830/20000, loss: 0.0592694, test_acc: 0.9518\n",
      "episode: 17840/20000, loss: 0.0562768, test_acc: 0.9518\n",
      "episode: 17850/20000, loss: 0.0494119, test_acc: 0.9518\n",
      "episode: 17860/20000, loss: 0.0569796, test_acc: 0.9518\n",
      "episode: 17870/20000, loss: 0.0520212, test_acc: 0.9518\n",
      "episode: 17880/20000, loss: 0.0538859, test_acc: 0.9518\n",
      "episode: 17890/20000, loss: 0.0599229, test_acc: 0.9518\n",
      "episode: 17900/20000, loss: 0.0558548, test_acc: 0.9518\n",
      "episode: 17910/20000, loss: 0.0543275, test_acc: 0.9518\n",
      "episode: 17920/20000, loss: 0.0553433, test_acc: 0.9518\n",
      "episode: 17930/20000, loss: 0.0571594, test_acc: 0.9518\n",
      "episode: 17940/20000, loss: 0.0545765, test_acc: 0.9518\n",
      "episode: 17950/20000, loss: 0.0562881, test_acc: 0.9518\n",
      "episode: 17960/20000, loss: 0.0506856, test_acc: 0.9518\n",
      "episode: 17970/20000, loss: 0.0518457, test_acc: 0.9518\n",
      "episode: 17980/20000, loss: 0.0581239, test_acc: 0.9518\n",
      "episode: 17990/20000, loss: 0.0524292, test_acc: 0.9518\n",
      "episode: 18000/20000, loss: 0.0601143, test_acc: 0.9756\n",
      "episode: 18010/20000, loss: 0.0567756, test_acc: 0.9756\n",
      "episode: 18020/20000, loss: 0.0517585, test_acc: 0.9756\n",
      "episode: 18030/20000, loss: 0.0543316, test_acc: 0.9756\n",
      "episode: 18040/20000, loss: 0.0575401, test_acc: 0.9756\n",
      "episode: 18050/20000, loss: 0.0531890, test_acc: 0.9756\n",
      "episode: 18060/20000, loss: 0.0529243, test_acc: 0.9756\n",
      "episode: 18070/20000, loss: 0.0519847, test_acc: 0.9756\n",
      "episode: 18080/20000, loss: 0.0556362, test_acc: 0.9756\n",
      "episode: 18090/20000, loss: 0.0568400, test_acc: 0.9756\n",
      "episode: 18100/20000, loss: 0.0560349, test_acc: 0.9756\n",
      "episode: 18110/20000, loss: 0.0548565, test_acc: 0.9756\n",
      "episode: 18120/20000, loss: 0.0544677, test_acc: 0.9756\n",
      "episode: 18130/20000, loss: 0.0568434, test_acc: 0.9756\n",
      "episode: 18140/20000, loss: 0.0583693, test_acc: 0.9756\n",
      "episode: 18150/20000, loss: 0.0554830, test_acc: 0.9756\n",
      "episode: 18160/20000, loss: 0.0602382, test_acc: 0.9756\n",
      "episode: 18170/20000, loss: 0.0534541, test_acc: 0.9756\n",
      "episode: 18180/20000, loss: 0.0569142, test_acc: 0.9756\n",
      "episode: 18190/20000, loss: 0.0476351, test_acc: 0.9756\n",
      "episode: 18200/20000, loss: 0.0595942, test_acc: 0.9756\n",
      "episode: 18210/20000, loss: 0.0577240, test_acc: 0.9756\n",
      "episode: 18220/20000, loss: 0.0513456, test_acc: 0.9756\n",
      "episode: 18230/20000, loss: 0.0533250, test_acc: 0.9756\n",
      "episode: 18240/20000, loss: 0.0573065, test_acc: 0.9756\n",
      "episode: 18250/20000, loss: 0.0557965, test_acc: 0.9756\n",
      "episode: 18260/20000, loss: 0.0563315, test_acc: 0.9756\n",
      "episode: 18270/20000, loss: 0.0544557, test_acc: 0.9756\n",
      "episode: 18280/20000, loss: 0.0522440, test_acc: 0.9756\n",
      "episode: 18290/20000, loss: 0.0495287, test_acc: 0.9756\n",
      "episode: 18300/20000, loss: 0.0538019, test_acc: 0.9756\n",
      "episode: 18310/20000, loss: 0.0576892, test_acc: 0.9756\n",
      "episode: 18320/20000, loss: 0.0487200, test_acc: 0.9756\n",
      "episode: 18330/20000, loss: 0.0554841, test_acc: 0.9756\n",
      "episode: 18340/20000, loss: 0.0526459, test_acc: 0.9756\n",
      "episode: 18350/20000, loss: 0.0478097, test_acc: 0.9756\n",
      "episode: 18360/20000, loss: 0.0556153, test_acc: 0.9756\n",
      "episode: 18370/20000, loss: 0.0552827, test_acc: 0.9756\n",
      "episode: 18380/20000, loss: 0.0586658, test_acc: 0.9756\n",
      "episode: 18390/20000, loss: 0.0562653, test_acc: 0.9756\n",
      "episode: 18400/20000, loss: 0.0531126, test_acc: 0.9756\n",
      "episode: 18410/20000, loss: 0.0595338, test_acc: 0.9756\n",
      "episode: 18420/20000, loss: 0.0565610, test_acc: 0.9756\n",
      "episode: 18430/20000, loss: 0.0549032, test_acc: 0.9756\n",
      "episode: 18440/20000, loss: 0.0561305, test_acc: 0.9756\n",
      "episode: 18450/20000, loss: 0.0544571, test_acc: 0.9756\n",
      "episode: 18460/20000, loss: 0.0570010, test_acc: 0.9756\n",
      "episode: 18470/20000, loss: 0.0562013, test_acc: 0.9756\n",
      "episode: 18480/20000, loss: 0.0584871, test_acc: 0.9756\n",
      "episode: 18490/20000, loss: 0.0518500, test_acc: 0.9756\n",
      "episode: 18500/20000, loss: 0.0548290, test_acc: 0.9834\n",
      "episode: 18510/20000, loss: 0.0489335, test_acc: 0.9834\n",
      "episode: 18520/20000, loss: 0.0552937, test_acc: 0.9834\n",
      "episode: 18530/20000, loss: 0.0543663, test_acc: 0.9834\n",
      "episode: 18540/20000, loss: 0.0532387, test_acc: 0.9834\n",
      "episode: 18550/20000, loss: 0.0560999, test_acc: 0.9834\n",
      "episode: 18560/20000, loss: 0.0533835, test_acc: 0.9834\n",
      "episode: 18570/20000, loss: 0.0524802, test_acc: 0.9834\n",
      "episode: 18580/20000, loss: 0.0553617, test_acc: 0.9834\n",
      "episode: 18590/20000, loss: 0.0545934, test_acc: 0.9834\n",
      "episode: 18600/20000, loss: 0.0463174, test_acc: 0.9834\n",
      "episode: 18610/20000, loss: 0.0550571, test_acc: 0.9834\n",
      "episode: 18620/20000, loss: 0.0596854, test_acc: 0.9834\n",
      "episode: 18630/20000, loss: 0.0568867, test_acc: 0.9834\n",
      "episode: 18640/20000, loss: 0.0559304, test_acc: 0.9834\n",
      "episode: 18650/20000, loss: 0.0563307, test_acc: 0.9834\n",
      "episode: 18660/20000, loss: 0.0571629, test_acc: 0.9834\n",
      "episode: 18670/20000, loss: 0.0535170, test_acc: 0.9834\n",
      "episode: 18680/20000, loss: 0.0484677, test_acc: 0.9834\n",
      "episode: 18690/20000, loss: 0.0536978, test_acc: 0.9834\n",
      "episode: 18700/20000, loss: 0.0547250, test_acc: 0.9834\n",
      "episode: 18710/20000, loss: 0.0526909, test_acc: 0.9834\n",
      "episode: 18720/20000, loss: 0.0570406, test_acc: 0.9834\n",
      "episode: 18730/20000, loss: 0.0509876, test_acc: 0.9834\n",
      "episode: 18740/20000, loss: 0.0561527, test_acc: 0.9834\n",
      "episode: 18750/20000, loss: 0.0558375, test_acc: 0.9834\n",
      "episode: 18760/20000, loss: 0.0553006, test_acc: 0.9834\n",
      "episode: 18770/20000, loss: 0.0547535, test_acc: 0.9834\n",
      "episode: 18780/20000, loss: 0.0560066, test_acc: 0.9834\n",
      "episode: 18790/20000, loss: 0.0502144, test_acc: 0.9834\n",
      "episode: 18800/20000, loss: 0.0581601, test_acc: 0.9834\n",
      "episode: 18810/20000, loss: 0.0561636, test_acc: 0.9834\n",
      "episode: 18820/20000, loss: 0.0526665, test_acc: 0.9834\n",
      "episode: 18830/20000, loss: 0.0529976, test_acc: 0.9834\n",
      "episode: 18840/20000, loss: 0.0566065, test_acc: 0.9834\n",
      "episode: 18850/20000, loss: 0.0516884, test_acc: 0.9834\n",
      "episode: 18860/20000, loss: 0.0570735, test_acc: 0.9834\n",
      "episode: 18870/20000, loss: 0.0582035, test_acc: 0.9834\n",
      "episode: 18880/20000, loss: 0.0521070, test_acc: 0.9834\n",
      "episode: 18890/20000, loss: 0.0513955, test_acc: 0.9834\n",
      "episode: 18900/20000, loss: 0.0575165, test_acc: 0.9834\n",
      "episode: 18910/20000, loss: 0.0546534, test_acc: 0.9834\n",
      "episode: 18920/20000, loss: 0.0509440, test_acc: 0.9834\n",
      "episode: 18930/20000, loss: 0.0495490, test_acc: 0.9834\n",
      "episode: 18940/20000, loss: 0.0513236, test_acc: 0.9834\n",
      "episode: 18950/20000, loss: 0.0565634, test_acc: 0.9834\n",
      "episode: 18960/20000, loss: 0.0544702, test_acc: 0.9834\n",
      "episode: 18970/20000, loss: 0.0577796, test_acc: 0.9834\n",
      "episode: 18980/20000, loss: 0.0540163, test_acc: 0.9834\n",
      "episode: 18990/20000, loss: 0.0589253, test_acc: 0.9834\n",
      "episode: 19000/20000, loss: 0.0534131, test_acc: 0.9322\n",
      "episode: 19010/20000, loss: 0.0575812, test_acc: 0.9322\n",
      "episode: 19020/20000, loss: 0.0590359, test_acc: 0.9322\n",
      "episode: 19030/20000, loss: 0.0513564, test_acc: 0.9322\n",
      "episode: 19040/20000, loss: 0.0553664, test_acc: 0.9322\n",
      "episode: 19050/20000, loss: 0.0476404, test_acc: 0.9322\n",
      "episode: 19060/20000, loss: 0.0572146, test_acc: 0.9322\n",
      "episode: 19070/20000, loss: 0.0542241, test_acc: 0.9322\n",
      "episode: 19080/20000, loss: 0.0574334, test_acc: 0.9322\n",
      "episode: 19090/20000, loss: 0.0537824, test_acc: 0.9322\n",
      "episode: 19100/20000, loss: 0.0587652, test_acc: 0.9322\n",
      "episode: 19110/20000, loss: 0.0544275, test_acc: 0.9322\n",
      "episode: 19120/20000, loss: 0.0565229, test_acc: 0.9322\n",
      "episode: 19130/20000, loss: 0.0510443, test_acc: 0.9322\n",
      "episode: 19140/20000, loss: 0.0575173, test_acc: 0.9322\n",
      "episode: 19150/20000, loss: 0.0576900, test_acc: 0.9322\n",
      "episode: 19160/20000, loss: 0.0561048, test_acc: 0.9322\n",
      "episode: 19170/20000, loss: 0.0575332, test_acc: 0.9322\n",
      "episode: 19180/20000, loss: 0.0531530, test_acc: 0.9322\n",
      "episode: 19190/20000, loss: 0.0535583, test_acc: 0.9322\n",
      "episode: 19200/20000, loss: 0.0478837, test_acc: 0.9322\n",
      "episode: 19210/20000, loss: 0.0529453, test_acc: 0.9322\n",
      "episode: 19220/20000, loss: 0.0504833, test_acc: 0.9322\n",
      "episode: 19230/20000, loss: 0.0532089, test_acc: 0.9322\n",
      "episode: 19240/20000, loss: 0.0559536, test_acc: 0.9322\n",
      "episode: 19250/20000, loss: 0.0509930, test_acc: 0.9322\n",
      "episode: 19260/20000, loss: 0.0516681, test_acc: 0.9322\n",
      "episode: 19270/20000, loss: 0.0564026, test_acc: 0.9322\n",
      "episode: 19280/20000, loss: 0.0541614, test_acc: 0.9322\n",
      "episode: 19290/20000, loss: 0.0553575, test_acc: 0.9322\n",
      "episode: 19300/20000, loss: 0.0533538, test_acc: 0.9322\n",
      "episode: 19310/20000, loss: 0.0610560, test_acc: 0.9322\n",
      "episode: 19320/20000, loss: 0.0567316, test_acc: 0.9322\n",
      "episode: 19330/20000, loss: 0.0474525, test_acc: 0.9322\n",
      "episode: 19340/20000, loss: 0.0513211, test_acc: 0.9322\n",
      "episode: 19350/20000, loss: 0.0594779, test_acc: 0.9322\n",
      "episode: 19360/20000, loss: 0.0548045, test_acc: 0.9322\n",
      "episode: 19370/20000, loss: 0.0570699, test_acc: 0.9322\n",
      "episode: 19380/20000, loss: 0.0553621, test_acc: 0.9322\n",
      "episode: 19390/20000, loss: 0.0550422, test_acc: 0.9322\n",
      "episode: 19400/20000, loss: 0.0531480, test_acc: 0.9322\n",
      "episode: 19410/20000, loss: 0.0509557, test_acc: 0.9322\n",
      "episode: 19420/20000, loss: 0.0559191, test_acc: 0.9322\n",
      "episode: 19430/20000, loss: 0.0563888, test_acc: 0.9322\n",
      "episode: 19440/20000, loss: 0.0540510, test_acc: 0.9322\n",
      "episode: 19450/20000, loss: 0.0536312, test_acc: 0.9322\n",
      "episode: 19460/20000, loss: 0.0551152, test_acc: 0.9322\n",
      "episode: 19470/20000, loss: 0.0548475, test_acc: 0.9322\n",
      "episode: 19480/20000, loss: 0.0549540, test_acc: 0.9322\n",
      "episode: 19490/20000, loss: 0.0516395, test_acc: 0.9322\n",
      "episode: 19500/20000, loss: 0.0576834, test_acc: 0.9839\n",
      "episode: 19510/20000, loss: 0.0554508, test_acc: 0.9839\n",
      "episode: 19520/20000, loss: 0.0588612, test_acc: 0.9839\n",
      "episode: 19530/20000, loss: 0.0575226, test_acc: 0.9839\n",
      "episode: 19540/20000, loss: 0.0546748, test_acc: 0.9839\n",
      "episode: 19550/20000, loss: 0.0540829, test_acc: 0.9839\n",
      "episode: 19560/20000, loss: 0.0529233, test_acc: 0.9839\n",
      "episode: 19570/20000, loss: 0.0592055, test_acc: 0.9839\n",
      "episode: 19580/20000, loss: 0.0506593, test_acc: 0.9839\n",
      "episode: 19590/20000, loss: 0.0551223, test_acc: 0.9839\n",
      "episode: 19600/20000, loss: 0.0585485, test_acc: 0.9839\n",
      "episode: 19610/20000, loss: 0.0536373, test_acc: 0.9839\n",
      "episode: 19620/20000, loss: 0.0517198, test_acc: 0.9839\n",
      "episode: 19630/20000, loss: 0.0509322, test_acc: 0.9839\n",
      "episode: 19640/20000, loss: 0.0581087, test_acc: 0.9839\n",
      "episode: 19650/20000, loss: 0.0545879, test_acc: 0.9839\n",
      "episode: 19660/20000, loss: 0.0566943, test_acc: 0.9839\n",
      "episode: 19670/20000, loss: 0.0551788, test_acc: 0.9839\n",
      "episode: 19680/20000, loss: 0.0617020, test_acc: 0.9839\n",
      "episode: 19690/20000, loss: 0.0561655, test_acc: 0.9839\n",
      "episode: 19700/20000, loss: 0.0552586, test_acc: 0.9839\n",
      "episode: 19710/20000, loss: 0.0533799, test_acc: 0.9839\n",
      "episode: 19720/20000, loss: 0.0524764, test_acc: 0.9839\n",
      "episode: 19730/20000, loss: 0.0564944, test_acc: 0.9839\n",
      "episode: 19740/20000, loss: 0.0558033, test_acc: 0.9839\n",
      "episode: 19750/20000, loss: 0.0528678, test_acc: 0.9839\n",
      "episode: 19760/20000, loss: 0.0567393, test_acc: 0.9839\n",
      "episode: 19770/20000, loss: 0.0538503, test_acc: 0.9839\n",
      "episode: 19780/20000, loss: 0.0508926, test_acc: 0.9839\n",
      "episode: 19790/20000, loss: 0.0570032, test_acc: 0.9839\n",
      "episode: 19800/20000, loss: 0.0547169, test_acc: 0.9839\n",
      "episode: 19810/20000, loss: 0.0555893, test_acc: 0.9839\n",
      "episode: 19820/20000, loss: 0.0538745, test_acc: 0.9839\n",
      "episode: 19830/20000, loss: 0.0546031, test_acc: 0.9839\n",
      "episode: 19840/20000, loss: 0.0577840, test_acc: 0.9839\n",
      "episode: 19850/20000, loss: 0.0543694, test_acc: 0.9839\n",
      "episode: 19860/20000, loss: 0.0557654, test_acc: 0.9839\n",
      "episode: 19870/20000, loss: 0.0544225, test_acc: 0.9839\n",
      "episode: 19880/20000, loss: 0.0566589, test_acc: 0.9839\n",
      "episode: 19890/20000, loss: 0.0514782, test_acc: 0.9839\n",
      "episode: 19900/20000, loss: 0.0538990, test_acc: 0.9839\n",
      "episode: 19910/20000, loss: 0.0549971, test_acc: 0.9839\n",
      "episode: 19920/20000, loss: 0.0560021, test_acc: 0.9839\n",
      "episode: 19930/20000, loss: 0.0537922, test_acc: 0.9839\n",
      "episode: 19940/20000, loss: 0.0543004, test_acc: 0.9839\n",
      "episode: 19950/20000, loss: 0.0550817, test_acc: 0.9839\n",
      "episode: 19960/20000, loss: 0.0500139, test_acc: 0.9839\n",
      "episode: 19970/20000, loss: 0.0511456, test_acc: 0.9839\n",
      "episode: 19980/20000, loss: 0.0533765, test_acc: 0.9839\n",
      "episode: 19990/20000, loss: 0.0510212, test_acc: 0.9839\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = '/kaggle/input/episodes20000dqnmsetarget-hypertension/3d_256-512-256-episodes_20000--dqn-mse-target_treatment_ascvd.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3197b7289b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m               .format(e, EPISODES, loss, cur_val_error))\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/episodes20000dqnmsetarget-hypertension/3d_256-512-256-episodes_20000--dqn-mse-target_treatment_{}.h5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_treatment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-69de3487d24b>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;31m# TODO(rchao): Save this attribute in a decoupled checkpoint file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/kaggle/input/episodes20000dqnmsetarget-hypertension/3d_256-512-256-episodes_20000--dqn-mse-target_treatment_ascvd.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "\n",
    "    # patients_trajectories = random.sample(patients_set, batch_size)\n",
    "    if sample_patient:\n",
    "        patient = random.sample(patients_set, 1)\n",
    "        minibatch = interested_train[interested_train['study_id'] == patient]\n",
    "    else:\n",
    "        minibatch = interested_train.sample(batch_size)\n",
    "\n",
    "    # minibatch = random.sample(agent.memory, batch_size)\n",
    "    if e % episodes_till_target_update:\n",
    "        agent.update_target()\n",
    "\n",
    "    # for visit in minibatch:\n",
    "    #     loss = agent.replay(minibatch[visit], True)\n",
    "\n",
    "    agent.learning_rate = agent.learning_rate * (1 - agent.learning_rate_decay)\n",
    "    loss = agent.replay(minibatch, True)\n",
    "\n",
    "    if e % 500 == 0:\n",
    "        cur_val_error = sum(np.argmax(interested_tests[target_column_renames].values, axis=1) == np.argmax(\n",
    "            agent.model.predict(interested_tests[state_cols]), axis=1)) / interested_tests.shape[0]\n",
    "    # Logging training loss every 10 timesteps\n",
    "    if e % 10 == 0:\n",
    "        print(\"episode: {}/{}, loss: {:.7f}, test_acc: {:.4f}\"\n",
    "              .format(e, EPISODES, loss, cur_val_error))\n",
    "            \n",
    "agent.save(\"/kaggle/input/episodes20000dqnmsetarget-hypertension/3d_256-512-256-episodes_20000--dqn-mse-target_treatment_{}.h5\".format(target_treatment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
